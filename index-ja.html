<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>キャラクター画像一枚でVTuberになれるシステムを作ってみた【体も動かせるよ編】
      </title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">    
    <link href="css/theme-ja.css" rel="stylesheet">

    <!-- MathJax -->
    <script>
    MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  svg: {
	    fontCache: 'global'
	  }
	};
    </script>
    <script src="mathjax/tex-chtml.js" id="MathJax-script" async></script>    
    <script type="text/javascript" src="js/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="js/bigfoot.min.js"></script>
    <link rel="stylesheet" type="text/css" href="css/bigfoot-default.css">
  </head>

  <body>
    <!--
  	 <nav class="navbar navbar-expand-md navbar-light fixed-bottom bg-light">
      <div class="collapse navbar-collapse justify-content-center" id="navbarCollapse">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#">&nbsp; &nbsp; Top</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Abstract</a>
          </li>          
          <li class="nav-item">
            <a class="nav-link" href="#background">Background</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#problem-spec">Problem Spec</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#data">Data</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#networks">Networks</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#conclusion">Conclusion</a>
          </li>
        </ul>        
      </div>
    </nav>
    -->
    
    <div class="container" style="max-width: 640px;">      
    	<span style="visibility: hidden;">
	      \(
	      \def\sc#1{\dosc#1\csod}
	      \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\ves}[1]{\boldsymbol{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \DeclareMathOperator*{\argmax}{arg\,max}
        \DeclareMathOperator*{\argmin}{arg\,min}        
	      \)
    	</span>
      <h1 align="center">&nbsp;</h1>
      <h2 align="center">キャラクター画像一枚で<br>VTuberになれるシステムを作ってみた</h2>
      <h4 align="center">【体も動かせるよ編】</h4>
      <p align="center">
      <a href="http://pkhungurn.github.io/index-ja.html">プラムック・カンガーン</a></a>
      </p>
      <h1 align="center">&nbsp;</h1>
    </div>

  <p>
  <a name="eyecatcher"></a>
  <div align="center">
		<video id="eyecatcher" autoplay muted playsinline loop>
		    <source src="data/eyecatcher.mp4" type="video/mp4">
		</video>
		<br/>
		上の動画中のキャラクターはバーチャルYouTuberまたはそれに関連したキャラクターです。この記事中の画像や映像は彼らのファンアート・二次創作です。 <a href="#fn_eyecatcher_footnote" rel="footnote">[footnote]</a>
	</div>
  </p>

  <div class="footnotes"><ul>
    <li class="footnote" id="fn_eyecatcher_footnote">					
      <p align="left">ほとんどの動画内のバーチャルYouTuberは<a href="https://www.anycolor.co.jp">ANYCOLOR</a>、<a href="https://cover-corp.com/">カバー</a>、<a href="https://www.774.ai/">774 inc.</a>、<a href="https://twitter.com/noriopro">のりプロ</a>、<a href="https://www.kmnz.jp/">KMNZ</a>等の企業様に所属してます。<a href="https://www.youtube.com/channel/UCWCc8tO-uUl_7SJXIKJACMw">神楽めあ</a>様、<a href="https://www.youtube.com/channel/UCt30jJgChL8qeT9VPadidSw">しぐれうい</a>様、<a href="https://www.youtube.com/channel/UCwUNuXd6rN08SQFzlIH4Ozg">なつめえり</a>様、<a href="https://www.youtube.com/channel/UCIdEIHpS0TdkqRkHL5OkLtA">名取さな</a>様、<a href="https://www.youtube.com/channel/UCkPIfBOLoO0hVPG-tI2YeGg">兎鞠まり</a>様、<a href="https://www.youtube.com/channel/UCPf-EnX70UM7jqjKwhDmS8g">魔王マグラナ</a>様、<a href="https://www.youtube.com/channel/UCj_KuUzpOXAliYEesJwdrbw">伊東ライフ</a>様、<a href="https://www.youtube.com/channel/UCwXOUuxUxCJ1yD0JfogUvMg">ユキミお姉ちゃん</a>様、<a href="https://www.youtube.com/channel/UCqXvL55GYHtRZhBS03LVGnQ">楪穂波</a>様の画像も使わせていただきました。誠に申し訳ございませんが、切り抜き動画やMADや同人ゲーム等の二次創作と同じように使用許可は全く取っておりません。</p>
    </li>
  </ul>
  </div>

	<div class="container" style="max-width: 640px;">
    <h1>&nbsp;</h1>
		<a name="abstract"></a>
    この度は一枚のキャラクター画像からアニメーションを生成するニューラルネットワークのシステムの第３回目の試作を発表したいと思います。<a href="http://pkhungurn.github.io/talking-head-anime-2">去年作ったシステム</a>はキャラクターの頭しか動かせませんが、新しいシステムは体も動かせます。具体的に、以下の動きが実装されました。</p>
  
    <p>
      <table align="center">
        <tr>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_y.mp4" type="video/mp4">
            </video>          
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_z.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_breathing.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td align="center">体の $y$ 軸まわりの回転</td>
          <td align="center">体の $z$ 軸まわりの回転</td>
          <td align="center" valign="top">呼吸</td>
        </tr>
      </table>
    </p>

    <p>
      新しいシステムを用いて、人間の動きをアニメキャラクターに反映させる既存のツールを更新しました。更新された機能は以下の動画で確認できます。
    </p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/OO8aykBkxsA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>またシステムの高速化と軽量化の方法も実験しました。画質の劣化を抑えながら、必要なメモリーを大幅に減らすことができ（パラメータのデータ容量を18倍縮小、必要RAMは3倍縮小）、処理速度も少し上げることが出来ました。</p>
		
    <h1>&nbsp;</h1>
	<h2>1 &nbsp; はじめに</h2>

    <p>バーチャルＹｏｕＴｕｂｅｒ（ＶＴｕｂｅｒ）になりやすくすることを目標に、私は２０１９年から自由研究をやり続けてきました。 具体的には、アニメキャラクターの顔イラストを一枚受け取り、そのイラストを動かせるニューラルネットワークのシステムを作りました。システムは顔を回転させることと表情を変更させることが出来ますが、様々な問題があります。それらを解決しなければ、システムは実践的なコンテンツ制作ツールになれません。今回の研究目的は以下の２つの問題を改善することです。</p>
    
    <ol>
        <li><b>体が動かない。</b>既存のシステムはキャラクターの顔しか動かせません。しかし、プロのLive2Dモデラーが作ったVTuberモデルは体をある程度回転させられたり、上半身を上下に揺らせる「呼吸モーション」も実装されています。</li>

        <li><b>GPUの負担が大きい。</b>システムのデータ容量は500MBである上に、強力なGPUを使わなければ利用できません。</li>
    </ol>
    
    <p>1番目の問題には、上半身が全部含まれるより大きい画像を扱えるようにシステムを改造しました。プロによるVTuberモデルのフィーチャに追いつくべく、先に述べた体の3種類の動きをシステムに追加しました。</p>
    
    <p>2番目の問題には、以下のモデル縮小方法を試しました。</p>
    
    <ul>
      <li>普通の畳み込み層の代わりに深さ単位分離可能畳み込み層(depthwise separable convolution layer)を用いる<a href="#fn_sifre_2014" rel="footnote">[Sifre 2014]</a>。</li>

      <li>32ビットの浮動小数点数型(<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html">float</a>)の代わりに16ビットの型(<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>)を用いる。</li>
    </ul>

    <p>両方を同時に導入した結果、システムのパラメータのデータ容量を18倍縮小することが出来、必要RAMも3~4倍縮小することが出来ました。</p>

    <div class="footnotes"><ul>
      <li class="footnote" id="fn_sifre_2014">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>
    </ul></div>

    <a name="background"></a>
	  <h2>2 &nbsp; 背景</h2>

    <p>この記事の試作は<a href="http://pkhungurn.github.io/talking-head-anime-2/index.html"></a>2021年に作られたシステム</a>を基にして開発されました。既存のシステムは(1)キャラクターの正面画像と(2)キャラクターのポーズを指定する<b>ボーズベクトル</b>を入力として受け取り、同じキャラクターが指定されたポーズを取った別の画像を出力します。構造は大まかに2つのニューラルネットワークに分解されます。<a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#face-morpher">顔モーファー</a>(face morpher)というネットワークはキャラクターの表情を変更させ、<a href="http://pkhungurn.github.io/talking-head-anime/index.html#face-rotator">顔ローテーター</a>というネットワークは顔を回転させます。</p>

    <p align="center">
      <a name="fig:old-system-overview" />
      <table align="center">
          <tr>
              <td align="center">
                  <a href="data/overview_old_two_step_process_ja.png"><img src="data/overview_old_two_step_process_ja.png" width="600"></a>
              </td>
          </tr>
          <tr>
              <td align="center">
                  <b>図2.1：</b>2021年のシステム。
              </td>
          </tr>
      </table>
    </p>

    <p>システムの教師データは<a href="https://sites.google.com/view/vpvp/">MikuMikuDance</a>(MMD)という3Dアニメーションソフトウェア用のキャラクターモデルをレンダーリングして作られました。データセットを作るために、私はMMDモデルを約8000個を集めて、アノテーションを付けました。その過程の詳細はこれらの記事(<a href="https://pkhungurn.github.io/talking-head-anime/index.html#dataset">リンク</a>、<a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#data">リンク</a>)に書いてあります。</p>



   <a name="moving-the-body"></a>
	 <h2>3 &nbsp; 体を動かす</h2>

    <p>キャラクターの体を動かすためにシステムを改造しなければなりません。まず、入力画像はキャラクターの上半身を全部描写する必要があったので、画像の解像度を$256 \times 256$から$512 \times 512$に上げました。顔モーファーは2021年の仕様のままにしましたが、顔ローテーターは<b>体ローテーター</b>という新しいニューラルネットワークに置き換えました。このネットワークの役割は(1)顔と体を回転させることと(2)呼吸モーションを生成することです。</p>

    <p>
        <a name="fig:new-system-overview" />
        <table>
          <tr>
            <td align="center">
              <a href="data/overview_new_two_step_process_ja.png">
                <img src="data/overview_new_two_step_process_ja.png" width="600">
              </a>
            </td>
          </tr> 
          <tr>
            <td align="center">
              <b>図3.1：</b>新しいシステム
            </td>
          </tr>
        </table>
    </p>

    <h3>3.1 &nbsp; 体ローテーターの構造</h3>

    <p>体ローテーターは2つのサブネットワークで構成されています。<b>半解像度ローテーター</b>というネットワークは体ローテーターと同じ目的を果たしますが、入力画像は$256 \times 256$に縮小された体ローテーターの入力であり、出力画像も同じ解像度です。それは$512 \times 512$に拡大され、<b>エディター</b>というネットワークに渡します。エディターは画像に編集と合成を施し、最後に画像を一枚出力します。</p>
  </div>

  <div class="container" style="max-width: 1024px"></div>
    <p>
        <table align="center">
        <tr>
            <td>
            <a href="data/overview_body_rotator_ja.png">
                <img src="data/overview_body_rotator_ja.png" alt="" width="1024">
            </a>
            </td>
        </tr>
        <tr>
            <td align="center"><b>図3.1.1：</b>体ローテーターの大まかな構造。</td>
        </tr>
        </table>
    </p>
  </div>

  <div class="container" style="max-width: 640px;">

    <p>先に述べた「解像度を半分にして回転させ、それから拡大して編集する」というやや煩わしいやり方を選択した理由は、既存の顔ローテーターの処理速度に劣らないように体ローテーターを高速化するためです。体ローテーターの入力画像は$512 \times 512$であり、顔ローテーターの入力画像より画素が4倍あります。なので、顔ローテーターと同じようなネットワーク構造を用いたら、体ローテーターは顔ローテーターより4倍遅くなり、リアルタイムのアプリケーションで使えなくなります。しかし、入力画像を$256 \times 256$に縮小することで、既存のネットワーク構造を用いても処理速度は変わりません。半解像度ローテーターを走らせた後、最後の出力を生成するために必要な処理は出力した画像を$512 \times 512$に拡大してディテールを追加することだけなので、「体の回転させる」タスクに比べると大幅に易しくなります。つまり、エディターはパラメータ数が他のネットワークより少なくても大丈夫なので、入力画像は画素が4倍増えても半解像度ローテーターと同じ処理速度を出せるように調整できます。具体的に、半解像度ローテーターのデータ容量は130MBですが、エディターのデータ容量は30MBだけです。</p>

    <p>半解像度ローテーターの本体はエンコーダー・デコーダネットワークで、キャラクターの上半身画像とポーズベクトルを受け取り、特徴量のテンソルを出力します。そのテンソルに以下の2つの画像処理手法を施し、出力画像を1枚ずつ生成します。</p>

    <ol>
      <li><b>直接生成。</b>特徴量テンソルは$3 \times 3$の畳み込み層と双曲線正接層に処理され、その出力は画像として見なされます。</li>
      <li><b>ワープ。</b>特徴量テンソルから<i>アピアランスフロー</i>(appearance flow)を作ります。これは出力画像の各画素に入力画像のどこから色をコピーするかを示します。アピアランスフローを入力画像に施しますと、画素が再配置されて新しい画像になります。この手法は、ECCV 2016でのZhouらの論文<a href="#fn_zhou_2016">[Zhou et al. 2016]</a>に由来するものです。</li>
    </ol>    

    <div class="footnotes">
			<ul>				
				<li class="footnote" id="fn_zhou_2016">
					Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros.
					<b>View Synthesis by Appearance Flow.</b> 
					ECCV 2016. 
					<a href="https://arxiv.org/abs/1605.03557">[arXiv]</a>
				</li>
			</ul>
		</div>

    <a name="fig:half-res-rotator" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/half_res_rotator_ja.png">
              <img src="data/half_res_rotator_ja.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>図3.1.2：</b>半解像度ローテーター。
          </td>
        </tr>
      </table>
    </p>

    <p>半解像度ローテーターは画像を2枚出力しますが、アピアランスフローのオフセット<a href="#fn_appearance_flow_offset">[footnote]</a>も出力しますので、実際に出力されるものは3つあります。直接生成された画像はシステムを学習させる時にしか使わないので、推論時には捨てられます。ワープで生成された画像とアピアランスフローのオフセットは$512 \times 512$に拡大され、エディターに渡されます。</p>
    
    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_appearance_flow_offset">アピアランスフローのオフセットとは、アピアランスフローと画素を移動させないアピアランスフロー($F_\mrm{identity}$)の差分 <p>ニューラルネットワークには大体アピアランスフローを作るよりオフセットの方を作るのが簡単です。例えば、キャラクターの体を小さい角度で回転させるアピアランスフローを考察してみましょう。オフセットの値はどこでもゼロに近いと考えられます。しかし、フローの方は$F_\mrm{identity}$とほぼ同じです。$F_\mrm{identity}$の値はグリッドパターンに従って配置された画素の座標なので、ゼロテンソルより何倍も複雑です。</p>

        <p>なので、私のワープの実装はまずアピアランスフローのオフセットを生成し、それに$F_\mrm{identity}$を足してアピアランスフローを作ります。</p>        
        </li>
      </ul>
    </div>

    <p>エディターの構造は半解像度ローテーターのと似ていますが、本体はエンコーダー・デコーダではなくてU-Netです<a href="#fn_ronneberger_2015_0">[Ronneberger et al. 2015]</a>。本体が出力する特徴量テンソルは以下の画像処理手法に使われています。</p>

    <div class="footnotes">
      <ul>
          <li class="footnote" id="fn_ronneberger_2015_0">
              <p align="left">
              Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
              <b>U-Net: Convolutional Networks for Biomedical Image Segmentation.</b>
              Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015.
              <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">[Project]</a>
              <a href="https://arxiv.org/abs/1511.06702">[arXiv]</a>
              </p>
          </li>
      </ul>
    </div>

    <ol>
      <li><b>入力画像をワープする。</b>特徴量テンソルから新しいアピアランスフローのオフセットを作り、それに半解像度ローテーターのオフセットと$F_\mrm{identity}$を足して新しいアピアランスフローを作ります。そのアピアランスフローを使って最初の$512 \times 512$の入力画像をワープします。</li>
      <li><b>ワープで生成された画像を部分的に変更する。</b>同じ特徴量テンソルから(1)前のステップの出力画像との差分を表す画像と(2)その差分の不透明度を表すアルファマスクを作ります<a href="#fn_pumarola_2018">[Pumarola et al. 2018]</a>。それらを使って<a href="https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%AB%E3%83%95%E3%82%A1%E3%83%96%E3%83%AC%E3%83%B3%E3%83%89">アルファブレンド</a>で前のステップの画像と差分を合成します。その結果はエディターの出力になります。</li>
    </ol>    

    <div class="footnotes">
			<ul>
				<li class="footnote" id="fn_pumarola_2018">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
			</ul>
		</div>

    <a name="fig:editor" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/editor_ja.png">
              <img src="data/editor_ja.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>図3.1.3：</b>エディター。
          </td>
        </tr>
      </table>
    </p>

    <p>上述の構造には複数の変わったデザインが含まれます。一つは、半解像度ローテーターは直接生成で画像を1枚作りますが、その画像は推論時に何にも使われずすぐに捨てられます。もう一つは、エディターの複雑な構造です。例えば、最後の出力画像は、以上の2つの画像処理手法を用いて生成する代わりに、特徴量テンソルから直接生成できるはずです。これらの変わったデザインは複数のデザインの効果測定を行い、一番いいデザインを選んだ結果です。選別の過程はこの記事の<a href="full.html#sec:ablation-study">完全版</a>に書いてあります。</p>
    
    <h3>3.2 &nbsp; 教師データ</h3>

    <p><a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#data">2021年に作ったデータセット</a>は上半身全体を映していないため、体ローテーターを学習させるには新しいデータセットを作らなければなりません。学習データは前のデータセットと同じく集めた8000体のMMDモデルをレンダーリングすることで作られましたが、MMDモデルの扱い方が変わりました。</p>

    <p><b>入力画像のポーズ。</b>前のデータセットは、入力画像の中のキャラクターがいわゆる「Aポーズ」を取る仕様になっています。</p>

    <p>
      <a href="#fig:rest-pose"></a>
      <table align="center">
        <tr>
          <td align="center">
            <a href="data/rest_pose.png"><img src="data/rest_pose.png" alt="" width="240"></a>
          </td>         
        </tr>
        <tr>
          <td align="center">
            <b>図3.2.1：</b>Aポーズ。キャラクターは<a href="https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA">キズナアイ</a>(© Kizuna AI)。
          </td>
        </tr>
      </table>
    </p>

    <p>しかし、推論時にはシステムの入力データは人が描いたイラストレーションです。Aポーズを取ったキャラクターのイラストレーションは多くないため、そのポーズが仕様のデータセットで学習させられたシステムは汎用性が低すぎます。システムをより役に立つようにするには入力画像のポーズの多様性を増やす必要があります。新しいデータセットの中のキャラクターは基本的に立ちポーズを取りますが、その多くはAポーズのように棒立ちではなく、体を少し左右に向けたり、腰を小さい角度で回転させたりします。腕はかなり自由に配置されますが、(1)手は顔より下に配置されることと(2)腕と手が顔に近すぎないことが望ましいです。</p>

    <p>
      <a name="fig:sample_inputs" />
      <table align="center">
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/00.png">
              <img src="data/kizuna_ai_sample_inputs/00.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/01.png">
              <img src="data/kizuna_ai_sample_inputs/01.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/02.png">
              <img src="data/kizuna_ai_sample_inputs/02.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/03.png">
              <img src="data/kizuna_ai_sample_inputs/03.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/04.png">
              <img src="data/kizuna_ai_sample_inputs/04.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/05.png">
              <img src="data/kizuna_ai_sample_inputs/05.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/06.png">
              <img src="data/kizuna_ai_sample_inputs/06.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/07.png">
              <img src="data/kizuna_ai_sample_inputs/07.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>図3.2.2：</b>新しいデータセットの入力画像のポーズ例。
          </td>
        </tr>
      </table>
    </p>

    <p>上述のポーズを生成するために、私は<a href="https://www.nicovideo.jp/">ニコニコ動画</a>や<a href="https://bowlroll.net/">BowlRoll</a>からMMDモデル用のポーズをダウンロードしました。4731個集めましたが、使えるのは832個だけでした。入力画像のポーズを作るには、まず842個のポーズの中から1個をランダムに選んで、そのポーズとランダムに腕の角度を変更されたAポーズを合成します。手法の詳細は記事の<a href="full.html#sec:input-pose">完全版</a>に書いてあります。</p>

    <p><b>髪の長いキャラクターの扱い。</b>MMDモデルでは、頭を回転させると、髪の毛全体も一つの剛体のように一緒に回転します。</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=360 height=360 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v3.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>図3.2.3：</b>MMDモデルの首ボーンを$z$軸まわりに回転させると髪の毛全体が首と一緒に回転します。キャラクターは<a href="https://www.youtube.com/channel/UCeLzT-7b2PBcunJplmWtoDg">周防パトラ</a>(&copy; HoneyStrap)で、MMDモデルは<a href="https://3d.nicovideo.jp/works/td60253">OTUKI</a>さんの作品です。
          </td>
        </tr>
      </table>
    </p>

    <p>しかし、ニューラスニットワークシステムにとってこの動作を真似することは非常に難しいです。まず、システムは髪の毛と髪の毛ではない部分を正しく区別しなければなりせん。正しく区別できない場合、体または服装が切断される不快な間違いが生じます。その他に、入力画像では隠れているが、回転して見えてくる髪の毛の部分も正しく生成する必要があります。</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <img width=300 height=300 src="data/failure_cases/enomiya_milk/enomiya_milk.png" alt="">
          </td>
          <td align="center">
            <video width=300 height=300 muted autoplay playsinline loop>
              <source src="data/failure_cases/enomiya_milk/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="2">
            <b>図3.2.4：</b>右の動画は本記事のシステムを使って左の画像から生成されました。しかし、システムの教師データには図3.2.3のような髪の毛の動きが入ります。動画の下の部分を見てみると、髪の毛のディテールが失われることは明らかです。他に、システムは手が髪の毛の一部だと思い込んだらしく、髪の毛を動かした時に指を切ったり伸ばしたりしていました。キャラクターは<a href="https://www.youtube.com/channel/UCJCzy0Fyrm0UhIrGQ7tHpjg">愛宮みるく</a>(&copy; Noripro)です。
          </td>
        </tr>
      </table>      
    </p>

    <p>最新の機械学習技術を用いて髪の毛の区別と生成を正しくできるかもしれませんが、この2つのタスクを避けることが出来たら、問題は大幅に易しくなります。そのために、MMDモデルの<a href="https://web.stanford.edu/class/cs248/pdf/class_13_skinning.pdf">スキニング</a>のアルゴリズムを改造して、首と頭の影響を頭からあまり離れていない髪の毛の部分に制限しました。その結果は以下の動画で確認できます。</p>
    
    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=360 height=360 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v4.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>図3.2.5：</b>首と頭の影響が制限された後の髪の毛の動き。首が回転しても、頭から離れている髪の毛は動かないため、アニメーション生成のタスクがより簡単になります。
          </td>
        </tr>
      </table>      
    </p>

    <p>まとめると、システムが体の動きを生成できるように改造するには、体ローテーターという新しいサブネットワークを実装する必要があり、それを学習させるデータセットも新しく作らなければなりません。体ローテーターは、まず入力画像を縮小し、その中のキャラクターの体を動かし、そして画像を拡大し調整します。前のプロジェクトの時より4倍大きくなった入力画像を素早く処理できるようにこの変わったデザインを選びました。新しいデータセットはキャラクターが様々なポーズを取るように生成されました。他に、髪が長いキャラクターの扱いを簡単化するべく、MMDモデルの髪の毛に首と頭ボーンの影響を制限しました。</p>

    <h2>4 &nbsp; 効率化</h2>

    <p>私のシステムの短所のーつはリソースが沢山かかることです。パラメータのデータ容量は517MBですし、一枚の画像を処理する時間は、Titan RTXといった強力なGPUを使っても、35msぐらいです。データ容量が大きければ大きいほどモバイルデバイスで使いにくくなり、GPUの処理能力が低ければ低いほど処理時間が長くなります。システムの汎用性を向上するには、必要なメモリー量と処理速度の改善が重要です。私は以下の効率化手法を試しました。</p>

    <ol>
      <li><b>深さ単位分離可能畳み込み層の使用。</b>私のシステムの中のニューラルネットワークは全部<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">畳み込みニューラルネットワーク</a>なので、それらの一番重要な構成要素は畳み込み層です。ネットワークの本体(エンコーダーデコーダ又はU-Net)の中の各畳み込み層を2つのより小さい畳み込み層に置き換えることで、ネットワークのデータ容量を縮小することが出来ます。一番目は入力テンソルの各チャンネルを個別に処理する「深さ単位」の畳み込み層で、二番目はカーネルが$1 \times 1$の普通の畳み込み層です<a href="#fn_sifre_2014_2">[Sifre 2014]</a>。理論上、この手法はネットワークのデータ容量を9倍縮小出来ますが、必要RAMの量は減らないかもしれません。最良の場合、処理速度も9倍向上します。(これらの予測の裏付けは記事の<a href="full.html#sec:improving-efficiency">完全版</a>に書いてあります。)</li>
      <li><b>halfの使用。</b>システムは<a href="http://pytorch.org">PyTorch</a>を使って実装しました。PyTorchの既定のデータ型は<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html">float</a>(32ビットの浮動小数点数型)ですが、システムの機能を実装するにはfloatの精度は高すぎるかもしれません。floatの代わりに<a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>(16ビットの浮動小数点数型)を使ったら、計算の精度は低下しますが、データ容量と必要RAM量が半分になります。しかし、処理速度が上がるかどうかは分かりません。</li>
    </ol>    

    <div class="footnotes"><ul>
      <li class="footnote" id="fn_sifre_2014_2">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>      
    </ul></div>

    <p>深さ単位分離可能畳み込み層を用いるかどうかとhalfを用いるかどうかによって4つのシステムのバリエーションを作りました。深さ単位分離可能畳み込み層を用いる場合、バリエーションの名前に「separable」という言葉を入れて、そうではない場合は「standard」を入れます。使っている浮動小数点数型の名前もバリエーションの名前に入れます。故に、4つのバリエーションの名前は「standard-float」と「separable-float」と「standard-half」と「separable-half」です。</p>

    <h3>4.1 &nbsp; データ容量</h3>

    <p>上述した効率化手法の最大の利点はデータ容量の縮小です。予測通り、深さ単位分離可能畳み込み層はデータ容量を約9倍縮小し、halfはさらにそれを半分にしました。</p>

    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td align="center" colspan="4"><b>データ容量(MB)とstandard-floatに対する縮小率</b></td>
          </tr>
          <tr>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>          
          <tr>
            <td align="right">
              <tt>
                517.56
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                55.48
              </tt>
              <br />
              (9.33x)
            </td>
            <td align="right">
              <tt>
                258.84
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                27.82
              </tt>
              <br />
              (18.60x)
            </td>
          </tr>
        </tbody>
      </table>      
    </p>

    <h3>4.2 &nbsp; 必要RAM量</h3>

    <p>システムのバリエーションが入力画像を1枚処理するために使ったGPUのRAMを測定しました。以下の3台のコンピュータで実験を行いました。</p>

    <ul>
      <li><b><i>コンピュータA</i></b>はNvidia Titan RTXのGPU、Intel Core i9-9900KF(3.60 GHz)のCPU、64GBのRAMが搭載されるデスクトップPCで、ハイエンドのゲーミングPCを代表します。</li>

      <li><b><i>コンピュータB</i></b>はNvidia GeForce GTX 1090 TiのGPU、Intel Core i7-8700K(3.70 GHZ)のCPU、32GBのRAMが搭載されるデスクトップPCで、やや古くて普通のゲーミングPCを代表します。</li>

      <li><b><i>コンピュータC</i></b>はNvidia GeForce MS250のモバイルGPU、Intel Core i5-1035G1(1.19 GHz)のCPU、8GBのRAMが搭載されるノートパソコンで、処理能力の低いコンピュータを代表します。</li>
    </ul>

    <p>実験結果を以下の表に示します。</p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>使ったRAMの量(MB)とstandard-floatに対する縮小率</b></td>
          </tr>
          <tr>
            <td>
              <b>コンピュータ</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              コンピュータA
            </td>
            <td align="right">
              <tt>
                816.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                417.49
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                467.39
              </tt>
              <br />
              (1.75x)
            </td>
            <td align="right">
              <tt>
                274.80
              </tt>
              <br />
              (2.97x)
            </td>
          </tr>
          <tr>
            <td>
              コンピュータB
            </td>
            <td align="right">
              <tt>
                816.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                417.49
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                417.39
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                210.80
              </tt>
              <br />
              (3.88x)
            </td>
          </tr>
          <tr>
            <td>
              コンピュータC
            </td>
            <td align="right">
              <tt>
                813.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                414.08
              </tt>
              <br />
              (1.97x)
            </td>
            <td align="right">
              <tt>
                415.89
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                209.30
              </tt>
              <br />
              (3.89x)
            </td>
          </tr>
        </tbody>
      </table>
    </p>
  </div> 

  <div class="container" style="max-width: 640px;">
    <p>各効率化手法を別々に導入すると必要RAM量が約2倍低下しました。両方導入すると縮小率が約3～4倍になりました。</p>

    <h3>4.3 &nbsp; 処理速度</h3>

    <p>システムが一枚の画像を処理するのに必要な時間も計測しました。</p>    
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>平均処理時間(ms)とstandard-floatの平均処理時間の縮小率</b></td>
          </tr>
          <tr>
            <td>
              <b>コンピュータ</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              コンピュータA
            </td>
            <td align="right">
              <tt>
                34.105
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                26.777
              </tt>
              <br />
              (1.27x)
            </td>
            <td align="right">
              <tt>
                23.803
              </tt>
              <br />
              (1.43x)
            </td>
            <td align="right">
              <tt>
                24.540
              </tt>
              <br />
              (1.39x)
            </td>
          </tr>
          <tr>
            <td>
              コンピュータB
            </td>
            <td align="right">
              <tt>
                43.841
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                46.959
              </tt>
              <br />
              (0.93x)
            </td>
            <td align="right">
              <tt>
                38.019
              </tt>
              <br />
              (1.15x)
            </td>
            <td align="right">
              <tt>
                38.848
              </tt>
              <br />
              (1.13x)
            </td>
          </tr>
          <tr>
            <td>
              コンピュータC
            </td>
            <td align="right">
              <tt>
                690.751
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                335.364
              </tt>
              <br />
              (2.06x)
            </td>
            <td align="right">
              <tt>
                1125.345
              </tt>
              <br />
              (0.61x)
            </td>
            <td align="right">
              <tt>
                385.041
              </tt>
              <br />
              (1.79x)g
            </td>
          </tr>
        </tbody>
      </table>
    </p>
  </div>  

  <div class="container" style="max-width: 640px;">
    <p>各効率化手法は処理スピードを上げたとは言えますが、最良の場合は2倍しか上げられなかったので、結果はあまり大きくなかったです。後から考えれば、これは驚くほどのことではありません。何故かと言いますと、システムの中のニューラルネットワークは様々な種類の層で作られるのに、深さ単位分離可能畳み込み層の使用は畳み込み層にしかできません。その結果、他の種類の層は全く効率化されませんでした。</p>

    <p>結論として効率化手法を両方導入することで、システムのデータ容量を18倍縮小し、必要なGPUのRAM量を3倍低下し、そして少し処理スピードを上げることが出来ます。なので、両方とも用いるのがお勧めです。</p>

    <h2>5 &nbsp; 結果</h2>

    <p>システムを使って、72人のVTuberと関連のキャラクターの画像からアニメーションを作りました。その中の16人のアニメーションは以下の図で確認出来ますが、他のキャラクターのアニメーションは<a href="#eyecatcher">アイキャッチャー</a>にあります。</p>    
  </div>

  <div class="container" style="max-width: 1024px;">
    <p >
      <table align="center">
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/tsukino_mito/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kuzuha/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/sasaki_saku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/yashiro_kizuku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/otogibara_era/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suzuhara_lulu/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_sango/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kitakoji_hisui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shirakami_fubuki/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/minato_aqua/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/amane_kanata/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/houshou_marine/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/inaba_haneru/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_patra/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shigure_ui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kagura_mea/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>図5.1：</b>「standard-float」のシステムのバリエーションで作られた16人のVTuberのアニメーション。
          </td>
        </tr>
      </table>      
    </p>    
  </div>  
  
  <div class="container" style="max-width: 640px;">
    <p></p>

    <p>去年作ったシステムと同様、 GUI操作で画像をコントロールできるツールを作成しました。</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/SETaasUhVXY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>また、<a href="https://www.ifacialmocap.com/">iFacialMocap</a>というiOSアプリケーションで取り込んだ人間の動きを、アニメのキャラクターに反映するツールも作りました。      

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/OO8aykBkxsA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <h2>6 &nbsp; 失敗事例</h3>

    <p>私のシステムはある程度綺麗なアニメーションを生成することが出来ますが、入力画像が学習データに似てないと、変な動きや歪みが生成される可能性があります。具体的に言いますと、大きい帽子の扱いが苦手のようです。</p>
  </div>

  <div class="container" style="max-width: 1024px">
    <table align="center">
      <tr>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nui_sociere/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/millie_parfait/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nina_kosaka/video_for_web.mp4" type="video/mp4">
          </video>
        </td>
      </tr>
      <tr>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCUc8GZfFxtmk7ZwSO7ccQ0g">ニュイ・ソシエール</a><br>(&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UC47rNmkDcNgbOcM-2BwzJTQ">ミリー・パフェ</a><br>(&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCkieJGn3pgJikVW8gmMXE2w">狐坂ニナ</a><br>(&copy; ANYCOLOR, Inc.)
        </td>
      </tr>
    </table>
    <br>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>頭の近くにある細いものを削除する傾向もあります。細い髪の毛は消されても大きな問題はありませんが、天使の輪など常に見える装飾物の場合は間違いが目立ちます。</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ninomae_inanis/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ouro_kronii/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>        
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCMwGHR0BTZuLsmjY_NT5Pwg">一伊那尓栖</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/c/OuroKroniiCh">オーロ・クロニー</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
      </table>
    </p>
  
    <p>他に、学習データには武器や楽器等の道具が少ないため、それらを正しく変形することが出来ません。</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/gawr_gura/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/mori_calliope/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>          
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCoSrY_IQQVpmIRZ9Xf-y93g">がうる・ぐら</a>(&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCL_qhgtOy0dy1Agp8vkySQg">森カリオペ</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/kazama_iroha/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/failure_cases/rikka/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC_vMYWcDjmfdpH6r4TTn1MQ">風真いろは</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC9mf_ZVpouoILRY9NUIaK-w">律可</a>
            (&copy; Cover corp.)
          </td>
        </tr>
      </table>
    </p>

    <a name="sec:conclusion"></a>
    <h2>7 &nbsp; 結論</h2>

    <p>一枚のキャラクター画像からアニメーションを生成するシステムを改善する試みについて述べました。一つのサブネットワークを作り直すことで、システムは体の動きを生成できるようになり、プロLive2Dモデラーによるモデルの機能に一歩近づきました。デザインが簡単なキャラクターは綺麗に動かせますが、大きい帽子、武器、楽器、頭の周りの細い装飾物等の学習データにあまり存在しない要素があったら、目立つ間違いを出力してしまう傾向があります。</p>

    <p>新しい機能を追加する他に、システムの効率化手法(深さ単位分離可能畳み込み層と「half」)を2つ試しました。両方導入することで、システムのデータ容量を18倍縮小し、必要RAM量を3～4倍低下させ、そして少し処理スピードを上げることが出来ました。必要メモリが少なくなったシステムは処理能力の低いコンピュータで実行しやすくなりましたが、処理スピードをさらに向上しなければなりません。</p>

    <p>最後に、この記事は研究レポートの簡略版で、完全版(英語)は<a href="full.html">このリンク</a>でご覧できます。完全版には問題の仕様、ネットワークの学習過程、既存研究との比較、ネットワーク構造の評価等の内容が書いてあります。気になる点がありましたら読んでみてください。</p>

    <h2>8 &nbsp; お断り</h2>

    <p>私自身はグーグル合同会社でソフトウェアエンジニアをやらせていただいてますが、このプロジェクトは仕事と全く関係ありません。会社のリソースを使わずに自由時間でやった趣味に過ぎません。グーグルに就職する前は大学院でコンピュータグラフィックスを研究したことはありますが、今は普通のソフトウェアエンジニアとしてグーグルマップのバックエンドを開発して生計を立てています。この記事に書いてある見解は私個人のものであり、会社の見解ではありません。</p>

    <h2>9 &nbsp; スペシャルサンクス</h2>

		<p><a href="https://www.linkedin.com/in/andrewluchen">Andrew Chen</a>様、<a href="https://ekapolc.github.io/">Ekapol Chuangsuwanich</a>様、<a href="https://aixile.github.io/">Yanghua Jin</a>様、<a href="https://minjun.li/">Minjun Li</a>様、Uehara Mamoru様、<a href="https://twitter.com/alitaso346">Alice Maruyama</a>様、<a href="https://ppasupat.github.io/">Panupong Pasupat</a>様、<a href="https://alantian.net/">Yingtao Tian</a>様、<a href="https://puchupala.com/">Pongsakorn U-chupala</a>様に感謝申し上げます。</p>

    <hr>
    <b>更新履歴</b>
    <ul>
        <li>2022年6月6日：初公開。</li>
        <li>2022年7月21日：日本語版公開。</li>
    </ul>

    <hr>
    <p align="right"><font size="1">Project <a href="https://en.wikipedia.org/wiki/Tagetes">Marigold</a></font></p>
    </div>
    <script src="js/bootstrap.bundle.min.js"></script>    
    <script>
    $.bigfoot();
  </script>
  </body>  
</html>
