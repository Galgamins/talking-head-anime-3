<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Talking Head(?) Anime from a Single Image 3: Now the Body Too (Abridged Version)</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">    
    <link href="css/theme.css" rel="stylesheet">

    <!-- MathJax -->
    <script>
    MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  svg: {
	    fontCache: 'global'
	  }
	};
    </script>
    <script src="mathjax/tex-chtml.js" id="MathJax-script" async></script>    
    <script type="text/javascript" src="js/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="js/bigfoot.min.js"></script>
    <link rel="stylesheet" type="text/css" href="css/bigfoot-default.css">
  </head>

  <body>
    <!--
  	 <nav class="navbar navbar-expand-md navbar-light fixed-bottom bg-light">
      <div class="collapse navbar-collapse justify-content-center" id="navbarCollapse">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#">&nbsp; &nbsp; Top</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Abstract</a>
          </li>          
          <li class="nav-item">
            <a class="nav-link" href="#background">Background</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#overview">Overview</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#problem-spec">Problem Spec</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#data">Data</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#networks">Networks</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#results">Results</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#conclusion">Conclusion</a>
          </li>
        </ul>        
      </div>
    </nav>
    -->
    
    <div class="container" style="max-width: 640px;">      
    	<span style="visibility: hidden;">
	      \(
	      \def\sc#1{\dosc#1\csod}
	      \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\ves}[1]{\boldsymbol{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \DeclareMathOperator*{\argmax}{arg\,max}
        \DeclareMathOperator*{\argmin}{arg\,min}        
	      \)
    	</span>
      <h1 align="center">&nbsp;</h1>
      <h1 align="center">Talking Head(?) Anime<br>from a Single Image 3:<br>Now the Body Too</h1>
      <p align="center">
      <a href="http://pkhungurn.github.io/">Pramook Khungurn</a></a>
      </p>
      <h1 align="center">&nbsp;</h1>
    </div>

  <p>
  <a name="eyecatcher"></a>
  <div align="center">
		<video id="eyecatcher" autoplay muted playsinline loop>
		    <source src="data/eyecatcher.mp4" type="video/mp4">
		</video>
		<br/>
		The characters are corporate/independent virtual YouTubers. Images and videos in this article are their fan arts. <a href="#fn_eyecatcher_footnote" rel="footnote">[footnote]</a>
	</div>
  </p>

  <div class="footnotes"><ul>
    <li class="footnote" id="fn_eyecatcher_footnote">					
      <p align="left">Most virtual YouTubers are affiliated with <a href="https://www.anycolor.co.jp">ANYCOLOR, Inc.</a>, <a href="https://cover-corp.com/">cover corp</a>, <a href="https://www.774.ai/">774 inc.</a>, and <a href="https://twitter.com/noriopro">Noripuro</a>. The rest are independent. Copyrights of the images belong to their respective owners. </p>
    </li>
  </ul>
  </div>

	<div class="container" style="max-width: 640px;">
    <h1>&nbsp;</h1>
		<a name="abstract"></a>
		<p><b>Abstract.</b> I present my third iteration of a neural network system that can animate an anime character, given just only one image of it. While the <a href="http://pkhungurn.github.io/talking-head-anime-2">previous iteration</a> can only animate the head, this iteration can animate the upper body as well. In particular, I added the following three new types of movements.</p>
  
    <p>
      <table align="center">
        <tr>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_y.mp4" type="video/mp4">
            </video>          
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_z.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_breathing.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td align="center">Body rotation<br> around the $y$-axis</td>
          <td align="center">Body rotation<br> around the $z$-axis</td>
          <td align="center" valign="top">Breathing</td>
        </tr>
      </table>
    </p>

    <p>
      With the new system, I updated an existing tool of mine that can transfer human movement to anime characters. The following is what it looks like with the expanded capabilities.
    </p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/TScnh3XC_14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>I also experimented with making the system smaller and faster. I was able to reduce significatly reduce its memory requirement (18 times reduction in size and 3 times reduction in RAM usage) and made it slightly faster while incurring little deterioration in image quality.</p>
		
    <h1>&nbsp;</h1>
	<h2>1 &nbsp; Introduction</h2>

    <p>Since 2019, I have been doing personal projects with the goal of making it much easier to become a virtual YouTuber (VTuber). In particular, I created a neural network system that can animate the face of any existing anime character, given only its <a href="https://en.wikipedia.org/wiki/Head_shot">head shot</a> as input. While my system could rotate the face and generate rich facial expressions, it is still far from practical as a streaming and/or content creation tool because of several problems. In this article, I report my attempt to solve the following two.</p>
    
    <ol>
        <li><b>Lack of body movement.</b> All movement my system can generate is limited to the head. However, a typical professionally-made VTuber model can rotate the upper body to some extent. It also features a breathing motion in which the chest or the entire upper body would rhythmically wobble up and down.</li>

        <li><b>High resource requirement.</b> The system is about 500 MB in size and requires a powerful desktop GPU to run.</li>
    </ol>
    
    <p>For the first problem, I extended the system to enable handling of larger images that contain a character's whole upper body. To match the possible movements of professionally-made VTuber models, I added the three types of movements depicted in the abstract.</p>
    
    <p>For the second problem, I experimented with two techniques to optimize my neural networks: (1) using depthwise separable convolutions <a href="#fn_sifre_2014" rel="footnote">[Sifre 2014]</a> instead of standard convolutions and (2) representing numbers with the 16-bit floating point type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>) instead of the standard 32-bit one (aka float). Employing both techniques together, I reduced the system's size by 18 times and RAM usage by 3 to 4 times. The techniques also also made the system slighly faster.</p>

    <div class="footnotes"><ul>        
      <li class="footnote" id="fn_sifre_2014">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>
    </ul></div>

    <a name="background"></a>
	<h2>2 &nbsp; Background</h2>

    <p>The starting point for this iteration is  <a href="http://pkhungurn.github.io/talking-head-anime-2/index.html">the system I created in 2021</a>.  It takes as input (1) an image of a character's head and (2) a vector called the <b>pose vector</b> that specifies the character's desired pose. It then outputs another image of the same character, now posed accordingly. The system consists of two main subnetworks. The aptly-named <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#face-morpher">face morpher</a> changes facial expression, and the also aptly-named <a href="http://pkhungurn.github.io/talking-head-anime/index.html#face-rotator">face rotator</a> rotates the face.</p>

    <p align="center">
      <a name="fig:old-system-overview" />
      <table align="center">
          <tr>
              <td align="center">
                  <a href="data/overview_old_two_step_process.png"><img src="data/overview_old_two_step_process.png" width="600"></a>
              </td>
          </tr>
          <tr>
              <td align="center">
                  <b>Figure 2.1</b> 
                  My old neural network system.
              </td>
          </tr>
      </table>
    </p>

    <p>I created a dataset to train the system by rendering 3D models created for an animation software called <a href="https://sites.google.com/view/vpvp/">MikuMikuDance</a> (MMD). To do so, I manually downloaded and annotated a collection of around 8,000 MMD models. Details of this process can be found <a href="https://pkhungurn.github.io/talking-head-anime/index.html#dataset">here</a> and <a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#data">here</a>.</p>



   <a name="moving-the-body"></a>
	<h2>3 &nbsp; Moving the Body</h2>

    <p>I need to redesign my system because the input specification has changed. The input image now shows a character's whole upper body, not just the head. It is also larger ($512 \times 512$) than what the last system works with ($256 \times 256$). While I kept face morpher the same, I replaced the face rotator with a new network called the <b>body rotator</b>. It is responsible for (1) rotating both the head and the body and (2) generating the breathing motion.</p>

    <p>
        <a name="fig:new-system-overview" />
        <table>
          <tr>
            <td align="center">
              <a href="data/overview_new_two_step_process.png">
                <img src="data/overview_new_two_step_process.png" width="600">
              </a>
            </td>
          </tr> 
          <tr>
            <td align="center">
              <b>Figure 3.1</b> The new version of the system.
            </td>
          </tr>
        </table>
    </p>

    <h3>3.1 &nbsp; Body Rotator's Architecture</h3>

    <p>The body rotator is made of two subnetworks. The <b>half-resolution rotator</b> solves the exact same problem as the network it is a part of. However, it operates on the input image after being scaled down to $256 \times 256$, and its outputs are also of the same resolution. Some of the outputs are resized back to $512 \times 512$ and fed to the <b>editor</b>, which refines and combines them into the final output image</p>
  </div>

  <div class="container" style="max-width: 1024px"></div>
    <p>
        <table align="center">
        <tr>
            <td>
            <a href="data/overview_body_rotator.png">
                <img src="data/overview_body_rotator.png" alt="" width="1024">
            </a>
            </td>
        </tr>
        <tr>
            <td align="center"><b>Figure 3.1.1</b> The overall architecture of the body rotator.</td>
        </tr>
        </table>
    </p>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>I employed the above "rotate-at-half-resolution-and-then-refine" design to make the body rotator as fast as the face rotator from the last project. Because the body rotator's input (a $512 \times 512$ image) has 4 times more pixels than that of the face rotator (a $256 \times 256$ image), it can be 4 times slower if the same architecture is used. By scaling the input down to $256 \times 256$ first, I can reuse the old architecture without any performance penalty. After the half-resolution rotator, the problem becomes much easier as we only need to refine its "coarse" outputs. The editor can thus be much smaller and consequently run as fast as the half-resolution rotator despite operating on images that are 4 times larger. Indeed, while the half-resolution rotator is about 130 MB in size, the editor is only about 30 MB.</p>

    <p>The half-resolution rotator has an encoder-decoder main body, which converts the input image and the pose vector to a feature tensor. The feature tensor is then used to generate two output images through two mechanisms.</p>

    <ol>
      <li><b>Direct generation.</b> The feature tensor is fed to a $3 \times 3$ convolution unit and then a hyperbolic tangent nonlinearity to directly produce an image.</li>

      <li><b>Warping.</b> From the feature tensor, we generate an <i>appearance flow</i>: a map which indicates, for each pixel in the output image, which pixel in the input image color should be copied from. Applying it thus warps the input image by moving its pixel around. This technique comes from Zhou et al.'s ECCV 2016 paper <a href="#fn_zhou_2016">[Zhou et al. 2016]</a>.</li>
    </ol>

    <div class="footnotes">
			<ul>				
				<li class="footnote" id="fn_zhou_2016">
					Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros.
					<b>View Synthesis by Appearance Flow.</b> 
					ECCV 2016. 
					<a href="https://arxiv.org/abs/1605.03557">[arXiv]</a>
				</li>
			</ul>
		</div>

    <a name="fig:half-res-rotator" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/half_res_rotator.png">
              <img src="data/half_res_rotator.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.1.2</b> The half-resolution rotator.
          </td>
        </tr>
      </table>
    </p>

    <p>However, the network actually produces three outputs because I also made the warping mechanism output the appearance flow offset <a href="#fn_appearance_flow_offset">[footnote]</a>  used to derive the appearance flow. Of the three outputs, the image from direct generation is used only at training time and so is always discarded at inference time. On the other hand, the warped image and the appearance flow offset are scaled up and then passed to the editor.</p>
    
    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_appearance_flow_offset">An <b>appearance flow offset</b> is the difference between an appearance flow and the flow $F_\mrm{identity}$ that preserves all pixels.
        
        <p>In general, it is easier to have a network generate a flow offset than a flow. For example, consider a flow that rotates the body by a very small degree. The flow offset would be close to zero everywhere, but the flow would be very similar to $F_\mrm{identity}$. Now, $F_\mrm{identity}$ contains pixel locations with perfectly regular spacing. It is thus clearly more complicated than a zero tensor.</p>

        <p>Because of the above reason, my warping mechanism always generates an apperance flow offset before adding $F_\mrm{identity}$ to form the complete appearance flow.</p>
        </li>
      </ul>
    </div>

    <p>The editor's main body is a U-Net <a href="#fn_ronneberger_2015_0">[Ronneberger et al. 2015]</a> instead of an encoder-decoder. The feature tensor produced by the U-Net is fed to two of image processing steps.</p>

    <div class="footnotes">
      <ul>
          <li class="footnote" id="fn_ronneberger_2015_0">
              <p align="left">
              Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
              <b>U-Net: Convolutional Networks for Biomedical Image Segmentation.</b>
              Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015.
              <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">[Project]</a>
              <a href="https://arxiv.org/abs/1511.06702">[arXiv]</a>
              </p>
          </li>
      </ul>
    </div>

    <ol>
      <li><b>Warping the input image.</b> From the feature tensor, a new appearance flow offset is created. It is then added to the input appearance flow offset, and the result is then used to warp the original $512 \times 512$ input image.</li>
      <li><b>Partially change the warped image.</b> Again, from the feature tensor, we generate (1) an image that represents change to the output of the previous step and (2) an alpha mask that tells where the change should be applied <a href="#fn_pumarola_2018">[Pumarola et al. 2018]</a>. We then perform an <a href="https://en.wikipedia.org/wiki/Alpha_compositing">alpha blending</a> between the change image and the warped image and treat the result as the output of the editor.</li>
    </ol>

    <div class="footnotes">
			<ul>
				<li class="footnote" id="fn_pumarola_2018">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
			</ul>
		</div>

    <a name="fig:editor" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/editor.png">
              <img src="data/editor.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.1.3</b> The editor.
          </td>
        </tr>
      </table>
    </p>

    <p>The architecture I just described has a number of unusual design choices. First, it seems wasteful for the half-resolution rotator to produce the directly generated image just only for it to be discarded. Second, the editor could have been simpler. For example, it can directly generate the output image instead of using the two image processing steps above. These choices were the result of evaluating multiple design alternatives and choosing the best performing one. Details can be found in the <a href="full.html#sec:ablation-study">full version</a> of this write-up.</p>

    <h3>3.2 &nbsp; Dataset</h3>

    <p>Training the body rotator requires a brand new dataset because <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#data">the previous one</a> only contain images of characters' heads, not their upper bodies. I still generate training data by rendering MMD models, and I reused the collection of 8,000 models mentioned earlier. There are some notable changes to how I generated the data.</p>

    <p><b>Input poses.</b> In the previous dataset, the input image must depict a character in the "rest pose."</p>

    <p>
      <a href="#fig:rest-pose"></a>
      <table align="center">
        <tr>
          <td align="center">
            <a href="data/rest_pose.png"><img src="data/rest_pose.png" alt="" width="240"></a>
          </td>         
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.2.1</b> The rest pose. The character is <a href="https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA">Kizuna AI</a> (© Kizuna AI).
          </td>
        </tr>
      </table>
    </p>

    <p>However, my goal is to have a system that works on drawings, and it is very hard to find ones in the wild where the character is exactly in the rest pose. To make the system more versatile, I allowed the input pose to vary. In general, the head and the body can be rotated by a few degrees. The arms can be posed rather freely, but they should generally be below and far from the face.</p>

    <p>
      <a name="fig:sample_inputs" />
      <table align="center">
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/00.png">
              <img src="data/kizuna_ai_sample_inputs/00.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/01.png">
              <img src="data/kizuna_ai_sample_inputs/01.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/02.png">
              <img src="data/kizuna_ai_sample_inputs/02.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/03.png">
              <img src="data/kizuna_ai_sample_inputs/03.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/04.png">
              <img src="data/kizuna_ai_sample_inputs/04.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/05.png">
              <img src="data/kizuna_ai_sample_inputs/05.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/06.png">
              <img src="data/kizuna_ai_sample_inputs/06.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/07.png">
              <img src="data/kizuna_ai_sample_inputs/07.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>Figure 3.2.2</b> Examples of valid input poses.
          </td>
        </tr>
      </table>
    </p>

    <p>To generate such input poses, I downloaded pose data created for MMD models from web sites such as <a href="https://www.nicovideo.jp/">Nico Nico</a> and <a href="https://bowlroll.net/">BowlRoll</a>. I ended up collecting 4,731 poses in total and identified 832 usable ones. An input pose is created by blending one of these 832 poses with a simple "rest-like" pose. Details of this process can be found in the <a href="full.html#sec:input-pose">full write-up</a>.</p>

    <p><b>Dealing with long-haired characters.</b> In an MMD model, when one rotates the head, the whole hair mass would also rotate with it as can be seen in the video below.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=360 height=360 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v3.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>Figure 3.2.3</b> The result of rotating the neck bone of an MMD model around the $z$-axis. Notice that the whole hair mass moves like a single rigid body, following the head. The character is <a href="https://www.youtube.com/channel/UCeLzT-7b2PBcunJplmWtoDg">Suou Patra</a> (&copy; HoneyStrap), and the 3D model was created by <a href="https://3d.nicovideo.jp/works/td60253">OTUKI</a>.
          </td>
        </tr>
      </table>
    </p>

    <p>This behavior, however, makes it very hard for the system to animate characters with long hair. First, it must correctly identify which part of the input image is the hair and which is not; otherwise, disturbing errors such as severed body parts or clothing might show up. Second, it must also correctly hallucinate the hair mass that becomes visible because of the rotation.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <img width=300 height=300 src="data/failure_cases/enomiya_milk/enomiya_milk.png" alt="">
          </td>
          <td align="center">
            <video width=300 height=300 muted autoplay playsinline loop>
              <source src="data/failure_cases/enomiya_milk/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="2">
            <b>Figure 3.2.4</b> To generate the video on the right, I used a model to animate the image on the left, but it was trained on a dataset where the whole hair mass moves with the head like in Figure 3.2.3. In the bottom half of the video, we can see that the details of the hair strands are lost. Moreover, the model seemed to think that the character's hands were a part of the hair, so it cut the fingers off when the hair moved. The character is <a href="https://www.youtube.com/channel/UCJCzy0Fyrm0UhIrGQ7tHpjg">Enomiya Milk</a> (&copy; Noripro).
          </td>
        </tr>
      </table>      
    </p>
    
    <p>While it may be possible to do correctly segment and hallucinate with current machine learning techniques, I realized that it was much easier to avoid doing so and still generate plausible and beautiful animations. I modified the <a href="https://web.stanford.edu/class/cs248/pdf/class_13_skinning.pdf">skinning algorithm</a> for MMD models so that the neck and the head bones can only influence parts that are not too far below the neck. The effect can be seen below.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=360 height=360 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v4.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>Figure 3.2.5</b> Hair movement after limiting the influence of the neck and the head bones. We can see that the hair mass below the shoulders does not move at all, and this make the system's job much easier.
          </td>
        </tr>
      </table>      
    </p>

    <p>To summarize, extending the system to handle upper body movements requires designing a new subnetwork, the body rotator, and preparing a new dataset to train it. To handle larger input images, I designed the body rotator so that it would process them at a lower resolution first and then upsample and refine the results. The new dataset was augmented to have a variety of input poses. Moreover, when posing an MMD model for data generation, I limited the influence of head movement on hair in order to avoid solving a hard segmentation problem when animating long-haired characters.</p>

    <h2>4 &nbsp; Improving Efficiency</h2>

    <p>My system is resource hungry. It is about 517 MB in size, and it takes about 35 ms to process an image end to end using my Titan RTX graphics card. The large size makes it hard to deploy the system on mobile devices, and processing speed would only worsen when using less powerful GPUs. It is thus crucial to make the system use less memory and faster in order to improve its versatility. To do so, I experimented with two techniques.</p>

    <ol>
      <li>
        <b>Depthwise separable convolutions.</b> All networks in my system are <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks</a> (CNNs), meaning that their main building blocks are convolution layers. To improve the networks' efficiency, I replaced all convolution layers in their main bodies (i.e., the encoder-decoder networks and the U-Nets) with two smaller convolution layers that are applied in succession. The first is a "depthwise" convolution layer that treats each channel of the input tensor independently. The second is a $1 \times 1$ convolution layer that mixes the channels together <a href="#fn_sifre_2014_2">[Sifre 2014]</a>. In theory, this technique should reduce the size of each network by a factor of 9, but it might not reduce the amount of RAM the network uses. In the best case, the network can become 9 times faster as well. (You can see the reasoning for these predictions in <a href="full.html#sec:improving-efficiency">the full write-up</a>.)<br><br>
      </li>
      <li>
        <b>Using "half."</b> I implemented my network with <a href="http://pytorch.org">PyTorch</a>, and it uses the 32-bit floating point type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html">float</a>) to represent almost all data and parameters. Nevertheless, the numerical precision afforded by float might not be necessary, and so PyTorch features an option to use the 16-bit floating point type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>) instead of float. While I am not sure a priori how much using half can improve the processing speed, there's the obvious benefit that the system's size and RAM usage would be reduced in half.
      </li>
    </ol>

    <div class="footnotes"><ul>
      <li class="footnote" id="fn_sifre_2014_2">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>      
    </ul></div>

    <p>I created four variants of the system based on whether depthwise separable convolutions are used and whether half is used instead of float. If a variant uses depthwise separable convolutions, it is designated with the word "separable;" otherwise, it is designated with the word "standard." A variant is also designated with the floating point type it uses. As a result, there are 4 variants: "standard-float," "separable-float," "standard-half," and "separable-half."</p>

    <h3>4.1 &nbsp; System Size</h3>

    <p>The clearest benefit of the techniques is size reduction. As predicted, using depthwise separable convolution reduced the size in MB by a factor of about 9, and using half cut it even further in half.</p>

    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td align="center" colspan="4"><b>Size in MB (and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>          
          <tr>
            <td align="right">
              <tt>
                517.56
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                55.48
              </tt>
              <br />
              (9.33x)
            </td>
            <td align="right">
              <tt>
                258.84
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                27.82
              </tt>
              <br />
              (18.60x)
            </td>
          </tr>
        </tbody>
      </table>      
    </p>

    <h3>4.2 &nbsp; RAM Usage</h3>

    <p>I measured the amount of GPU RAM used by the variants as they processed a pair of input image and a pose vector. I conducted the experiments on three computers.</p>

    <ul>
      <li><b><i>Computer A</i></b> is a desktop PC with an Nvidia Titan RTX GPU, a 3.60 GHz Intel Core i9-9900KF CPU, and 64 GB of RAM. It represents a high-end gaming PC.</li>

      <li><b><i>Computer B</i></b> is a desktop PC with an Nvidia GeForce GTX 1090 Ti GPU, a 3.70 GHz Intel Core i7-8700K CPU, and 32 GB of RAM. It represents a typical (yet somewhat outdated) gaming PC.</li>

      <li><b><i>Computer C</i></b> is a laptop with an Nvidia GeForce MS250, a 1.19 GHz Intel Core i5-1035G1 CPU, and 8 GB of RAM. It represents a general PC with low processing power.</li>
    </ul>

    <p>The numbers are available in the table below.</p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>RAM usage in MB (and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Computer</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              Computer A
            </td>
            <td align="right">
              <tt>
                816.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                417.49
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                467.39
              </tt>
              <br />
              (1.75x)
            </td>
            <td align="right">
              <tt>
                274.80
              </tt>
              <br />
              (2.97x)
            </td>
          </tr>
          <tr>
            <td>
              Computer B
            </td>
            <td align="right">
              <tt>
                816.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                417.49
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                417.39
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                210.80
              </tt>
              <br />
              (3.88x)
            </td>
          </tr>
          <tr>
            <td>
              Computer C
            </td>
            <td align="right">
              <tt>
                813.91
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                414.08
              </tt>
              <br />
              (1.97x)
            </td>
            <td align="right">
              <tt>
                415.89
              </tt>
              <br />
              (1.96x)
            </td>
            <td align="right">
              <tt>
                209.30
              </tt>
              <br />
              (3.89x)
            </td>
          </tr>
        </tbody>
      </table>
    </p>
  </div> 

  <div class="container" style="max-width: 640px;">
    <p>When applied separately, each technique reduced memory consumption by factors of about 2. When applied together, they reduced RAM usage by approximately 3 to 4 times.</p>

    <h3>4.3 &nbsp; Speed</h3>

    <p>I also measured the average time it took the variants to process an input pair.</p>    
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>Average processing time in milliseconds<br>(and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Computer</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              Computer A
            </td>
            <td align="right">
              <tt>
                34.105
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                26.777
              </tt>
              <br />
              (1.27x)
            </td>
            <td align="right">
              <tt>
                23.803
              </tt>
              <br />
              (1.43x)
            </td>
            <td align="right">
              <tt>
                24.540
              </tt>
              <br />
              (1.39x)
            </td>
          </tr>
          <tr>
            <td>
              Computer B
            </td>
            <td align="right">
              <tt>
                43.841
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                46.959
              </tt>
              <br />
              (0.93x)
            </td>
            <td align="right">
              <tt>
                38.019
              </tt>
              <br />
              (1.15x)
            </td>
            <td align="right">
              <tt>
                38.848
              </tt>
              <br />
              (1.13x)
            </td>
          </tr>
          <tr>
            <td>
              Computer C
            </td>
            <td align="right">
              <tt>
                690.751
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                335.364
              </tt>
              <br />
              (2.06x)
            </td>
            <td align="right">
              <tt>
                1125.345
              </tt>
              <br />
              (0.61x)
            </td>
            <td align="right">
              <tt>
                385.041
              </tt>
              <br />
              (1.79x)g
            </td>
          </tr>
        </tbody>
      </table>
    </p>
  </div>  

  <div class="container" style="max-width: 640px;">
    <p>The techniques generally had positive impact on speed. However, the effect was not significant as the largest improvement I could observe was only by a factor of about 2. This is perhaps obvious in hindsight because, in addition to convolution layers, the networks also contained other layer types, and these were not optimized at all.</p>

    <p>In conclusion, employing both technques decreased the system's size by a significant factor of 18, reduced its GPU RAM usage by a factor of about 3, and also made it slightly faster. As a result, one should always apply the techniques together.</p>

    <h2>5 &nbsp; Results</h2>

    <p>I used the system to animate 72 images of VTubers and related characters. Sixteen resulting videos are available below. The rest can be seen in the <a href="#eyecatcher">eyecatcher</a>.</p>
  </div>

  <div class="container" style="max-width: 1024px;">
    <p >
      <table align="center">
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/tsukino_mito/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kuzuha/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/sasaki_saku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/yashiro_kizuku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/otogibara_era/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suzuhara_lulu/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_sango/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kitakoji_hisui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shirakami_fubuki/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/minato_aqua/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/amane_kanata/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/houshou_marine/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/inaba_haneru/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_patra/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shigure_ui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kagura_mea/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>Figure 5.1</b> The result of using the system (the "standard-float" variant) to animate 16 drawn characters.
          </td>
        </tr>
      </table>      
    </p>    
  </div>  
  
  <div class="container" style="max-width: 640px;">
    <p>As was done in my previous project, I created a tool that allowed the user to control drawings by manipulating GUI elements.</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/9BzdxrYVSrs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>I also created another tool that could transfer a human's movement, captured by an iOS application called <a href="https://www.ifacialmocap.com/">iFacialMocap</a>, to anime characters.</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/TScnh3XC_14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <h2>6 &nbsp; Failure Cases</h3>

    <p>My system is capable of generating good-looking animations when applied to many different characters. However, it can yield implausible results when fed inputs that deviate significantly from the training set. In particular, it does not seem to handle large hats well.</p>
  </div>

  <div class="container" style="max-width: 1024px">
    <table align="center">
      <tr>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nui_sociere/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/millie_parfait/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nina_kosaka/video_for_web.mp4" type="video/mp4">
          </video>
        </td>
      </tr>
      <tr>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCUc8GZfFxtmk7ZwSO7ccQ0g">Nui Sociere</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UC47rNmkDcNgbOcM-2BwzJTQ">Millie Parfait</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCkieJGn3pgJikVW8gmMXE2w">Nina Kosaka</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
      </tr>
    </table>
    <br>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>It also has a tendency to erase thin structures around the head. While this might be acceptable for thin hair filaments, the result can be very noticeable for halos and rigid ornaments that should always be present.</p>
  
    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ninomae_inanis/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ouro_kronii/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>        
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCMwGHR0BTZuLsmjY_NT5Pwg">Ninomae Ina'nis</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/c/OuroKroniiCh">Ouro Kronii</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
      </table>
    </p>
  
    <p>Moreover, it cannot correctly deal with props such as weapons and musical instruments because, unfortunately, the training data contain very few examples with such items.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/gawr_gura/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/mori_calliope/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>          
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCoSrY_IQQVpmIRZ9Xf-y93g">Gawr Gura</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCL_qhgtOy0dy1Agp8vkySQg">Mori Calliope</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/kazama_iroha/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/failure_cases/rikka/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC_vMYWcDjmfdpH6r4TTn1MQ">Kazama Iroha</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC9mf_ZVpouoILRY9NUIaK-w">Rikka</a>
            (&copy; Cover corp.)
          </td>
        </tr>
      </table>
    </p>

    <a name="sec:conclusion"></a>
    <h2>7 &nbsp; Conclusion</h2>

    <p>In this article, I have discussed my attempt to improve my animation-from-a-single-image system. By changing one constituent network, I enabled the system to rotate the body and generate breathing motion, making its features closer to those offered by professionally-made Live2D models. The system outputs plausible animations for characters with simple designs, but it struggles on those with props not sufficiently covered by the training dataset. These include large hats, weapons, musical instruments, and other thin ornamental structures.</p>

    <p>I also explored making the system more efficient through using depthwise separable convolutions and the "half" type.  Employing both, I made the system 18 times smaller, descreased its GPU RAM usage by about 3 to 4 times, and also slightly improved its speed. While this makes it easier to deploy the system on less powerful devices, more research is needed to make it significantly faster.</p>

    <p>Lastly, note that this article is an abridged version of the <a href="full.html">full (and much longer) write-up</a>. The latter contains, among other things, more details on problem specification, training proces, comparison with other works, and evaluation of alternative architectures.</p>

    <h2>8 &nbsp; Disclaimer</h2>

    <p>While I am an employee of Google Japan, this project is my personal hobby which I did in my free time without using Google's resources. It has nothing to do with work as I am a normal software engineer writing Google Maps backends for a living. Moreover, I currently do not belong to any of Google's or Alphabet's research organizations. Opinions expressed in this article is my own and not the company's. Google, though, may claim rights to the article's technical inventions.</p>

    <h2>9 &nbsp; Special Thanks</h2>

		<p>I would like to thank <a href="https://www.linkedin.com/in/andrewluchen">Andrew Chen</a>, <a href="https://ekapolc.github.io/">Ekapol Chuangsuwanich</a>, <a href="https://aixile.github.io/">Yanghua Jin</a>, <a href="https://minjun.li/">Minjun Li</a>, <a href="https://ppasupat.github.io/">Panupong Pasupat</a>, <a href="https://alantian.net/">Yingtao Tian</a>, and <a href="https://puchupala.com/">Pongsakorn U-chupala</a> for their comments.</p>

    <hr>
    <b>Update History</b>
    <ul>
        <li>2022/06/06: First publication.</li>
    </ul>

    <hr>
    <p align="right"><font size="1">Project <a href="https://en.wikipedia.org/wiki/Tagetes">Marigold</a></font></p>
    </div>
    <script src="js/bootstrap.bundle.min.js"></script>    
    <script>
    $.bigfoot();
  </script>
  </body>  
</html>
