<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>Talking Head(?) Anime from a Single Image 3: Now the Body Too (Full Version)</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">    
    <link href="css/theme.css" rel="stylesheet">

    <!-- MathJax -->
    <script>
    MathJax = {
	  tex: {
	    inlineMath: [['$', '$'], ['\\(', '\\)']]
	  },
	  svg: {
	    fontCache: 'global'
	  }
	};
    </script>
    <script src="mathjax/tex-chtml.js" id="MathJax-script" async></script>    
    <script type="text/javascript" src="js/jquery-3.5.1.min.js"></script>
    <script type="text/javascript" src="js/bigfoot.min.js"></script>
    <link rel="stylesheet" type="text/css" href="css/bigfoot-default.css">
  </head>

  <body>
  	<nav class="navbar navbar-expand-md navbar-light fixed-bottom bg-light">
      <div class="collapse navbar-collapse justify-content-center" id="navbarCollapse">
        <ul class="navbar-nav">
          <li class="nav-item">
            <a class="nav-link" href="#">&nbsp; &nbsp; Top</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#abstract">Abstract</a>
          </li>          
          <li class="nav-item">
            <a class="nav-link" href="#background">Background</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#moving-the-body">
              Moving the Body
            </a>
          </li>          
          <li class="nav-item">
            <a class="nav-link" href="#sec:improving-efficiency">Improving Efficiency</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#sec:applications-to-drawings">Applications to Drawings</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#sec:related-works">Related Works</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#sec:conclusion">Conclusion</a>
          </li>
        </ul>        
      </div>
    </nav>
    
    <div class="container" style="max-width: 640px;">      
    	<span style="visibility: hidden;">
	      \(
	      \def\sc#1{\dosc#1\csod}
	      \def\dosc#1#2\csod{{\rm #1{\small #2}}}

        \newcommand{\dee}{\mathrm{d}}
        \newcommand{\Dee}{\mathrm{D}}
        \newcommand{\In}{\mathrm{in}}
        \newcommand{\Out}{\mathrm{out}}
        \newcommand{\pdf}{\mathrm{pdf}}
        \newcommand{\Cov}{\mathrm{Cov}}
        \newcommand{\Var}{\mathrm{Var}}

        \newcommand{\ve}[1]{\mathbf{#1}}
        \newcommand{\mrm}[1]{\mathrm{#1}}
        \newcommand{\ves}[1]{\boldsymbol{#1}}
        \newcommand{\etal}{{et~al.}}
        \newcommand{\sphere}{\mathbb{S}^2}
        \newcommand{\modeint}{\mathcal{M}}
        \newcommand{\azimint}{\mathcal{N}}
        \newcommand{\ra}{\rightarrow}
        \newcommand{\mcal}[1]{\mathcal{#1}}
        \newcommand{\X}{\mathcal{X}}
        \newcommand{\Y}{\mathcal{Y}}
        \newcommand{\Z}{\mathcal{Z}}
        \newcommand{\x}{\mathbf{x}}
        \newcommand{\y}{\mathbf{y}}
        \newcommand{\z}{\mathbf{z}}
        \newcommand{\tr}{\mathrm{tr}}
        \newcommand{\sgn}{\mathrm{sgn}}
        \newcommand{\diag}{\mathrm{diag}}
        \newcommand{\Real}{\mathbb{R}}
        \newcommand{\sseq}{\subseteq}
        \newcommand{\ov}[1]{\overline{#1}}
        \DeclareMathOperator*{\argmax}{arg\,max}
        \DeclareMathOperator*{\argmin}{arg\,min}        
	      \)
    	</span>
      <h1 align="center">&nbsp;</h1>
      <h1 align="center">Talking Head(?) Anime<br>from a Single Image 3:<br>Now the Body Too</h1>
      <p align="center">
      <a href="http://pkhungurn.github.io/">Pramook Khungurn</a></a>
      </p>
      <h1 align="center">&nbsp;</h1>
    </div>

  <p>
  <a name="eyecatcher"></a>
  <div align="center">
		<video id="eyecatcher" autoplay muted playsinline loop>
		    <source src="data/eyecatcher.mp4" type="video/mp4">
		</video>
		<br/>
		The characters are corporate/independent virtual YouTubers. Images and videos in this article are their fan arts. <a href="#fn_eyecatcher_footnote" rel="footnote">[footnote]</a>
	</div>
  </p>

  <div class="footnotes"><ul>
    <li class="footnote" id="fn_eyecatcher_footnote">					
      <p align="left">Most virtual YouTubers are affiliated with <a href="https://www.anycolor.co.jp">ANYCOLOR, Inc.</a>, <a href="https://cover-corp.com/">cover corp</a>, <a href="https://www.774.ai/">774 inc.</a>, and <a href="https://twitter.com/noriopro">Noripuro</a>. The rest are independent. Copyrights of the images belong to their respective owners. </p>
    </li>
  </ul>
  </div>

	<div class="container" style="max-width: 640px;">
    <h1>&nbsp;</h1>
		<a name="abstract"></a>
		<p><b>Abstract.</b> I present my third iteration of a neural network system that can animate an anime character, given just only one image of it. While the <a href="http://pkhungurn.github.io/talking-head-anime-2">previous iteration</a> can only animate the head, this iteration can animate the upper body as well. In particular, I added the following three new types of movements.</p>
  
    <p>
      <table align="center">
        <tr>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_y.mp4" type="video/mp4">
            </video>          
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_body_z.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=200 height=200 muted autoplay playsinline loop>
              <source src="data/parameter_animations/gibara_breathing.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td align="center">Body rotation<br> around the $y$-axis</td>
          <td align="center">Body rotation<br> around the $z$-axis</td>
          <td align="center" valign="top">Breathing</td>
        </tr>
      </table>
    </p>

    <p>
      With the new system, I updated an existing tool of mine that can transfer human movement to anime characters. The following is what it looks like with the expanded capabilities.
    </p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/TScnh3XC_14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>I also experimented with making the system smaller and faster. I was able to reduce significatly reduce its memory requirement (18 times reduction in size and 3-4 times reduction in RAM usage) and made it slightly faster while incurring little deterioration in image quality.</p>
		
    <h1>&nbsp;</h1>
		<h2>1 &nbsp; Introduction</h2>

    <p>Since 2018, I have been a fan of the <a href="https://en.wikipedia.org/wiki/VTuber">virtual YouTubers</a> (VTuber). In fact, I like them so much that, starting from 2019, I have been doing two personal AI projects whose aims were to make it much easier to become a VTuber. In the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html">2021 version of the project</a>, I created a neural network system that can animate the face of any existing anime character, given only its <a href="https://en.wikipedia.org/wiki/Head_shot">head shot</a> as input. My system lets users animate characters without having to create controllable models (either 3D models by using softare such as <a href="https://www.autodesk.com/products/3ds-max/overview">3ds Max</a>, <a href="https://www.autodesk.com/products/maya">Maya</a> or <a href="https://www.blender.org/">Blender</a>, or 2.5D ones by using software such as <a href="https://www.live2d.com/">Live2D</a> or <a href="http://esotericsoftware.com/">Spine</a>) beforehand. It has the potential to greatly reduce the cost of avatar creation and character animation.</p>

    <p>While my system can rotate the face and generate rich facial expressions, it is still far from practical as a streaming and/or content creation tool. One reason is that all movement is limited to the head. A typical VTuber model, however, can rotate the upper body to some extent. It also features a breathing motion in which the character's chest or the entire upper body would rhythmically wobble up and down even if the human performer is not actively controlling the character.</p>

    <p>The system also has another major problem: it is resource intensive. It is about 600 MB in size and requires a powerful desktop GPU to run. In order to enable usage on less powerful computers, I must optimize the system's size, memory usage, and speed.</p>

    <p>In this article, I report my attempt to address the above two problems.</p>
    
    <p>For the problem of upper body movement, I extended my latest system by adding 3 types of movements: rotation of the hip around the $y$-axis, rotation of the hip around the $z$-axis, and breathing. The new system can now animate the upper body in addition to the head, making its features close to those of professionally-made VTuber models. I accomplished this without significantly increasing the network size or processing time.</p>
    
    <p>For the problem of high resource requirements, I experimented with two techniques to optimize my neural networks. The first is using depthwise separable convolutions <a href="#fn_sifre_2014" rel="footnote">[Sifre 2014]</a> instead of standard convolution. The second is representing numbers with the 16-bit floating point type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>) instead of the 32-bit one (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html">float</a>). Using both techniques, I was able to reduce the system's size in bytes by a factor of 18 and GPU RAM usage by a factor of 3 to 4. The techniques also provided a small improvement to speed.</p>

    <div class="footnotes"><ul>        
      <li class="footnote" id="fn_sifre_2014">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>
    </ul></div>

    <a name="background"></a>
		<h2>2 &nbsp; Background</h2>

    <p>I created a deep neural network system whose purpose was to animate the head of an anime character. The system takes as input (1) an image of the character's head in the front facing configuration with its eyes wide open and (2) a 42-dimensional vector called the <b>pose vector</b> that specifies the character's desired pose. It then proceeds to output another image of the same character after being posed accordingly. The system can rotate the character's face by up to $15^\circ$ around three axes. Moreover, it can change the shapes of the eyebrows, eyelids, irises, and mouth, allowing the character to show various emotions and convincingly imitate (Japanese) speech.</p>

    <p>The system is largely decomposed into two main subnetworks. The <b>face morpher</b> is tasked with changing the character's facial expression, and its design is documented in the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#face-morpher">write-up of the 2021 project</a>. The <b>face rotator</b> is tasked with rotating the face, and its design is available in the <a href="http://pkhungurn.github.io/talking-head-anime/index.html#face-rotator">write-up of the 2019 project</a>. <a href="#fig:old-system-overview">Figure 2.1</a> illustrates how the networks are put together.</p>

    <p align="center">
      <a name="fig:old-system-overview" />
      <table align="center">
          <tr>
              <td align="center">
                  <a href="data/overview_old_two_step_process.png"><img src="data/overview_old_two_step_process.png" width="600"></a>
              </td>
          </tr>
          <tr>
              <td align="center">
                  <b>Figure 2.1</b> 
                  An overview of my neural network system.
              </td>
          </tr>
      </table>
    </p>

    <p>For this article, the face rotator is especially relevant because it is the network that I have to redesign in order to expand the system's capability. The network itself is made up of two subnetworks. The <b>two-algorithm face rotator</b> uses two image tranformation techniques to generate images of the character's rotated face, and the <b>combiner</b> merges the two generated images to create the final output.</p>

    <p align="center">
      <a name="fig:face-rotator" />
      <table align="center">
          <tr>
              <td align="center">
                  <a href="data/face_rotator.png"><img src="data/face_rotator.png" width="600"></a>
              </td>
          </tr>
          <tr>
              <td align="center">
                  <b>Figure 2.2</b> The face rotator.
              </td>
          </tr>
      </table>
    </p>

    <p>The two image tranformation techniques are:</p>

    <ul>
      <li><b>Partial image change.</b> We generate an image that represents change to the input image and an alpha mask that tells where the change should be applied. The technique thus preserves some parts of the input image while altering others. It is thus suitable for the face rotation problem because we only need to change the pixels belonging to the head. The technique comes from Pumarola et al.'s ECCV 2018 paper <a href="#fn_pumarola_2018">[Pumarola et al. 2018]</a>.</li>

      <li><b>Warping.</b> We generate an <i>appearance flow</i>: a map which indicates, for each pixel in the output image, from which pixel in the input image color should be copied. Applying it thus warps the input image by moving the pixels around. This technique comes from Zhou et al.'s ECCV 2016 paper <a href="#fn_zhou_2016">[Zhou et al. 2016]</a>.</li>
    </ul>

    <div class="footnotes">
			<ul>
				<li class="footnote" id="fn_pumarola_2018">
					Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer.
					<b>GANimation: Anatomically-aware Facial Animation from a Single Image.</b> 
					ECCV 2018. 
					<a href="https://www.albertpumarola.com/research/GANimation/">[Project]</a>
				</li>
				<li class="footnote" id="fn_zhou_2016">
					Tinghui Zhou, Shubham Tulsiani, Weilun Sun, Jitendra Malik, and Alexei A. Efros.
					<b>View Synthesis by Appearance Flow.</b> 
					ECCV 2016. 
					<a href="https://arxiv.org/abs/1605.03557">[arXiv]</a>
				</li>
			</ul>
		</div>

    <p>When the face is rotated by a few degrees, most changes to the input image can be thought of as moving existing pixels to new locations. Warping can thus handle these changes very well, and the generated image would be sharp because existing pixels would be faithfully reproduced. Nevertheless, warping cannot generate new pixels, which are needed when unseen parts of the head become visible after rotation. Partial image change, on the other hand, can generate new pixels from scratch, but they tend to be blurry. By combining both approaches <a href="#fn_combiner">[footnote]</a>, we can use pixels hallucinated by partial image change to fill areas that warping cannot handle, thus getting the best of both worlds.</p>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_combiner">
          See details in Section 6.2 of the <a href='http://pkhungurn.github.io/talking-head-anime/index.html'>write-up of the 2019 project</a>.
        </li>
      </ul>
    </div>

    <a name="moving-the-body"></a>
		<h2>3 &nbsp; Moving the Body</h2>

    <p>In this large section, I discuss how I extended my 2021 system so that it can move the body as well. I will start by defining exactly the problem I would like to solve (<a href="#sec:problem-spec">Section 3.1</a>). Then, I will give a brief overview of the whole system. In particular, I will discuss which networks from the previous projects I reused and which I created anew (<a href="sec:system-overview">Section 3.2</a>). Next, I will elaborate on how I generated the datasets to train the new networks (<a href="#sec:data">Section 3.3</a>). I will then describe the networks' architectures and training procedures (<a href="sec:networks">Section 3.4</a>), and lastly I will evaluate the networks' performance (<a href="sec:results">Section 3.5</a>).</p>

    <a name="sec:problem-spec"></a>
		<h3>3.1 &nbsp; Problem Specification</h3>

    <p>As with the previous version of the system, the new version in this article takes as input an image of a character and a pose vector. The image is now of resolution $512 \times 512$ in order to fully show the upper body. The character should be standing approximately upright, and the head should be roughly contained in the $128 \times 128$ box in the middle of the top half of the image.</p>

    <p align="center">
      <a name="fig:sample_input_with_size_legends" />
      <table align="center">
        <tr>
            <td align="center">
                <a href="data/kizuna_ai_sample_inputs/with_size_legends.png"><img src="data/kizuna_ai_sample_inputs/with_size_legends.png" width="600"></a>
            </td>
        </tr>
        <tr>
            <td>
                <b>Figure 3.1.1</b> A valid input to the new version of the system. The character is <a href="https://www.youtube.com/channel/UC4YaOt1yT-ZeyB0OmxHgolA/videos">Kizuna AI</a> (© Kizuna AI).
            </td>
        </tr>
      </table>
    </p>

    <p>The character's eyes must be wide open, but the mouth can be either completely closed or wide open. However, while the character's head must be front facing in the old version, the new version relaxes this constraint. The head and the body can be rotated to by a few degrees. The arms can be posed rather freely, but they should generally be below and far from the face. Allowing these variations makes the system more versatile because it is hard to find images of anime characters in the wild whose face is perfectly front facing and whose body is perfectly upright.</p>

    <p>
      <a name="fig:sample_inputs" />
      <table align="center">
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/00.png">
              <img src="data/kizuna_ai_sample_inputs/00.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/01.png">
              <img src="data/kizuna_ai_sample_inputs/01.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/02.png">
              <img src="data/kizuna_ai_sample_inputs/02.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/03.png">
              <img src="data/kizuna_ai_sample_inputs/03.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td>
            <a href="data/kizuna_ai_sample_inputs/04.png">
              <img src="data/kizuna_ai_sample_inputs/04.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/05.png">
              <img src="data/kizuna_ai_sample_inputs/05.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/06.png">
              <img src="data/kizuna_ai_sample_inputs/06.png" width="145">
            </a>            
          </td>
          <td>
            <a href="data/kizuna_ai_sample_inputs/07.png">
              <img src="data/kizuna_ai_sample_inputs/07.png" width="145">
            </a>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>Figure 3.1.2</b> Examples of valid input images to the system.
          </td>
        </tr>
      </table>
    </p>

    <p>The input image must have an alpha channel. Moreover, for every pixel that is not a part of the character, the RGBA value must be $(0,0,0,0)$.</p>

    <p>The pose vector now has 45 dimensions, and you can see the semantics of each parameter in <a href="#sec:pose-parameters">Appendix A</a>. 42 parameters have mostly the same semantics as <a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#sec:problem-spec"> those in the last version of the system</a>. The only changes from the old version are the ranges of the parameters for head rotation around the $y$- and $z$-axis. In the old version, these parameters correspond to rotation angles in the range $[-15^\circ, 15^\circ]$. In the new version, the range shrinks to $[-10^\circ, 10^\circ]$ for a reason that will momentarily become apparent.</p>

    <p>There are three new parameters, and they control the body.</p>

    <ul>
      <li>One parameter controls the rotation of the hip around the $y$-axis (the one that points upward). Its value is in the range $[-1,1]$ and corresponds in the obvious way to rotating the hip by angles in the range $[-5^\circ,5^\circ]$.

      <p align="center">
        <video width=512 height=512 muted autoplay playsinline loop>
            <source src="data/parameter_animations/body_y.mp4" type="video/mp4">
        </video>
      </p>
      </li>

      <li>One parameter controls the rotation of the hip around the $z$-axis (the one that points to the front of the character). Its value is also in the range $[-1,1]$ and corresponds in the obvious way rotating the hip by angles in the range $[-5^\circ,5^\circ]$.

      <p align="center">
        <video width=512 height=512 muted autoplay playsinline loop>
            <source src="data/parameter_animations/body_z.mp4" type="video/mp4">
        </video>
      </p>
      </li>

      <li>The last parameter controls the "breathing motion" where the chest moves slightly up and down. Its value is in the range $[0,1]$. The value $0$ corresponds to the chest being at its lowest position (fully exhaled), and the value $1$ corresponds to the chest being raised to the highest possible position (fully inhaled).

      <p align="center">
        <video width=512 height=512 muted autoplay playsinline loop>
            <source src="data/parameter_animations/breathing.mp4" type="video/mp4">
        </video>
      </p>
      </li>      
    </ul>

    <p>With the above three parameters, it becomes possible to move a character's upper body like how typical VTubers move theirs.</p>
    
    <p>Note that I previously mentioned that I reduced the range of the head rotation around the $y$-axis and the $z$-axis from $[-15^\circ, 15^\circ]$ to $[-10^\circ, 10^\circ]$. I did so because rotating the hip causes the face to move as well, and I would like to preserve the $[-15^\circ, 15^\circ]$ range in which the face can be oriented. That is, because the hip can be rotated by angles in the range $[-5^\circ, 5^\circ]$, I reduced the face's angle range to $[-10^\circ, 10^\circ]$ because $10^\circ + 5^\circ = 15^\circ$.</p>    

    <p>Lastly, let us recall the output's specification. After being given an image of a character and a pose vector, the system must produce a new image of the same character, posed according to the pose vector.</p>

    <a name="sec:system-overview"></a>
		<h3>3.2 &nbsp; System Overview</h3>

    <p><a href="#fig:new-system-overview">Figure 3.2.1</a> gives an overview of the new version of the system. It is similar to the old one (<a href="#fig:old-system-overview">Figure 2.1</a>), but now it deals with the upper body instead of just only the face. It still has two steps, and the first step still modifies the character's facial expression. For this step, I reuse the face morpher network that is the centerpiece of the previous year's project. The second step must not only rotate both the face and the body but also make the character breath, so the old face rotator from 2019 cannot be used. The network for the second step is now called the <b>body rotator</b>, and it must be designed and trained anew.</p>

    <p>
      <a name="fig:new-system-overview" />
      <table>
        <tr>
          <td align="center">
            <a href="data/overview_new_two_step_process.png">
              <img src="data/overview_new_two_step_process.png" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.2.1</b> An overview of the new version of the system.
          </td>
        </tr>
      </table>
    </p>

    <a name="sec:data"></a>
		<h3>3.3 &nbsp; Data</h3>

    <p>We must now prepare datasets to train the body rotator. Continuing the practice I adopted in previous projects, I created them by rendering 3D models created for the animation software <a href="https://sites.google.com/view/vpvp/">MikuMikuDance</a>, and I reused a collection of around 8,000 MMD models I manually downloaded and annotated. Details on how I created the collection can be found <a href="https://pkhungurn.github.io/talking-head-anime/index.html#dataset">here</a> and <a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#data">here</a>.</p>

    <p>A dataset's format must follow the specification of the body rotator's input and output. In particular, the input consists of two objects. One is a $512 \times 512$ image of a character whose facial expression has been modified by the face morpher. The other is the part of the pose vector that controls (1) the rotation of the face and body and (2) the breathing motion. This part has 6 dimensions: 3 for face rotation, 2 for body rotation, and 1 for the breathing motion. The output, of course, is another $512 \times 512$ image of the same character, but now its pose has been modified according to the $6$-dimensional pose vector.</p>

    <a name="sec:input-pose">
    <h4>3.3.1 &nbsp; Posing for the Input Image</h4>

    <p>One main difference between the body rotator and the face rotator from the 2021 project is the character's body pose in the input image. For the face rotator, the character must be in the "rest" pose. In other words, the face must be looking forward and must not be tilted or rotated sideways. Moreover, the body must be perfectly upright. The arms must stretch straight sideways and point diagonally downward. (See <a href="#fig:rest-pose">Figure 3.3.1.1</a>.) On the other hand, as stated in <a href="#sec:problem-spec">Section 3.1</a>, the new body rotator must be able to accept variations in the initial pose like in <a href="#fig:sample_inputs">Figure 3.1.2</a>.</p>

    <p>
      <a href="#fig:rest-pose"></a>
      <table align="center">
        <tr>
          <td align="center">
            <a href="data/rest_pose.png"><img src="data/rest_pose.png" alt="" width="360"></a>
          </td>         
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.3.1.1</b> The MMD model of Kizuna AI in the rest pose.
          </td>
        </tr>
      </table>
    </p>

    <p>This requirement makes data generation harder. For the old version, I only have to render an MMD model without posing it because MMD modelers almost always create their models in the rest pose to begin with. On the contrary, the new version requires an MMD model to be posed twice. It must take a non-rest pose in the input image, and then that pose must be altered according to the pose vector in the output image.</p>

    <p>One must then figure out what poses to use in the input images, and my answer is to use those shared by the MMD community. I downloaded pose data in <a href="https://w.atwiki.jp/vpvpwiki/pages/322.html">VPD format</a>, created specifically for MMD models from web sites such as <a href="https://www.nicovideo.jp/">Nico Nico</a> and <a href="https://bowlroll.net/">BowlRoll</a> and ended up collecting 4,731 poses in total. However, a pose may not be usable for several reasons.</p>

    <ol>
      <li>It is not a standing pose.</li>
      <li>The face turns too much sideways, upward, or downward.</li>
      <li>After the model is posed, the face or a large part of it is not contained in the middle $128 \times 128$ box described in Section 3.1.</li>
    </ol>

    <p>I created a tool that allowed me to manually classify whether a pose is usable or not through visual inspection. With it, I identified about 832 usable poses (a yield of 19.1%). You can see the tool in action in the video below.</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/hx7mLkbZlak" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <p>One way to specify the pose in the input image is to uniformly sample a usable pose from the collection above. However, I felt that using just 832 poses was not diverse enough, so I augmented the sampled pose further. After sampling a pose from the collection, I sample a "rest pose" by sampling the angle the arms should make with the $y$-axis from the range $[12.5^\circ, 30^\circ]$ and rotating the model's arm accordingly. I then blended the pose from the collection with the rest pose, using a blending factor $\alpha$ sampled from the range $[0,1]$. This process is depicted in the figure below.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <a href="data/input_pose_process.png"><img width="600" src="data/input_pose_process.png" alt=""></a>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Figure 3.3.1.2</b> The process to sample a pose to be used in the input image.</td>
        </tr>
      </table>
    </p>

    <p>Note that a pose of an MMD model is a collection of two types of values.</p>

    <ul>
      <li>A scalar weight for each <a href="https://en.wikipedia.org/wiki/Morph_target_animation">blendshape</a> the model has.</li>
      <li>A <a href="https://en.wikipedia.org/wiki/Quaternion">quaternion</a> to represent rotation at each bone in the model's <a href="https://en.wikipedia.org/wiki/Skeletal_animation">skeleton</a>.</li>
    </ul>

    <p>Blending two poses together thus involves interpolating the above values. More specifically, we perform linear interpolation on the blendshape weights, and <a href="https://en.wikipedia.org/wiki/Slerp">spherical linear interpolation</a> (slerp) on the quaternions.</p>

    <a name="sec:pose-parameter-semantics">
    <h4>3.3.2 &nbsp; Semantics of the 6 Pose Parameters</h4>

    <p>In order to generate training examples, one must determine what each component of the pose vector means in terms of MMD models. For example, when the breathing parameter is, say, $0.75$, what bone(s) in an MMD model does one modify and what should the modification be? I shall now discuss the semantics of the 6 parameters in turn.</p>

    <h5>3.3.2.1 &nbsp; The Hip Rotation Parameters</h5>

    <p>Let us start with the one that is the easiest to describe: the hip $y$-rotation parameter. In this case, one must rotate two bones around the $y$-axis by the same angle. One bone is called  "upper body" (上半身), and the other is called "lower body" (下半身). According to the specification in <a href="#sec:problem-spec">Section 3.1</a>, the angle is $v \times 5^\circ$ where $v$ is the parameter value.</p>

    <p>For the semantics of the hip $z$-rotation parameter, just replace the $y$-axis with the $z$-axis.</p>

    <h5>3.3.2.2 &nbsp; The Breathing Parameter</h5>

    <p>The semantics of the breathing parameter is more involved as most MMD models do not have bones or morphs for specifically controlling breathing. As a result, I have to define what breathing means on my own, and I chose to modify the translation parameters of a 5 bones in the chest area.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <a href="data/breathing_motion/kizuna_ai_with_bones_and_legends.png">
              <img width="480" src="data/breathing_motion/kizuna_ai_with_bones_and_legends.png">
            </a>
          </td>
        </tr>
        <tr>
          <td><b>Figure 3.3.2.2.1</b> Bones modified to enact the breathing motion.</td>
        </tr>
      </table>
    </p>

    <p>When we inhale, our lungs expands, and it pushes our chest both outward and upward. To simulate this movement, I set the translation parameter of the "upper body" bone to the vector $(0,0,v\times D)$ to make the chest protrudes outward and that of the "upper body 2" bone to $(0, v \times D, 0)$ to make it extends upward. Here, $v$ is the breathing parameter value and $D$ is the maximum displacement, a per-model constant that we shall discuss later. The effect of the modification can be seen in the following video.</p>

    <p align="center">
      <video width=512 height=512 muted autoplay playsinline loop>
          <source src="data/breathing_motion/upper_body_bones_only.mp4" type="video/mp4">
      </video>
    </p>    

    <p>However, we can also see that it also has the side effect of making the head and the shoulders move diagonally back and forth. Nevertheless, when we breathe, our head and shoulders rarely move. To keep them stationary, I also set the translation parameters of three remaining bones (i.e., left shoulder, right shoulder, and neck) to $(0, -v \times D, -v \times D)$ to cancel the translations of the two upper body bones. The effect of the cancellation can be seen in the video below.</p>

    <p align="center">
      <video width=512 height=512 muted autoplay playsinline loop>
          <source src="data/breathing_motion/all_bones.mp4" type="video/mp4">
      </video>
    </p>    

    <p>The maximum displacement $D$ is set to $1/64$ of the height of the character's head. </p>

    <p>When the model is viewed from the front, we can see that the chest moves up and down while the head and the shoulders remain stationary. This movement gives the impression that the character is breathing as we wanted</p>

    <p align="center">
      <video width=512 height=512 muted autoplay playsinline loop>
          <source src="data/parameter_animations/breathing.mp4" type="video/mp4">
      </video>
    </p>

    <h5>3.3.2.3 &nbsp; The Head Rotation Parameters</h5>

    <p>There are three head rotation parameters:</p>

    <ul>
      <li>Rotation of the neck bone around the $z$-axis.</li>
      <li>Rotation of the head bone around the $x$-axis.</li>
      <li>Rotation of the head bone around the $y$-axis.</li>
    </ul>

    <p>There are no changes to the bones and the axes above. However, I changed how the parameters affect the model's shape.</p>

    <p>Typically, when a bone of an 3D model is rotated, bones that are children of that bone and vertices that these bones influence also move. For example, when one rotates the neck bone, vertices on the neck, the whole head, and also the whole hair mass would also rotate with the neck, as can be seen in the video below.</p>
    
    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=512 height=512 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v3.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>Figure 3.3.2.3.1</b> The typical result of rotating the neck bone around the $z$-axis. Notice that the whole hair mass moves like a single rigid body, following the head. The character is <a href="https://www.youtube.com/channel/UCeLzT-7b2PBcunJplmWtoDg">Suou Patra</a> (&copy; HoneyStrap), and the 3D model was created by <a href="https://3d.nicovideo.jp/works/td60253">OTUKI</a>.
          </td>
        </tr>
      </table>      
    </p>

    <p>This behavior, however, makes it very hard for a neural network to animate characters with long hair. First, it must identify correctly which part of the input image is the hair and which part is the body and the arms that are in front of it. This is a hard segmentation problem that must be done 100% correctly. Otherwise, disturbing errors such as severed body parts or clothing might show up. Second, as the hair mass moves, parts that were occluded by the body can become visible, and the network must hallucinate these parts correctly. Note that these difficulties do not exist in the previous version of my system because it could only animate headshots. We cannot see long hair in the input to begin with!</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <img width=300 height=300 src="data/failure_cases/enomiya_milk/enomiya_milk.png" alt="">
          </td>
          <td align="center">
            <video width=300 height=300 muted autoplay playsinline loop>
              <source src="data/failure_cases/enomiya_milk/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="2">
            <b>Figure 3.3.2.3.2</b> TTo generate the video on the right, I used a model to animate the image on the left, but it was trained on a dataset where the whole hair mass moves with the head like in Figure 3.3.2.3.1. In the bottom half of the video, we can see that the details of the hair strands are lost. Moreover, the model seemed to think that the character's hands were a part of the hair, so it cut the fingers off when the hair moved. The character is <a href="https://www.youtube.com/channel/UCJCzy0Fyrm0UhIrGQ7tHpjg">Enomiya Milk</a> (&copy; Noripro).
          </td>
        </tr>
      </table>      
    </p>

    <p>While it may be possible to solve the above problems with current machine learning techniques, I realized that it was much easier to avoid them and still generate plausible and beautiful animations. The difficulty in our case comes from long-range dependency: a small movement of the head leads to large movement elsewhere faraway. The situation becomes much easier if hair pixels far from the head were kept stationary.</p>

    <p>I thus modified the skinning algorithm for MMD models so that the neck and the head bones can only influence vertices that are not too far below the vertical position of the neck. The new algorithm's effect can be seen in the following video.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center">
            <video width=512 height=512 muted autoplay playsinline loop>
              <source src="data/head_rotation/neck_z_v4.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <b>Figure 3.3.2.3.3</b> Hair movement after limiting the influence of the neck and head bones. We can see that the hair mass below the shoulders does not move at all, and this make the system's job much easier.
          </td>
        </tr>
      </table>      
    </p>

    <p>To recap, the head rotation parameters still correspond to rotating the same bones around the same axes. However, the influence of the these bones is limited to vertices that are not too far below the neck so that head movement cannot cause large movement elsewhere. This greatly simplifies animating characters with very long hair, which are quite common in illustrations in the wild.</p>

    <a name="sec:simulated-neck-shadows">
    <h4>3.3.3 &nbsp; Augmenting Renderings with Simulated Neck Shadows</h4>

    <p>Character illustrations in the wild often depict shadows casted by the head on the neck. We can clearly see that the skin just below the chin is often much darker than the face.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center"><a href="data/neck_shadows/neck_shadows_in_the_wild.png"><img src="data/neck_shadows/neck_shadows_in_the_wild.png" alt="" width="450"></a></td>
        </tr>
        <tr>
          <td>
            <b>Figure 3.3.3.1</b> Three drawn characters with neck shadows. The characters are 
            <a href="https://www.youtube.com/channel/UCXU7YYxy_iQd3ulXyO-zC2w">Fushimi Gaku</a>, 
            <a href="https://www.youtube.com/channel/UC2OacIzd2UxGHRGhdHl1Rhw">Hayase Sou</a>,
            and <a href="https://www.youtube.com/results?search_query=honma+himawari">Honma Himawari</a>. They are &copy; ANYCOLOR, Inc.
          </td>
        </tr>
      </table>
    </p>

    <p>However, in 3D models, the neck and face skin often have exactly the same tone. Thus, a neck shadow would be absent if a model is rendered without shadow mapping or other shadow-producing techniques. I chose not to implement such an algorithm because it would require much effort and would greatly complicate my data generation pipeline. As result, my previous datasets do not have neck shadows and are quite different from real-world data.</p>

    <p>When a character turns its face upward, the area of the neck previously occluded by the chin would become visible, and the network must hallucinate the pixels there. Ideally, if the neck shadow is present, the hallucinated pixels should have the same color. However, training a neural network with my previous datasets can lead to a problem where these pixels would be brighter than the surrounding shadow because it is fine to use the face skin's color during training. The figure below shows two such failure cases.</p>

    <a name="fig:neck-color-failure-cases" />
    <p>
      <table align="center">
        <tr>
          <td>
            <table align="center">
              <tr>
                <td align="center"><b>Input image</b></td>
                <td align="center"><b>After having face<br> turned upward</b></td>
              </tr>
              <tr>
                <td>
                  <img src="data/neck_shadows/suzuhara_lulu_original_cropped.png" alt="">
                </td>
                <td>
                  <img src="data/neck_shadows/suzuhara_lulu_failure_cropped.png" alt="">
                </td>
              </tr>        
              <tr>
                <td>
                  <img src="data/neck_shadows/ex_albio_original_cropped.png" alt="">
                </td>
                <td>
                  <img src="data/neck_shadows/ex_albio_failure_cropped.png" alt="">
                </td>
              </tr>  
            </table>
          </td>
        </tr>      
        <tr>
          <td><b>Figure 3.3.3.2</b> Failure cases in which hallucinated neck pixels are brighter than the surrounding neck shadows. The characters are <a href="https://www.youtube.com/channel/UC_a1ZYZ8ZTXpjg9xUY9sj8w">Suzuhara Lulu</a> (top) and <a href="https://www.youtube.com/channel/UCIytNcoz4pWzXfLda0DoULQ">Ex Albio</a> (bottom). They are &copy; ANYCOLOR, Inc.</td>
        </tr>
      </table>
    </p>

    <p>To alleviate the problem without having to implement a full-blown shadow algorithm, I simulated neck shadows by simply rendering the neck under a different lighting configuration than the rest of the body. Like the <a href="http://pkhungurn.github.io/talking-head-anime/index.html##rendering">previous versions</a> of the project, two light sources are present in the scene. As such, when implementing the <a href="https://www.khronos.org/opengl/wiki/Fragment_Shader">fragment shader</a> of my renderer, I only had to condition their intensities on whether the <a href="https://www.khronos.org/opengl/wiki/Fragment">fragment</a> being shaded belongs to the neck or not. The result of this rendering method can be seen in the figure below.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <table align="center">
              <tr>
                <td>
                  <img src="data/neck_shadows/yamakaze_no_neck_shadow.png" alt="">
                </td>
                <td>
                  <img src="data/neck_shadows/yamakaze_with_neck_shadow.png" alt="">
                </td>
              </tr>
              <tr>
                <td align="center">(a)</td>
                <td align="center">(b)</td>
              </tr>
            </table>
          </td>
        </tr>
        <tr>
          <td><b>Figure 3.3.3.3</b> An MMD model rendered (a) conventionally and (b) with simulated neck shadow. The character is <a href="https://kancolle.fandom.com/wiki/Yamakaze">Yamakaze</a> from the game <a href="https://en.wikipedia.org/wiki/Kantai_Collection">Kantai Collection</a>. The 3D model was created by <a href="https://www.nicovideo.jp/watch/sm30168979">cham</a>.</td>
        </tr>
      </table>
    </p>

    <p>When generating training examples, we must then provide two sets of lighting intensities so that one can be used to render the body, and the other can be used to render the neck. In the dataset I generated, I sampled the intensities so that the following properties hold:</p>

    <ul>
      <li>For 25% of the dataset, the neck are shaded under darker lights than the rest of the body.</li>
      <li>For 25% of the dataset, the neck and the body are shaded under the same lighting configuration.</li>
      <li>For 50% of the dataset, the lighting configurations used to render the neck and the body are sampled independently. (So, in some of these cases, the neck might appear brighter than the rest of the body.)</li>
    </ul>

    <p>The sampling method above, I believe, would allow the network to deal with the variety of character illustrations in the wild.</p>
    
    <h4>3.3.4 &nbsp; Generating a Training Example</h4>

    <p>A dataset is a collection of training examples, and a training example in our case is a triple $(I_{\mathrm{in}}, \ve{p}, I_{\mathrm{out}})$ where:
    </p>

    <ul>
      <li>$I_{\mrm{in}}$ is a $512 \times 512$ input image depicting a character in a standing pose.</li>
      <li>$\ve{p}$ is the 6-dimensional pose vector.</li>
      <li>$I_{\mrm{out}}$ is a $512 \times 512$ output image, which should depict the character in the input image whose pose has been altered by the pose vector.</li>
    </ul>

    <p>
      <table align="center">
        <tr>
          <td>
            <table align="center" cellpadding="5">
              <tr>
                <td align="center">
                  <a href="data/training_example/input.png">
                    <img style="border-color: black; border-style: solid; border-width: 1px" src="data/training_example/input.png" alt="" width="200">
                  </a>
                </td>
                <td align="center">
                  \begin{align*}
                    \begin{bmatrix}
                      0.45\\ 0.09\\ -0.60\\ -0.06\\ -0.30\\ 0.80
                    \end{bmatrix}
                  \end{align*}
                </td>
                <td align="center">
                  <a href="data/training_example/output.png">
                    <img style="border-color: black; border-style: solid; border-width: 1px" src="data/training_example/output.png" alt="" width="200">
                  </a>
                </td>
              </tr>
              <tr>
                <td align="center">$I_\mrm{in}$</td>
                <td align="center">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\ve{p}$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                <td align="center">$I_\mrm{out}$</td>
              </tr>
            </table>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Figure 3.3.4.1</b> A training example.</td>
        </tr>
      </table>
    </p>

    <p>The process of generating the above data items is rather involved. It requires sampling an MMD model, an input pose as in <a href="#sec:input-pose">Section 3.3.1</a>, a 6-dimensional pose vector $\ve{p}$, and two sets of light intensities as in <a href="#sec:simulated-neck-shadows">Section 3.3.3</a>. The input pose and $\ve{p}$ must then be combined using the specification in <a href="#sec:pose-parameter-semantics">Section 3.3.2</a> to obtain the pose to be used in the output image. The model, the poses, and the lighting configurations are then combined to render $I_\mrm{in}$ and $I_\mrm{out}$ using the rendering algorithm in <a href="#sec:simulated-neck-shadows">Section 3.3.3</a>. For completeness, I lay out the complete generation algorithm in the listing below. The reader, however, is advised to skip the description unless they are interested in reproducing it.</p>

    <table class="table">
      <thead class="table-dark"> 
        <tr>
          <th><i>Listing 3.3.4.2</i> Algorithm for generating a training example</th>
        </tr>
      </thead>
      <tbody>
        <tr class="table-secondary">
          <td>
            <ol>
              <li>A model from my collection of MMD models is sampled. For this step, I made sure that each of the models would have roughly the same number of training examples using it.</li>
              
              <li>The process for determining the input pose, detailed in <a href="#sec:input-pose">Section 3.3.1</a>, is invoked. In particular:
              <ul>
                <li>A pose in VPD format is sampled from the collection of 832 poses.</li>
                <li>A rest pose is sampled by sampling an arm angle from $[12.5^\circ, 30^\circ]$.</li>
                <li>A blending factor $\alpha$ is sampled from the range $[0,1]$.</li>
                <li>The input pose is computed by blending the sampled pose with the rest pose according to $\alpha$. For later reference, let us call this pose $\ve{P}_\mrm{in}$.</li>
              </ul>
              
              </li>
              <li>A pose vector $\ve{p}$ is sampled component by component and independenty.
                <ul>
                  <li>The 3 head rotation parameters are each sampled according to the <a href="https://pkhungurn.github.io/talking-head-anime/data/joint-parameter-distribution.png">probability distribution</a> I used in the previous version of the project.</li>
                  <li>The 2 body rotation parameters are sampled uniformly from the range $[-1,1].$</li>
                  <li>The breathing parameter is sampled according to a linear distribution $p(x)$ where $p(0) = 0.3$ and $p(1) = 1.7$.</li>
                </ul>
              </li>      
        
              <li>Two sets of light intensities are sampled according to the specification in <a href="#sec:simulated-neck-shadows">Section 3.3.3</a>.</li>
              
              <li>The input pose $\ve{P}_\mrm{in}$ is altered according to the sampled pose vector $\ve{p}$. This involves modifying the bones according to the semantics described in <a href="#sec:pose-parameter-semantics">Section 3.3.2</a>. Let us called the result of this modification $\ve{P}_\mrm{out}$.</li>
        
              <li>The sampled model is posed according to $\ve{P}_\mrm{in}$ and is then rendered under the sampled lighting intensities as described in Section 3.3.3 to produce the input image $I_\mrm{in}$.</li>
        
              <li>The sampled model is posed one more time according to $\ve{P}_\mrm{out}$ and is then rendered to produce the output image $I_{\mrm{out}}$.</li>
            </ol>        
          </td>
        </tr>
      </tbody>
    </table>    
    <h4>3.3.5 &nbsp; The Datasets</h4>

    <p>I followed the same dataset generation process as the previous versions of the projects. Before data generation, I divided the models I downloaded into three groups according to their source materials (i.e., what animes/mangas/games they came from) so that no two groups had models of the same character. I then used the three groups to generate the training, validation, and test datasets. The number of training examples and the number of models used to generate them are given in the table below.</p>

    <table class="table table-striped">
			<tr class="table-dark">
				<td>&nbsp;</td>
				<td align="right"><b>Training set</b></td>
				<td align="right"><b>Validation set</b></td>
				<td align="right"><b>Test set</b></td>
			</tr>
			<tr>
				<td># models used</td>
				<td align="right">7,827</td>
				<td align="right">79</td>
				<td align="right">68</td>
			</tr>
			<tr>
				<td># examples</td>
				<td align="right">500,000</td>
				<td align="right">10,000</td>
				<td align="right">10,000</td>
			</tr>
		</table>

    <a name="sec:networks"></a>
		<h3>3.4 &nbsp; Networks</h3>

    <p>I have just described the datasets for training the body rotator. In this section, I turn to the network's architecture and training process.</p>

    <h4>3.4.1 &nbsp; Overall Design</h4>

    <p>In my first attempt to design the body rotator, I reused the face rotator's architecture. There would be two subnetworks. The first one would produce two outputs, and the second one would then combine them into the final output image.</p>

    <p align="center">
      <a name="fig:face-rotator" />
      <table align="center">
          <tr>
              <td align="center">
                  <a href="data/face_rotator.png"><img src="data/face_rotator.png" width="600"></a>
              </td>
          </tr>
          <tr>
              <td>
                  <b>Figure 3.4.1.1</b> The face rotator's architecture. (This figure is the same as <a href="fig:face-rotator">Figure 2.2</a>. It is reproduced here for the reader's convenience.)
              </td>
          </tr>
      </table>
    </p>
    
    <p>The difficulty, however, is the input's size: a $512 \times 512$ image is 4 times larger than a $256 \times 256$ one. The networks above were designed to work with $256 \times 256$ images. Hence, if I use them without modification, they would become 4 times slower, which is clearly not fast enough for interactive applications. The 2021 system could only achieve between 10-20 fps even on a Titan RTX graphics card, and I do not want the new system to be much slower.</p>

    <p>My strategy, then, is to scale the input image down to $256 \times 256$ and then perform body rotation on it first. For this step, I can use a subnetwork whose architecture is similar to those in my previous projects without any performance penalty. I call this network the <b>half-resolution rotator</b> due to the fact that it has the same functionality as the whole body rotator but operates on half-resolution images. Its output, of course, are half-sized and so not immediately usable. Scaling the outputs up by a factor of 2 would provide images with the right resolution, but these images are "coarse" in the sense that they lack high-frequency details. I thus add another subnetwork called the <b>editor</b>, whose task is to combine the scaled-up outputs into one image and edit it to improve quality.</p>

    <p>Note that the editor is the only network that operates on full-resolution images, but it can afford to have lower capacity per input pixel because its task is much easier than that of the half-resolution rotator. We will see later that this design keeps the body rotator speedy enough for real-time applications despite the fact the input is now 4 times larger.</p>
    
    <p>The two networks do not follow the old design exactly. Like the two-algorithm face rotator, the half-resolution rotator still uses two image transformation techniques to produce output images, but they are not the same as the old ones.</p>

    <ul>
      <li>Partial image change is replaced by a new technique, <b>direct generation</b> (which will be discussed later).</li>

      <li>Warping is still used, but the unit responsible for it is modified to also output the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#appearance-flow-offset">appearance flow offset</a> that is used generate the warped image.</li>
    </ul>

    <p>The editor is similar to the combiner. However, instead of taking in outputs from both image tranformation techniques of the half-resolution rotator, it now only takes those from the warping one. The image created by direction generation is always discarded.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <a href="data/overview_body_rotator.png">
              <img src="data/overview_body_rotator.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Figure 3.4.1.2</b> The overall architecture of the body rotator.</td>
        </tr>
      </table>
    </p>
    
    <p>While direct generation seems wasteful and superfluous, it serves as an auxiliary task at training time, and I found that it improved the whole pipeline's performance. This counterintuitive design is a result of evaluating many design alternatives and choosing the best one. (More on this later in <a href="#results">Section 3.5</a>.)</p>

    <p>I will now discuss each of the subnetworks in more details.</p>

    <a name="sec:half-res-rotator" />
    <h4>3.4.2 &nbsp; The Half-Resolution Rotator</h4>

    <p>The half-resolution rotator's architecture is derived from that of the <a href="http://pkhungurn.github.io/talking-head-anime/index.html#two-algo-face-rotator">two-algorithm face rotator</a> from my 2019 project. So, it is built with components that I previously used. These include the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#image-features">image features</a> (alpha mask, image change, and appearance flow offset), the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#image-transformation-units">image transformation units</a> (partial image change unit, combining unit, and warping unit), and various units such as $\mrm{Conv3}$, $\mrm{Conv7}$, $\mrm{ConvDown}$, $\mrm{Tanh}$, and so on. I refer the reader to the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#networks">previous write-up</a> for details.</p>
    
    <a name="fig:half-res-rotator" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/half_res_rotator.png">
              <img src="data/half_res_rotator.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.4.2.1</b> An overview of the half-resolution rotator's architecture.
          </td>
        </tr>
      </table>
    </p>
      
    <p>From the above figure, the half-resolution rotator has an encoder-decoder main body, which takes in a $256 \times 256$ input image and a pose vector and then produces a feature tensor. It then employs two image transformation units on the result to generate the outputs.<p>

    <ul>
      <li>The first is the new <b>direct generation</b> unit. It simply creates an output image from a feature tensor by passing it through a $3 \times 3$ convolution ($\mrm{Conv3}$) and then a hyperbolic tangent activation function. One can think of the unit as a simplified version of the partial image change unit where no alpha mask is generated, and no blending is performed. I shall refer to its output as $I_\mrm{direct}$.</li>
    </ul>

    <a name="fig:direct-generation-unit" />
    <p>
      <table align="center">
        <tr>
          <td>
            <a href="data/direct_generation_unit.png">
              <img src="data/direct_generation_unit.png" alt="" width="500">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Figure 3.4.2.2</b> The direct generation unit.</td>
        </tr>
      </table>
    </p>

    <ul>
      <li>The second is a warping unit, which produces a warped image and the appearance flow offset that is used to generate it. Let us denote the warped image by $I_\mrm{warped}$.</li>
    </ul>

    <p>The outputs of these units are treated as the outputs of the half-resolution rotator.</p>

    

    <p>Recall that, in the two-algorithm face rotator of the 2019 project, the partial image change unit is used because, in the 2019 problem specification, the body does not move at all, so the network only has to change pixels belonging to the head. However, for the current problem specification, if any of the parameters that control the hip rotation is not zero, then every pixel would change. As a result, it becomes more economical to generate all output pixels directly, and so I replaced partial image change with direct generation.</p>

    <p>The specification of the encoder-decoder network is given the the table below.</p>

    <a name="table:half-res-rotator-encoder-decoder">
    <table>
      <tr>
        <td>
          <table class="table">
            <thead class="table-dark">
              <tr>
                <th scope="col">Tensors</th>
                <th scope="col">Shape</th>
              </tr>
            </thead>
            <tbody>
              <tr class="table-primary">
                <td>
                  $A_0 = $ input image
                </td>
                <td>
                  $4 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_1 = $ pose vector
                </td>
                <td>
                  $6$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_2 = A_1$ turned into a 2D tensor 
                </td>
                <td>
                  $6 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_3 = \mrm{Concat}(A_0, A_2)$
                </td>
                <td>
                  $10 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_1 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Cov7}(A_3)))$
                </td>
                <td>
                  $64 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_2 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_1)))$
                </td>
                <td>
                  $128 \times 128 \times 128$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_3 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_2)))$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_4 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_3)))$
                </td>
                <td>
                  $512 \times 32 \times 32$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_1 = \mrm{ResNetBlock}(B_4)$
                </td>
                <td>
                  $512 \times 32 \times 32$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_2 = \mrm{ResNetBlock}(C_1)$
                </td>
                <td>
                  $512 \times 32 \times 32$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $\qquad \vdots$
                </td>
                <td>
                  $\qquad \vdots$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_6 = \mrm{ResNetBlock}(C_5)$
                </td>
                <td>
                  $512 \times 32 \times 32$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_1 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(\mrm{UpsampleNn}(C_6))))$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_2 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(\mrm{UpsampleNn}(D_1))))$
                </td>
                <td>
                  $128 \times 128 \times 128$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_3 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(\mrm{UpsampleNn}(D_2))))$
                </td>
                <td>
                  $64 \times 256 \times 256$
                </td>
              </tr>              
            </tbody>      
          </table>
        </td>
      </tr>
      <tr>
        <td><b>Table 3.4.2.3</b> Specification of the encoder-decoder network that is the main body of the half-resolution rotator. Note that $D_3$ is the feature vector that is fed to the image tranformation units in order to generate the final outputs.</td>
      </tr>
    </table>
    <br>
    
    <p>The encoder-decoder network above is an upgraded version of <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#encoder-decoder-network">the one used in my 2021 project</a>. Differences from the old design include:</p>

    <ol>
      <li>Instead of using the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)">linear rectifier unit</a> (ReLU) as the activation function, I now use the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU">leaky ReLU</a> with slope $0.1$ instead.</li>

      <li>In the upscaling portion of the encoder-decoder, I use nearest-neighbor upscaling by a factor of 2 followed by a $\mrm{Conv3}$ instead of a transposed convolution unit. This is done to combat checkerboard artifacts in the outputs <a href="#fn_odena_2016">[Odena et al. 2016]</a>.</li>
    </ol>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_odena_2016">
          <p align="left">
            Augustus Odena, Vincent Dumoulin, and Chris Olah.
            <b>Deconvolution and Checkerboard Artifacts.</b>
            Distill. 2016.
            <a href="https://distill.pub/2016/deconv-checkerboard/">[WWW]</a>            
          </p>
        </li>        
      </ul>
    </div>

    <p>The units used to build the network are largely the same, but the semantics of some  have slightly changed, and I also introduced a number of new ones.</p>

    <ul>
      <li>$\mrm{Concat}(\cdot, \cdot)$ concatenates two tensors in the first dimension.</li>
      <li>$\mrm{LeakyReLU}(\cdot)$ applies the <a href="https://en.wikipedia.org/wiki/Rectifier_(neural_networks)#Leaky_ReLU">leaky rectified linear unit</a> activation function to each of the input tensor's element. The slope of the negative portion is $0.1$.</li>
      <li>$\mrm{ResNetBlock}(\cdot)$ still denotes a residual block. However, the activation function used is $\mrm{LeakyReLU}$ instead of $\mrm{ReLU}$.
        \begin{align*}
        \mathrm{ResNetBlock}(X) = X + \mathrm{InstNorm}(\mathrm{Conv3}(\mathrm{LeakyReLU}(\mathrm{InstNorm}(\mathrm{Conv3}(X))))).
        \end{align*}
      </li>
      <li>$\mrm{UpsampleNn}(\cdot)$ upsamples the argument tensor by a factor of $2$ with the nearest neighbor algorithm. It corresponds to PyTorch's <a href="https://pytorch.org/docs/stable/generated/torch.nn.Upsample.html"><tt>Upsample</tt></a> unit with the <tt>nearest</tt> mode.</li>      
    </ul>

    <p>The half-resolution rotator is about 128 MB in size.</p>

    <p><b>Training procedure.</b> I trained the half-resolution rotator using a process similar to that of the two-algorithm face rotator. Training is divided into two phases. In the first phase, the loss function was the L1-norms of the differences between the two generated images and the direct image.
    \begin{align*}
      \mcal{L}_{\mrm{HRR},1} = E_{(I_\mrm{in}, \ve{p}, I_{\mrm{out}}) \sim p_{\mrm{data}}}\Big[ \| I_\mrm{warped} - I_\mrm{out} \|_1 + \| I_\mrm{direct} - I_\mrm{out} \|_1 \Big]
    \end{align*}
    In the second phase, I added a perceptual feature reconstruction loss <a href="#fn_johnson_2016">[Johnson et al. 2016]</a> on $I_\mrm{direct}$ and adjusted the weights of existing terms.
    \begin{align*}
      \mcal{L}_{\mrm{HRR},2} 
      &= E_{(I_\mrm{in}, \ve{p}, I_{\mrm{out}}) \sim p_{\mrm{data}}}\bigg[ 20 \| I_\mrm{warped} - I_\mrm{out} \|_1 + \| I_\mrm{direct} - I_\mrm{out} \|_1 \\
      &\qquad + \frac{4 \times 256 \times 256}{5}\Phi(I_\mrm{out}, I_\mrm{direct}) \bigg].
    \end{align*}
    Here,
    \begin{align*} 
      \Phi(I_1, I_2) = \sum_{i=1}^3 \lambda_i \big( \| \phi_i(I^{rgb}_1) - \phi_i(I^{rgb}_2) \|_1 + \| \phi_i(I^{aaa}_1) - \phi_i(I^{aaa}_2) \|_1 \big)
    \end{align*}
    where
    <ul>
      <li>$\phi_1(\cdot)$, $\phi_2(\cdot)$, and $\phi_3(\cdot)$ denote the feature tensors outputted by the <tt>relu1_2</tt>, <tt>relu2_2</tt>, and <tt>relu3_3</tt> layers of the VGG16 network <a href="#fn_simonyan_2015">[Simonyan et al. 2015]</a>, respectively;</li>
      <li>$\lambda_i = (C_i H_i W_i)^{-1}$ where $C_i$, $H_i$, and $W_i$ are the number of channels, the height, and the width of $\phi_i(\cdot)$; and</li>
      <li>for any RGBA image $I$, we let $I^{rgb}$ denote the same image without the alpha channel, and $I^{aaa}$ a 3-channel image formed by repeating $I$'s alpha channel three times.</li>
    </ul>    
    </p>

    <p>The reader may notice the imbalance between the weights. The L1 losses, $20\| I_\mrm{warped} - I_\mrm{out} \|_1$ and $\| I_\mrm{direct} - I_\mrm{out} \|_1$, have small weights, but the weight of the perceptual loss, $\frac{4 \times 256 \times 256}{5} \Phi(I_\mrm{out}, I_\mrm{direct})$, is much larger. The reason for this imbalance is that $\Phi(I_\mrm{out}, I_\mrm{direct})$ is a sum of L1-norms that have been normalized by tensor sizes, but the other two losses are "unnormalized" L1-norms of $4 \times 512 \times 512$ tensors. The number $4 \times 256 \times 256$ scales $\Phi(I_\mrm{out}, I_\mrm{direct})$ up so that each of its term becomes an unnormalized $L1$-norm of a $4 \times 256 \times 256$ tensor. This puts the perceptual loss on roughly the same order of magnitude as the two L1 losses.</p>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_johnson_2016">
          <p align="left">
            Justin Johnson, Alexandre Alahi, Li Fei-Fei.
            <b>Perceptual Losses for Real-Time Style Transfer and Super-Resolution.</b>
            ECCV 2016.
            <a href="https://cs.stanford.edu/people/jcjohns/eccv16/">[Project]</a>
            <a href="https://arxiv.org/abs/1603.08155">[arXiv]</a>
          </p>
        </li>
        <li class="footnote" id="fn_simonyan_2015">
          <p align="left">
            Karen Simonyan and Andrew Zisserman.
            <b>Very Deep Convolutional Networks for Large-Scale Image Recognition.</b>
            ICRL 2015.
            <a href="https://www.robots.ox.ac.uk/~vgg/research/very_deep/">[Project]</a>
            <a href="https://arxiv.org/abs/1409.1556">[arXiv]</a>
          </p>
        </li>
      </ul>
    </div>

    <p>I trained the network with the Adam algorithm, setting $\beta_1 = 0.5$ and $\beta_2 = 0.999$. The learning rate was $10^{-4}$ for both phases, and the batch size was 8. The first phase lasted 1 epoch (500,000 examples shown), and the second phase lasted 12 epochs (6,000,000 examples shown).</p>

    <a name="sec:editor" />
    <h4>3.4.3 &nbsp; The Editor</h4>

    <p>Recall from <a href="fig:body-rotator">Figure 3.4.1.2</a> that the outputs of the half-resolution rotator are scaled up by a factor of 2. Then, they are fed to the editor along with the original $512 \times 512$ input image and the pose vector. The editor's job, then, is to produce an output image from these data.</p>

    <p>Unlike the half-resolution rotator, the editor's main body is a U-Net <a href="#fn_ronneberger_2015_0">[Ronneberger et al. 2015]</a> instead of an encoder-decoder. I made this choice because of the folk wisdom that U-Nets are good for tasks where the input and output images are aligned pixel-to-pixel. Here, I assume that the half-resolution rotator should have moved the pixels to roughly the right locations, and so the editor's output would align pixel-to-pixel with the half-resolution rotator's outputs.</p>

    <p>After being fed the all the inputs, the main body produces a feature tensor, which is then fed to a number of image processing steps, leading to the final output. The steps are:</p>

    <ol>
      <li><b>Warping the input image.</b> From the feature tensor, a new appearance flow offset is created. It is then added to the input appearance flow offset, and the result is then used to warp the original $512 \times 512$ input image.</li>
      <li><b>Partially change the warped image.</b> We simply apply a partial image change  to the warped image generated in the last step. The resulting image is treated as the output of the editor. Let us denote it by $I_\mrm{final}$.</li>
    </ol>

    <p>In other words, the editor further modifies the appearance flow offset created by the half-resolution rotator. Ideally, it should add high-frequency details that the rotator could not generate. The editor then "retouch" the the warped image generated by the improved appearance flow offset through partial image changes. The whole process is summarized in the Figure below.</p>

    <div class="footnotes">
      <ul>
          <li class="footnote" id="fn_ronneberger_2015_0">
              <p align="left">
              Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
              <b>U-Net: Convolutional Networks for Biomedical Image Segmentation.</b>
              Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015.
              <a href="https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/">[Project]</a>
              <a href="https://arxiv.org/abs/1511.06702">[arXiv]</a>
              </p>
          </li>
      </ul>
    </div>

    <a name="fig:editor" />
    <p>
      <table>
        <tr>
          <td align="center">
            <a href="data/editor.png">
              <img src="data/editor.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.4.3.1</b> An overview of the editor's architecture.
          </td>
        </tr>
      </table>
    </p>

    <p>The Specification of the U-Net network is given in the table below.</p>

    <a name="table:editor-u-net" />
    <table>
      <tr>
        <td>
          <table class="table">
            <thead class="table-dark">
              <tr>
                <th scope="col">Tensors</th>
                <th scope="col">Shape</th>
              </tr>
            </thead>
            <tbody>
              <tr class="table-primary">
                <td>
                  $A_0 = $ original input image
                </td>
                <td>
                  $4 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_1 = $ pose vector
                </td>
                <td>
                  $6$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_2 = $ scaled-up warped image <br>(generated by the half-resolution rotator)
                </td>
                <td>
                  $4 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_3 = $ scaled-up appearance flow offset <br>(generated by the half-resolution rotator)
                </td>
                <td>
                  $2 \times 512 \times 512$
                </td>
              </tr>                
              <tr class="table-primary">
                <td>
                  $A_4 = A_1$ turned into a 2D tensor 
                </td>
                <td>
                  $6 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-primary">
                <td>
                  $A_5 = \mrm{Concat}(A_0, A_4, A_2, A_3)$
                </td>
                <td>
                  $16 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_1 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Cov3}(A_5)))$
                </td>
                <td>
                  $32 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_2 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_1)))$
                </td>
                <td>
                  $64 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_3 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_2)))$
                </td>
                <td>
                  $128 \times 128 \times 128$
                </td>
              </tr>
              <tr class="table-warning">
                <td>
                  $B_4 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{ConvDown}(B_3)))$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_1 = \mrm{ResNetBlock}(B_4)$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_2 = \mrm{ResNetBlock}(C_1)$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $\qquad \vdots$
                </td>
                <td>
                  $\qquad \vdots$
                </td>
              </tr>
              <tr class="table-success">
                <td>
                  $C_6 = \mrm{ResNetBlock}(C_5)$
                </td>
                <td>
                  $256 \times 64 \times 64$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_1 = \mrm{Concat}(\mrm{UpsampleNn}(C_6), B_3)$
                </td>
                <td>
                  $384 \times 128 \times 128$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_2 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(D_1)))$
                </td>
                <td>
                  $128 \times 128 \times 128$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_3 = \mrm{Concat}(\mrm{UpsampleNn}(D_2), B_2)$
                </td>
                <td>
                  $192 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_4 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(D_2)))$
                </td>
                <td>
                  $64 \times 256 \times 256$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_5 = \mrm{Concat}(\mrm{UpsampleNn}(D_4), B_1)$
                </td>
                <td>
                  $96 \times 512 \times 512$
                </td>
              </tr>
              <tr class="table-danger">
                <td>
                  $D_6 = \mrm{LeakyReLU}(\mrm{InstNorm}(\mrm{Conv3}(D_5)))$
                </td>
                <td>
                  $32 \times 512 \times 512$
                </td>
              </tr>
            </tbody>      
          </table>
        </td>
      </tr>
      <tr>
        <td><b>Table 3.4.3.2</b> Specification of the U-Net network that is the main body of the editor. $D_6$ is the feature vector that the U-Net outputs.</td>
      </tr>
    </table>
    <br>

    <p>Let us note that the U-Net has lower capacity per input pixel than the encoder-decoder of the half-resolution rotator (<a href="#table:half-res-rotator-encoder-decoder">Table 3.4.2.3</a>). This can be seen from the number of channels of $B_1$, which is the first feature tensor both networks compute from the input. For the encoder-decoder, each pixel is allocated 64 channels, but the number is 32 for the U-Net.</p>

    <p>Recall that the time complexity of a convolution on a $C \times H \times W$ tensor is $O(C^2HW)$. Hence, halving the number of channels speeds up convolution by a factor of 4, and doubling the image size (i.e., $H \rightarrow 2H$ and $W \rightarrow 2W$) also slows it down by a factor of 4. Because the U-Net operates on tensors that are 2 times larger in height and width but have 2 times fewer channels, its convolutions thus have the same time complexity as those in the encoder-decoder network because the aforementioned changes cancel each other out. As a result, we can say that the U-Net's time complexity is of the same order of magnitude as that of the half-resolution rotator (modulo, of course, the more complex calculation when scaling feature tensors up after the bottleneck part). Capacity reduction thus keeps the editor fast despite the fact it operates on images that have 4 times more pixels.</p>

    <p>The editor is about 33 MB in size, which is about one forth the size of the half-resolution rotator. This is because a convolution unit that operates on a $C \times H \times W$ tensor requires $O(C^2)$ space to store its parameters. As a result, halving the number of channels reduces size by a factor of 4.</p>

    <p><b>Training procedure.</b> I used a loss function with 4 terms.
    \begin{align*}
      \mathcal{L}_{\mrm{editor}} = E_{(I_\mrm{in},\ve{p},I_\mrm{out}) \sim p_\mrm{data}} \Big[ \lambda_{\mrm{L1}} \mathcal{L}_{\mrm{L1}} + \lambda_{\mrm{percept}} \mathcal{L}_{\mrm{percept}} + \lambda_{\mrm{L1}}^{\mrm{neck}} \mathcal{L}_{\mrm{L1}}^{\mrm{neck}} + \lambda_{\mrm{percept}}^{\mrm{neck}} \mathcal{L}_{\mrm{percept}}^{\mrm{neck}} \Big].
    \end{align*}
    </p>
    
    <p>
    Here, $\mcal{L}_{\mrm{L1}}$ is the L1 difference between the groundtruth image and the final output.
    \begin{align*}
      \mcal{L}_{\mrm{L1}} = \| I_\mrm{out} - I_\mrm{final} \|_1.
    \end{align*}
    </p>

    <p>$\mcal{L}_{\mrm{percept}}$ is the perceptual feature reconstruction loss between $I_\mrm{out}$ and $I_\mrm{final}$. I did not evaluate the loss on the whole $512 \times 512$ images because I found it to be too slow. Instead, I divided the images into 4 quadrants of size $256 \times 256$ and evaluate
    \begin{align*}
      \mcal{L}_{\mrm{percept}} 
      = \frac{1}{4}\Big[ \Phi(I_\mrm{out}^{\mrm{Q1}}, I_\mrm{final}^{\mrm{Q1}}) + \Phi(I_\mrm{out}^{\mrm{Q2}}, I_\mrm{final}^{\mrm{Q2}}) + \Phi(I_\mrm{out}^{\mrm{Q3}}, I_\mrm{final}^{\mrm{Q3}}) + \Phi(I_\mrm{out}^{\mrm{Q4}}, I_\mrm{final}^{\mrm{Q4}}) \Big]
    \end{align*}
    where $I^{\mrm{Q1}}, I^{\mrm{Q2}}, I^{\mrm{Q3}}, I^{\mrm{Q4}}$ denote the 4 quadrants of image $I$. To speed up the computation of the above expression, I estimated it by uniformly sampling a quadrant and only evaluating the loss for that quadrant.
    </p>    

    <p>The $\mathcal{L}_{\mrm{L1}}^{\mrm{neck}}$ and $\mathcal{L}_{\mrm{percept}}^{\mrm{neck}}$ terms were added to alleviate the neck color problem that I encountered in <a href="#fig:neck-color-failure-cases">Figure 3.3.3.2</a>. They are the same as $\mcal{L}_{\mrm{L1}}$ and $\mcal{L}_{\mrm{percept}}$ except that they operate on the $64 \times 64$ subimage around the neck of the character.</p>

    <a name="fig:neck_subimage" />
    <p align="center">
      <table align="center">
        <tr>
            <td align="center">
                <a href="data/kizuna_ai_sample_inputs/neck_subimage.png"><img src="data/kizuna_ai_sample_inputs/neck_subimage.png" width="600"></a>
            </td>
        </tr>
        <tr>
            <td align="center">
                <b>Figure 3.4.3.3</b> The neck subimage that is used to evaluate $\mathcal{L}_{\mrm{L1}}^{\mrm{neck}}$ and $\mathcal{L}_{\mrm{percept}}^{\mrm{neck}}$.
            </td>
        </tr>
      </table>
    </p>

    <p>Let us denote the neck subimage of $I$ by $I^\mrm{neck}$. The two neck loss terms are given by:
    \begin{align*}
      \mathcal{L}_{\mrm{L1}}^{\mrm{neck}} &= \big\| I_\mrm{out}^{\mrm{neck}} - I_\mrm{final}^{\mrm{neck}} \big\|_1, \\
      \mathcal{L}_{\mrm{percept}}^{\mrm{neck}} &= \Phi( I_\mrm{out}^{\mrm{neck}}, I_\mrm{final}^{\mrm{neck}}).
    \end{align*}
    Because $I_\mrm{out}^{\mrm{neck}}$ and $I_\mrm{final}^{\mrm{neck}}$ are only $64 \times 64$ in resolution, directly evaluating the perceptual feature reconstruction loss on them was fast enough that I did not have to split them into quadrants like what I did for $\mathcal{L}_{\mrm{percept}}$.
    </p>

    <p>The coefficients of the terms were
    \begin{align*}
      \lambda_{\mrm{L1}} &= \frac{1}{4}, &
      \lambda_{\mrm{percept}} &= \frac{4 \times 256 \times 256}{5}, \\
      \lambda_{\mrm{L1}}^{\mrm{neck}} &= 16, & 
      \lambda_{\mrm{percept}}^{\mrm{neck}} &= \frac{4 \times 256 \times 256}{5}.
    \end{align*}
    </p>

    <p>Training the editor requires the half-resolution rotator because we have to use it to generate two of the editor's inputs. However, I froze its parameters so that only the editor's parameters were updated during training. Again, I used the Adam algorithm with $\beta_1 = 0.5$, $\beta_2 = 0.999$, learning of $10^{-4}$, and batch size of 8. Training lasted for 6 epochs (3,000,000 examples shown).</p>

    <a name="sec:results"></a>
		<h3>3.5 &nbsp; Results</h3>    

    <a name="sec:ablation-study">
    <h4>3.5.1 &nbsp; Comparison Against Other Design Variations</h4>

    <p>The design of the body rotator I presented the last section is rather counterintuitive: the half-resolution rotator has an output that is always discarded. I came to this design by picking the best one out of many variations.</p>

    <p>I evaluated 2 designs for the half-resolution rotator and 6 designs for the editor. I also considered a design where a standalone network performs the whole body rotation task. Because some half-resolution rotator designs are not compatible with certain editor designs, there were 10 valid variations in total.</p>

    <p><b>Half-resolution rotator designs.</b> Of course, one the of the designs is the one I presented in <a href="#sec:half-res-rotator">Section 3.4.2</a>. The other design is a variation of that design in which the direct generation branch is removed. Let us refer to the simpler design as "Rotator A," and the design in Section 3.4.2 as "Rotator B."</p>

    <p>
      <a name="fig:half-res-rotator-designs" />
      <table cellpadding="10" border="1">
        <tr>
          <td align="center" valign="bottom">
            <a href="data/rotator_designs/rotator_a.png">
              <img src="data/rotator_designs/rotator_a.png" alt="" width="280">
            </a>          
          </td>
          <td align="center" valign="bottom">
            <a href="data/rotator_designs/rotator_b.png">
              <img src="data/rotator_designs/rotator_b.png" alt="" width="280">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Rotator A</b></td>
          <td align="center"><b>Rotator B</b></td>
        </tr>      
      </table>      
      <center>
        <b>Figure 3.5.1.1</b> Half-resolution rotator designs.
      </center>      
    </p>

    <p>Rotator B was trained with the process described in <a href="#sec:half-res-rotator">Section 3.4.2</a>. Rotator A's process was similar because the only changes were the loss functions. The first phase's loss function was
    \begin{align*}
      \mcal{L}_{\mrm{RA},1} = E_{(I_\mrm{in},\ve{p},I_\mrm{out}) \sim p_{\mrm{data}}}\Big[ \| I_\mrm{out} - I_{\mrm{warped}} \| \Big],
    \end{align*}
    and the second phase's loss function was
    \begin{align*}
      \mcal{L}_{\mrm{RA},2} = E_{(I_\mrm{in},\ve{p},I_\mrm{out}) \sim p_{\mrm{data}}}\bigg[ 20 \| I_\mrm{out} - I_{\mrm{warped}} \|_1 + \frac{4 \times 256 \times 256}{5} \Phi(I_\mrm{out}, I_{\mrm{warped}})  \bigg].
    \end{align*}
    </p>

    <p><b>Editor designs.</b> I considered 6 designs for the editor. All of them has the same U-Net (<a href="#table:editor-u-net">Table 3.4.3.2</a>) as their main bodies, but they differ in what inputs they take in and how they generate the final output image.</p>

    <p>As for the inputs, there are 5 data items that the editor can take in.</p>
    
    <ul>
      <li>$I_\mrm{in}$, the original input image.</li>
      <li>$\ve{p}$, the pose vector.</li>
      <li>$I_\mrm{warped}$, the (scaled-up) warped image generated by the half-resolution rotator.</li>      
      <li>$\Delta F$, the (scaled-up) appearance flow offset generated by the half-resolution rotator.</li>
      <li>$I_\mrm{direct}$, the (scaled-up) direct image generated by the half-resolution rotator.</li>
    </ul>

    <p>So, an editor's inputs must form a subset of $\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, I_\mrm{direct}, \Delta F \}$. I explored four subsets.    
    </p>

    <ul>
      <li>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped} \} $</li>
      <li>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, \Delta F \} $</li>
      <li>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, I_\mrm{direct} \} $</li>      
      <li>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, \Delta F, I_\mrm{direct}\} $</li>
    </ul>

    <p>In other words, all editors must take in $I_\mrm{in}$, $\ve{p}$, and $I_\mrm{warped}$, but $\Delta F$ and $I_\mrm{direct}$ are optional.</p>

    <p>As for ways to generate the final output image $I_\mrm{final}$, I explored three approaches.</p>

    <ul>
      <li><b>Approach $\ves{\alpha}$</b>: Generate $I_\mrm{final}$ directly through a direct generation unit.</li>
      <li><b>Approach $\ves{\beta}$</b>: Perform a partial image change on $I_\mrm{warped}$</li>
      <li><b>Approach $\ves{\gamma}$</b>: Modify $\Delta F$ and use the result to warp $I_\mrm{in}$. Then, perform a partial image change on the new warped image.</li>
    </ul>

    <p>Note that Approach $\gamma$ requires $\Delta F$, but the other two approaches do not need it. Based on this observation, there are 6 possible designs as listed in the table and figure below.</p>

    <p>
      <table class="table table-striped">
        <thead class="table-dark"> 
          <tr>
            <th>Name</th>
            <th>Inputs</th>
            <th>How to Generate $I_\mrm{final}$</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Editor U</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped} \}$</td>
            <td>Approach $\alpha$</td>
          </tr>
          <tr>
            <td>Editor V</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, I_\mrm{direct} \}$</td>
            <td>Approach $\alpha$</td>
          </tr>
          <tr>
            <td>Editor W</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped} \}$</td>
            <td>Approach $\beta$</td>
          </tr>        
          <tr>
            <td>Editor X</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, I_\mrm{direct} \}$</td>
            <td>Approach $\beta$</td>
          </tr>
          <tr>
            <td>Editor Y</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, \Delta F \} $</td>
            <td>Approach $\gamma$</td>
          </tr>
          <tr>
            <td>Editor Z</td>
            <td>$\{ I_\mrm{in}, \ve{p}, I_\mrm{warped}, \Delta F, I_\mrm{direct}\} $</td>
            <td>Approach $\gamma$</td>
          </tr>
        </tbody>
      </table>
      <center>
        <b>Table. 3.5.1.2</b> Editor designs.
      </center>
    </p>
    

    <p>
      <a name="fig:editor-designs" />
      <table cellpadding="10" border="1">
        <tr>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_u.png">
              <img src="data/editor_designs/editor_u.png" alt="" width="280">
            </a>          
          </td>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_v.png">
              <img src="data/editor_designs/editor_v.png" alt="" width="280">
            </a>          
          </td>
        </tr>
        <tr>
          <td align="center"><b>Editor U</b></td>
          <td align="center"><b>Editor V</b></td>
        </tr>
        <tr>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_w.png">
              <img src="data/editor_designs/editor_w.png" alt="" width="280">
            </a>          
          </td>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_x.png">
              <img src="data/editor_designs/editor_x.png" alt="" width="280">
            </a>          
          </td>
        </tr>
        <tr>
          <td align="center"><b>Editor W</b></td>
          <td align="center"><b>Editor X</b></td>
        </tr>
        <tr>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_y.png">
              <img src="data/editor_designs/editor_y.png" alt="" width="280">
            </a>          
          </td>
          <td align="center" valign="bottom">
            <a href="data/editor_designs/editor_z.png">
              <img src="data/editor_designs/editor_z.png" alt="" width="280">
            </a>          
          </td>
        </tr>
        <tr>
          <td align="center"><b>Editor Y</b></td>
          <td align="center"><b>Editor Z</b></td>
        </tr>
      </table>      
      <center>
        <b>Figure 3.5.1.3</b> Editor designs.
      </center>
    </p>

    <p>Note that Editor Y is the one previously presented in <a href="#sec:editor">Section 3.4.3</a>.</p>

    <p>To form a complete body rotator, we must connect a half-resolution rotator with an editor. We can see that Rotator A cannot work with any editor that takes $I_\mrm{direct}$ as an input, but Rotator B is compatible with all editors. As a result, there are $3 + 6 = 9$ possible designs.</p>

    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <th></th>
            <th>Editor U</th>
            <th>Editor V</th>
            <th>Editor W</th>
            <th>Editor X</th>
            <th>Editor Y</th>
            <th>Editor Z</th>
          </th>
        </thead>
        <tbody>
          <tr>
            <th class="table-dark" align="center">Rotator A</th>
            <td align="center">〇</td>
            <td align="center">✕</td>
            <td align="center">〇</td>
            <td align="center">✕</td>
            <td align="center">〇</td>
            <td align="center">✕</td>
          </tr>
          <tr>
            <th class="table-dark" align="center">Rotator B</th>
            <td align="center">〇</td>
            <td align="center">〇</td>
            <td align="center">〇</td>
            <td align="center">〇</td>
            <td align="center">〇</td>
            <td align="center">〇</td>
          </tr>
        </tbody>
      </table>
      <b>Table 3.5.1.3</b> Compatibility between half-resolution rotator designs and editor designs. There are 9 valid designs for the body rotator in total.
    </p>

    <p>All editors were trained with the process in <a href="#sec:editor">Section 3.4.3</a>. Note that I must train two copies of each of Editor U, W, and Y because they all belong to two different body rotator designs. One copy must be trained with Rotator A and the other with Rotator B.</p>

    <p><b>Single network design.</b> I also evaluated an architecture where a single network is responsible for performing the body rotation task end-to-end. The network has an encoder-decoder main body whose construction is similar to that in <a href="#table:half-res-rotator-encoder-decoder">Table 3.4.2.3</a>. However, because the input is $512 \times 512$ rather than $256 \times 256$, the encoder-decoder features one extra downsampling step in the yellow section and one extra upsampling step in the red section. The first feature tensor created from the input is of size $32 \times 512 \times 512$ instead of $64 \times 256 \times 256$. The feature tensor outputted by the encoder-decoder is used to warp the input image and then partially change it. The network's architecture is depicted below.</p>

    <p>
      <a name="fig:single-network-design" />
      <table align="center">
        <tr>
          <td>
            <a href="data/rotator_designs/rotator_c.png">
              <img src="data/rotator_designs/rotator_c.png" alt="" width="600">
            </a>
          </td>
        </tr>
        <tr>
          <td align="center">
            <b>Figure 3.5.1.4</b> The single network rotator design.
          </td>
        </tr>
      </table>
    </p>

    <p>The network was trained in two phases. In the first phase, the loss function was 
    \begin{align*}
      \mcal{L}_{\mrm{SNR},1} = E_{I_\mrm{in},\ve{p},I_\mrm{out}} \Big[ \| I_\mrm{out} - I_\mrm{final} \|_1 \Big].
    \end{align*}
    In the second phase, perceptual feature reconstruction losses for the whole image and the neck subimage were added, and the loss function became
    \begin{align*}
    \mcal{L}_{\mrm{SNR},2} = E_{I_\mrm{in},\ve{p},I_\mrm{out}} \Big[ \frac{1}{4} \| I_\mrm{out} - I_\mrm{final} \|_1 + \lambda \mcal{L}_\mrm{percept} + \lambda \mcal{L}_\mrm{percept}^\mrm{neck} \Big]
    \end{align*}
    where $\lambda = (4 \times 256 \times 256) / 5$, and $\mcal{L}_\mrm{percept}$ and $\mcal{L}_\mrm{percept}^\mrm{neck}$ are as defined in <a href="#sec:editor">Section 3.4.3 </a><a href="#fn_snr_loss_function">[*]</a>. Other settings related to the training process (optimization algorithm, batch size, learning rate, and the phase lengths) were exactly the same as those in <a href="#sec:half-res-rotator">Section 3.4.2</a>. 
    </p>

    <div class="footnotes"><ul>        
      <li class="footnote" id="fn_snr_loss_function">					
        Note that, to make the training process comparable with other designs, $\mcal{L}^\mrm{neck}_\mrm{L1}$ should have been present in the second phase's loss. However, I made a mistake, and this is why the term was missing. 
      </li>
    </ul></div>

    <p><b>Quantitative evaluation.</b> I fed each design the test dataset and have it produce one output image per test example. I then computed the similarity between the output and the ground truth image with three similarity metrics: the root mean square error (RMSE), the Structural Similarity (SSIM) metric <a href="#fn_wang_2014">[Wang et al. 2004]</a>, and the Learn Perceptual Image Patch Similar (LPIPS) metric <a href="#fn_zhang_2018">[Zhang et al. 2018]</a>. I report the metric values averaged over the whole dataset in the table below.</p>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_wang_2014">
          <p align="left">
              Zhou Wang, Alan Conrad Bovik, Hamid Rahim Sheikh, and Eero P. Simoncelli.
              <b>Image Quality Assessment: From Error Visibility to Structural Similarity.</b>
              IEEE Transactions on Image Processing, Vol. 13, No 4, April 2004. <a href="https://www.cns.nyu.edu/pub/eero/wang03-reprint.pdf">[Paper]</a>
          </p>
      </li>
        <li class="footnote" id="fn_zhang_2018">
          Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, Oliver Wang.
          <b>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric.</b>
          CVPR. 2018.
          <a href="https://github.com/richzhang/PerceptualSimilarity">[GitHub]</a>
        </li>
      </ul>
    </div>      

    <p>
      <table class="table table-striped">
        <tr class="table-dark">
          <td><b>Body Rotator Design</b></td>
          <td align="right"><b>RMSE</b> $(\downarrow)$</th>
          <td align="right"><b>SSIM</b> $(\uparrow)$</th>
          <td align="right"><b>LPIPS</b>  $(\downarrow)$</th>
        </tr>
        <tr>
          <td>
            Rotator A + Editor U
          </td>
          <td align="right">
            <tt>0.15529800</tt>
          </td>
          <td align="right">
            <tt>0.90527600</tt>
          </td>
          <td align="right">
            <tt>0.06305300</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator A + Editor W
          </td>
          <td align="right">
            <tt>0.15511400</tt>
          </td>
          <td align="right">
            <tt>0.90652400</tt>
          </td>
          <td align="right">
            <tt>0.06088700</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator A + Editor Y
          </td>
          <td align="right">
            <tt>0.16160900</tt>
          </td>
          <td align="right">
            <tt>0.90324800</tt>
          </td>
          <td align="right">
            <tt>0.05088500</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor U
          </td>
          <td align="right">
            <tt>0.15586100</tt>
          </td>
          <td align="right">
            <tt>0.90644100</tt>
          </td>
          <td align="right">
            <tt>0.05237600</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor V
          </td>
          <td align="right">
            <tt>0.15574700</tt>
          </td>
          <td align="right">
            <tt>0.90663700</tt>
          </td>
          <td align="right">
            <tt>0.05211400</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor W
          </td>
          <td align="right">
            <tt>0.15550700</tt>
          </td>
          <td align="right">
            <tt>0.90823300</tt>
          </td>
          <td align="right">
            <tt>0.05051800</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor X
          </td>
          <td align="right">
            <tt>0.15545500</tt>
          </td>
          <td align="right">
            <tt>0.90821200</tt>
          </td>
          <td align="right">
            <tt>0.05051200</tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor Y
          </td>
          <td align="right">
            <tt><b>
              0.15437500
            </b></tt>
          </td>
          <td align="right">
            <tt><b>
              0.90950700
            </b></tt>
          </td>
          <td align="right">
            <tt><b>
              0.04874800
            </b></tt>
          </td>
        </tr>
        <tr>
          <td>
            Rotator B + Editor Z
          </td>
          <td align="right">
            <tt>0.15870300</tt>
          </td>
          <td align="right">
            <tt>0.90583800</tt>
          </td>
          <td align="right">
            <tt>0.05042200</tt>
          </td>
        </tr>
        <tr>
          <td>
            Single network rotator
          </td>
          <td align="right">
            <tt>0.17180100</tt>
          </td>
          <td align="right">
            <tt>0.89646500</tt>
          </td>
          <td align="right">
            <tt>0.05791900</tt>
          </td>
        </tr>
      </table>      
      <b>Table 3.5.1.5</b> Quantitative evaluation of the body rotator designs. $(\downarrow)$ means "lower is better", and $(\uparrow)$ means "higher is better."
    </p>

    <p>First, we can see that the single network design performed worse than all two-network designs according to the RMSE and SSIM metrics. Moreover, its LPIPS score is also much higher than the best two-network design. This result informs us that two networks are better than one.</p>

    <p>Next, one can see that "Rotator B + Editor Y" was the best because all of its three metrics were the best. Interestingly, "Rotator B + Editor Z" has slightly more capacity than "Rotator B + Editor Y," yet it performed worse according to all metrics. An explanation for this result might be that taking the direct image as input actually diverted the network's attention away from how to process the warped image and its appearance flow offset.</p>

    <p>"Rotator B + Editor Y" is also better than "Rotator A + Editor Y" on all metrics. In other words, although we discard direct images generated by Rotator B, having the rotator generate them actually leads to better inputs for the editor. This is an example of improving a task's performance by training a network to also solve related auxiliary tasks <a href="#fn_ruder_2017">[Ruder 2017]</a>.</p>

    <div class="footnotes"><ul>        
      <li class="footnote" id="fn_ruder_2017">					
        Sebastian Ruder.
        <b>An Overview of Multi-Task Learning in Deep Neural Networks.</b> 
        2017.
        <a href="https://arxiv.org/abs/1706.05098">[arXiv]</a>
      </li>
    </ul></div>  

    <p><b>Qualitative evaluation.</b> I created a sequence of pose vectors that contain all 6 types of movements controllable by the body rotator. I then used the designs to animate pictures of eight MMD models according to the pose vector sequence to render 10 videos. I also converted the pose sequence into ones that were applicable to the MMD models and animated them to create ground truth videos. The videos, arranged side by side for comparison, are available in <a href="#fig:compare-design-grid">Figure 3.5.1.6</a>. Another version of the videos where only the faces are shown are available in <a href="#fig:compare-design-grid-face">Figure 3.5.1.7</a>.</p>
  </div>

  <div class="container" style="max-width: 1080px">
    <a name="fig:compare-design-grid" />
		<table border="1" cellpadding="5">
		<tr>
		<td id="compareDesignGridVideoCell">
			<video muted controls loop width="1024">
			    <source src="data/eval/kizuna_ai/grid.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="compareDesignGridCharacterSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>
      <option value="minato_aqua">Minato Aqua</option>
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="suou_patra">Suou Patra</option>
			<option value="inaba_haneru">Inaba Haneru</option>			
			<option value="kitakami_futaba">Kitakami Futaba</option>
			<option value="kagura_suzu">Kagura Suzu</option>
		</select>
		</td>
		</tr>
		</table>
		<b>Figure 3.5.1.6</b> Comparison of animations generated by the 10 body rotator designs evaluated in this section. Again, for each character, the ground truth animation was created by rendering its 3D model. Other animations were generated by the body rotators to animate the first frame of the ground truth animation.
		<br>
		<br>
	</div>

  <script type="text/javascript">
		function changeCompareDesignGridCharacter() {
			var newTarget = $("#compareDesignGridCharacterSelect").val();
			var videoFileName = "data/eval/" + newTarget + "/grid.mp4";
			var html = "<video muted controls loop width=\"1024\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#compareDesignGridVideoCell").html(html);
		}

		$("#compareDesignGridCharacterSelect").change(changeCompareDesignGridCharacter);
	</script>

  <div class="container" style="max-width: 1080px">
    <a name="fig:compare-design-grid-face" />
		<table border="1" cellpadding="5">
		<tr>
		<td id="compareDesignGridFaceVideoCell">
			<video muted controls loop width="1024">
			    <source src="data/eval/kizuna_ai/grid_face.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="compareDesignGridFaceCharacterSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>
      <option value="minato_aqua">Minato Aqua</option>
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="suou_patra">Suou Patra</option>
			<option value="inaba_haneru">Inaba Haneru</option>			
			<option value="kitakami_futaba">Kitakami Futaba</option>
			<option value="kagura_suzu">Kagura Suzu</option>
		</select>
		</td>
		</tr>
		</table>
		<center><b>Figure 3.5.1.7</b> The same animations in Figure 3.5.1.6 but with the faces being zoomed in.</center><br>
	</div>

	<script type="text/javascript">
		function changeCompareDesignGridFaceCharacter() {
			var newTarget = $("#compareDesignGridFaceCharacterSelect").val();
			var videoFileName = "data/eval/" + newTarget + "/grid_face.mp4";
			var html = "<video muted controls loop width=\"1024\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#compareDesignGridFaceVideoCell").html(html);
		}

		$("#compareDesignGridFaceCharacterSelect").change(changeCompareDesignGridFaceCharacter);
	</script>

  <div class="container" style="max-width: 640px;">
    <p>From the animations, we can see the reasons why the chosen design (Rotator B + Editor Y) were clearly better than some alternatives.</p>

    <p><b><i>Comparison against single network design.</i></b> The single network design can produce black contours that are too large.</p>
    <p>
      <table align="center">
        <tr>
          <td align="center" valign="bottom"><b>Ground truth</b></td>
          <td align="center" valign="bottom"><b>Single network</b></td>
          <td align="center" valign="bottom"><b>Rotator B + Editor Y<br>(Section 3.4)</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/ground_truth/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/rotator_d/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/ground_truth/image/00000000.png" width="400" height="400" y="-80" x="-100"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/rotator_d/image/00000000.png" width="400" height="400" y="-80" x="-100"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/92/rotator_b_editor_y/image/00000000.png" width="400" height="400" y="-80" x="-100"/>
            </svg>
          </td>
        </tr>
      </table>
    </p>

    <p>It can also erase thin structures too aggressively.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center" valign="bottom"><b>Ground truth</b></td>
          <td align="center" valign="bottom"><b>Single network</b></td>
          <td align="center" valign="bottom"><b>Rotator B + Editor Y<br>(Section 3.4)</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/ground_truth/image/00000000.png" width="200" height="200" y="-25" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/rotator_d/image/00000000.png" width="200" height="200" y="-25" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-25" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/ground_truth/image/00000000.png" width="400" height="400" y="-50" x="-150"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/rotator_d/image/00000000.png" width="400" height="400" y="-50" x="-150"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/tokino_sora/29/rotator_b_editor_y/image/00000000.png" width="400" height="400" y="-50" x="-150"/>
            </svg>
          </td>
        </tr>
      </table>
    </p>

    <p><b><i>Comparison against "Rotator A" designs.</i></b> "Rotator A + Editor U" and "Rotator W" can produce aliasing artifacts and remove high frequency details from the outputs. (See the character's eyes in the figure below.)</p>
  </div>

  <div class="container" style="max-width: 1024px">
    <p>
      <table align="center">
        <tr>
          <td align="center" valign="bottom"><b>Ground truth</b></td>
          <td align="center" valign="bottom"><b>Rotator A + Editor U</b></td>
          <td align="center" valign="bottom"><b>Rotator A + Editor W</b></td>
          <td align="center" valign="bottom"><b>Rotator B + Editor Y<br>(Section 3.4)</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/ground_truth/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_a_editor_u/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_a_editor_w/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="75" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/ground_truth/image/00000000.png" width="400" height="400" x="-75" y="-225" />
            </svg>
          </td>
          <td>
            <svg width="200" height="75" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_a_editor_u/image/00000000.png" width="400" height="400" x="-75" y="-225" />
            </svg>
          </td>
          <td>
            <svg width="200" height="75" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_a_editor_w/image/00000000.png" width="400" height="400" x="-75" y="-225" />
            </svg>
          </td>
          <td>
            <svg width="200" height="75" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/90/rotator_b_editor_y/image/00000000.png" width="400" height="400" x="-75" y="-225" />
            </svg>
          </td>
        </tr>
      </table>
    </p>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>Moreover, I also observed that designs with "Rotator A" could produce very incorrect distortions.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center" valign="bottom"><b>Ground truth</b></td>
          <td align="center" valign="bottom">&nbsp;</td>
          <td align="center" valign="bottom"><b>Rotator B + Editor Y<br>(Section 3.4)</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/minato_aqua/91/ground_truth/image/00000000.png" width="200" height="200" y="-20" />
            </svg>
          </td>
          <td>
            &nbsp;
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/minato_aqua/91/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-20" />
            </svg>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Rotator A + Editor U</b></td>
          <td align="center"><b>Rotator A + Editor W</b></td>
          <td align="center"><b>Rotator A + Editor Y</b></td>
        </tr>
        <tr>          
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/minato_aqua/91/rotator_a_editor_u/image/00000000.png" width="200" height="200" y="-20" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/minato_aqua/91/rotator_a_editor_w/image/00000000.png" width="200" height="200" y="-20" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/minato_aqua/91/rotator_a_editor_y/image/00000000.png" width="200" height="200" y="-20" />
            </svg>
          </td>
        </tr>
      </table>
    </p>

    <p>There were, however, no major differences between the outputs of the chosen design (Rotator B + Editor Y) and other designs with "Rotator B." So, the choice of Editor Y was mainly informed by the quantitative comparison.</p>

    <p>In conclusion, I chose the "Rotator B + Editor Y" design because its performance, as indicated by the similarity metrics, was the best. Moreover, its outputs also looked better than ones produced by the single network design and those with "Rotator A."</p>

    <a name="sec:other-work-comparison"></a>
    <h4>3.5.2 &nbsp; Comparison with Other Work</h4>

    <p>There are many previous works that can generate animation from a single image, and I have surveyed them in details in the <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#background">write-up of my 2021 project</a>. All of my VTuber-related projects solve the problem of <b>parameter-based posing</b> where the input consists of a single image of the character and a pose vector, and the task is to pose the character accordingly. There is a related problems of <b>motion transfer</b>, where we are given an image or a video of a subject (the <i>source</i>), and we have to make another subject (the <i>target</i>) imitate the pose of the source.</p>

    <p>In the 2021 project, I compared my system to those proposed by Averbuch-Elor et al. <a href="#fn_averbuch_elor_2017">[2017]</a> and Siarohin's et al. <a href="#fn_siarohin_2019">[2019]</a>. While I was able to show that my system outperformed them when animating anime characters, I was not comparing apples to apples because other systems solve motion transfer, but mine solves parameter-based posing. In particular, other systems were at a disadvantage when the source character was not the same as the target character.</p>
    
    <p>There were previous works on parameter-based posing, but they were either already a part of my system (such as Pumarola et al.'s work) or not convenient to compare against (such as Ververas's and Zaferiou's <a href="#fn_ververas_2020">[2020]</a>). Fortunately, later in 2021, Ren et al. proposed a new neural network system for parameter-based posing called PIRenderer <a href="#fn_ren_2021">[2021]</a>. So, in this article, I will compare my new system against it.</p>

    <p>PIRenderer's overall design is similar to that of my body rotator. There is a network the produces a low-resolution appearance flow, and it is followed by a network that tries to refine the resulting warped image. However, the authors seem to be inspired by StyleGAN <a href="#fn_karras_2019">[Karras et al. 2019]</a> as they use a mapping network to convert the input pose into a latent code that is used to modulate other networks through adaptive instance normalization (AdaIN) <a href="#fn_huang_2017">[Huang and Belongie 2017]</a>. My networks, on the other hands, do not use any of these structures. Both PIRenderer and my system were trained with the perceptual losses introduced by Johnson et al. <a href="#fn_johnson_2016_2">[2016]</a>. The difference is that PIRenderer uses both the content and style loss, but my system only uses the former.</p>

    <div class="footnotes">
			<ul>					
				<li class="footnote" id="fn_averbuch_elor_2017">Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. <b>Bringing Portraits to Life.</b> SIGGRAPH Asia 2017. <a href="http://cs.tau.ac.il/~averbuch1/portraitslife/index.htm">[Project]</a></li>
				<li class="footnote" id="fn_siarohin_2019">Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. <b>First Order Motion Model for Image Animation</b>. NeurIPS 2019. <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">[Project]</a></li>
        <li class="footnote" id="fn_ververas_2020">
          Evangelos Ververas and Stefanos Zafeiriou.
          <b>SliderGAN: Synthesizing Expressive Face Images by Sliding 3D Blendshape Parameters.</b>
          Int J Comput Vis 128, 2629-2650 (2020). 
          <a href="https://link.springer.com/article/10.1007/s11263-020-01338-7">[Paper]</a>
        </li>
        <li class="footnote" id="fn_ren_2021">
          Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, and Shan Liu.
          <b>PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering.</b>
          ICCV 2021.
        </li>        
			</ul>
		</div>

    <p>While the system's <a href="https://github.com/RenYurui/PIRender">source code</a> is publicly available, I did not use it directly because it was easier for me to reimplement and adapt it to my coding framework. I also introduced the following changes.</p>

    <p>First, to make PIRenderer compatible with my system, I made the mapping network take a single pose vector as input instead of a window of pose vectors inside an animation sequence. This change simplifies its architecture because the "center crop" unit is no longer needed. The mapping network thus became a multi-layer perceptron (MLP) that turns a 6-dimensional pose vector into a 256-dimensional latent vector . Each of its hidden layer has 256 units.</p>

    <p>Second, to make a fair comparison between PIRenderer and my system, I adjusted PIRenderer's hyperparameters to make its networks roughly the same size as my networks. In particular, I raised the number of of maximum channels in each layer from 256 to 512 and then made the following adjustments.
    <ul>
      <li>I set the width of the warper to 40 to make it 130 MB in size, which is also about the size of my half resolution rotator (<a href="#sec:half-res-rotator">Section 3.4.2</a>).</li>

      <li>I set the width of the editor to 44 to make it about 32 MB in size, which is about the size of my editor (<a href="#sec:editor">Section 3.4.3</a>).</li>
    </ul>

    <p>Third, I changed PIRenderer's training process to be similar to the way I trained my networks. Training has two phases. In the first phase, the mapper and the warper were trained together for 12 epochs (6,000,000 examples). In the second phase, the mapper are the warper were frozen, and the editor were trained for 6 epochs (3,000,000 examples). Both phases used batch size of 8, learning rate of $10^{-4}$, and the Adam algorithm with $\beta_1 = 0.5$ and $\beta_2 = 0.999$. The main reason for not training all the networks together was that my GPU's memory could not handle the batch size of 8.</p>
  </p>    

    <p><b>Quantitative evaluation.</b>  I evaluated my system against PIRenderer with the three image similarity metrics used in the last subsection. The scores, computed with the test dataset, are given in the table below.</p>

    <p>
      <table class="table table-striped">
        <tr class="table-dark">
          <td><b>System</b></td>
          <td align="right"><b>RMSE</b> $(\downarrow)$</th>
          <td align="right"><b>SSIM</b> $(\uparrow)$</th>
          <td align="right"><b>LPIPS</b>  $(\downarrow)$</th>
        </tr>        
        <tr>
          <td>
            PIRenderer
          </td>
          <td align="right">
            <tt>
              0.16772200
            </tt>
          </td>
          <td align="right">
            <tt>
              0.89525700
            </tt>
          </td>
          <td align="right">
            <tt>
              0.05257600
            </tt>
          </td>
        </tr>
        <tr>
          <td>
            My system (<a href="#sec:networks">Section 3.4</a>)
          </td>
          <td align="right">
            <tt><b>
              0.15446600 	
            </b></tt>
          </td>
          <td align="right">
            <tt><b>
              0.90975400 	
            </b></tt>
          </td>
          <td align="right">
            <tt><b>
              0.04807400
            </b></tt>
          </td>
        </tr>        
      </table>
      <b>Table 3.5.2.1</b> Quantitative comparison between PIRenderer and my system. $(\downarrow)$ means "lower is better", and $(\uparrow)$ means "higher is better."
    </p>

    <p><b>Qualitative evaluation.</b> I used PIRenderer and my system to generate animations of the eight 3D models used to quantitatively compare my system against alternative architectures. The results are given in <a href="#fig:compare-pirenderer-grid">Figure 3.5.2.2</a> (full body) and <a href="#fig:compare-pirenderer-face-grid">Figure 3.5.2.3</a> (face zoom).</p>
  </div>
  
  <div class="container" style="max-width: 1080px">
    <a name="fig:compare-pirenderer-grid" />
		<table border="1" cellpadding="5">
		<tr>
		<td id="comparePirendererGridVideoCell">
			<video muted controls loop width="1024">
			    <source src="data/eval/kizuna_ai/pirenderer_grid.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="comparePirendererGridCharacterSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>
      <option value="minato_aqua">Minato Aqua</option>
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="suou_patra">Suou Patra</option>
			<option value="inaba_haneru">Inaba Haneru</option>			
			<option value="kitakami_futaba">Kitakami Futaba</option>
			<option value="kagura_suzu">Kagura Suzu</option>
		</select>
		</td>
		</tr>
		</table>
		<b>Figure 3.5.2.2</b> Comparison between the ground truth 3D animations and videos generated by PIRenderer and my system. The characters are Kizuna AI (&copy; Kizuna AI), <a href="https://www.youtube.com/channel/UCp6993wxpyDPHUpavwDFqgg">Tokino Sora</a> (&copy; Tokino Sora Ch.), <a href="https://t.co/JLElyrjgQe">Minato Aqua</a> (&copy; 2019 cover corp.), <a href="https://www.youtube.com/channel/UCom8rCUQZP98SIXJzMwjrxw">Akiyama Rentarou</a> (&copy; ひま食堂), Suou Patra (&copy; HoneyStrap), <a href="https://www.youtube.com/channel/UC0Owc36U9lOyi9Gx9Ic-4qg">Inaba Haneru</a> (&copy; 有閑喫茶あにまーれ), <a href="https://www.youtube.com/channel/UC5nfcGkOAm3JwfPvJvzplHg">Kitakami Futaba</a> (&copy; Appland, Inc.), and <a href="https://www.youtube.com/channel/UCUZ5AlC3rTlM-rA2cj5RP6w">Kagura Suzu</a> (&copy; Appland, Inc.).
		<br>
		<br>
	</div>

  <script type="text/javascript">
		function changeComparePirendererGridCharacter() {
			var newTarget = $("#comparePirendererGridCharacterSelect").val();
			var videoFileName = "data/eval/" + newTarget + "/pirenderer_grid.mp4";
			var html = "<video muted controls loop width=\"1024\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#comparePirendererGridVideoCell").html(html);
		}

		$("#comparePirendererGridCharacterSelect").change(changeComparePirendererGridCharacter);
	</script>

  <div class="container" style="max-width: 768px">
    <a name="fig:compare-pirenderer-grid-face" />
    <table border="1" cellpadding="5">
    <tr>
    <td id="comparePirendererGridFaceVideoCell">
      <video muted controls loop width="768">
          <source src="data/eval/kizuna_ai/pirenderer_grid_face.mp4" type="video/mp4">
      </video>
    </td>		
    </tr>
    <tr>
    <td align="center">
      Target Character: <select id="comparePirendererGridFaceCharacterSelect">
      <option value="kizuna_ai" selected="selected">Kizuna AI</option>
      <option value="tokino_sora">Tokino Sora</option>
      <option value="minato_aqua">Minato Aqua</option>
      <option value="akiyama_rentarou">Akiyama Rentarou</option>
      <option value="suou_patra">Suou Patra</option>
      <option value="inaba_haneru">Inaba Haneru</option>			
      <option value="kitakami_futaba">Kitakami Futaba</option>
      <option value="kagura_suzu">Kagura Suzu</option>
    </select>
    </td>
    </tr>
    </table>
    <center>
      <b>Figure 3.5.2.3</b> The same animations in Figure 3.5.2.2 but with the faces being zoomed in.
    </center>    
    <br>    
  </div>
  <script type="text/javascript">
    function changeComparePirendererGridFaceCharacter() {
      var newTarget = $("#comparePirendererGridFaceCharacterSelect").val();
      var videoFileName = "data/eval/" + newTarget + "/pirenderer_grid_face.mp4";
      var html = "<video muted controls loop width=\"768\">"
        + "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
        + "</video>";
      $("#comparePirendererGridFaceVideoCell").html(html);
    }

    $("#comparePirendererGridFaceCharacterSelect").change(changeComparePirendererGridFaceCharacter);
  </script>

  <div class="container" style="max-width: 640px;">
    <p>Figure 3.5.2.4 lists some differences I could observe from the animations. In general, my system produced blurrier images than PIRenderer did, and it could also erase thin structures such as hair strands and ribbons while PIRenderer preserved them better. However, I preferred my system over PIRenderer because the latter could deform faces in undesirable ways while my system preserved their shapes better.</p>

    <p>
      <table align="center">
        <tr>
          <td align="center"><b>Ground truth</b></td>
          <td align="center"><b>PIRenderer</b></td>
          <td align="center"><b>My System</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/ground_truth/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/pirenderer/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/ground_truth/image/00000000.png" width="400" height="400" y="-120" x="-180"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/pirenderer/image/00000000.png" width="400" height="400" y="-120" x="-180"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/inaba_haneru/186/rotator_b_editor_y/image/00000000.png" width="400" height="400" y="-120" x="-180"/>
            </svg>
          </td>          
        </tr>
        <tr>
          <td colspan="3">
            <table border="1" cellpadding="10">
              <tr>                
                <td>
                  <font size="2">
                    My system generated images that were blurrier than those generated by PIRenderer. It also tended to blur out or completely remove thin structures such as the ribbons above. On the other hand, PIRenderer's images were sharper, and thin structures were better preserved.
                  </font>                  
                </td>
              </tr>
            </table>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Ground truth</b></td>
          <td align="center"><b>PIRenderer</b></td>
          <td align="center"><b>My System</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/ground_truth/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/pirenderer/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
        </tr>        
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/ground_truth/image/00000000.png" width="800" height="800" y="-500" x="-160"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/pirenderer/image/00000000.png" width="800" height="800" y="-500" x="-160"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kizuna_ai/90/rotator_b_editor_y/image/00000000.png" width="800" height="800" y="-500" x="-160"/>
            </svg>
          </td>          
        </tr>
        <tr>
          <td colspan="3">
            <table border="1" cellpadding="10">
              <tr>                
                <td>
                  <font size="2">
                    However, I observed that PIRenderer's outputs contained more aliasing artifacts (i.e., jagginess) than those of my system (which tended to be smoother and sometimes overly blurry). Aliasing is very noticeable in animation, and so I preferred my system over PIRenderer in this regard.
                  </font>                  
                </td>
              </tr>
            </table>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Ground truth</b></td>
          <td align="center"><b>PIRenderer</b></td>
          <td align="center"><b>My System</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/ground_truth/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/pirenderer/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
          <td>
            <svg width="200" height="165" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-35" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/ground_truth/image/00000000.png" width="400" height="400" y="-200" x="-100"/>/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/pirenderer/image/00000000.png" width="400" height="400" y="-200" x="-100"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/suou_patra/179/rotator_b_editor_y/image/00000000.png" width="400" height="400" y="-200" x="-100"/>/>
            </svg>
          </td>          
        </tr>
        <tr>
          <td colspan="3">
            <table border="1" cellpadding="10" width="100%">
              <tr>                
                <td>
                  <font size="2">
                    When the angles were large, PIRenderer sometimes rotated the face less accurately than my system did.
                  </font>                  
                </td>
              </tr>
            </table>
          </td>
        </tr>
        <tr>
          <td align="center"><b>Ground truth</b></td>
          <td align="center"><b>PIRenderer</b></td>
          <td align="center"><b>My System</b></td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/ground_truth/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/pirenderer/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
          <td>
            <svg width="200" height="150" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/rotator_b_editor_y/image/00000000.png" width="200" height="200" y="-50" />
            </svg>
          </td>
        </tr>
        <tr>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/ground_truth/image/00000000.png" width="400" height="400" y="-240" x="-100"/>/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/pirenderer/image/00000000.png" width="400" height="400" y="-240" x="-100"/>
            </svg>
          </td>
          <td>
            <svg width="200" height="100" style="border: 1px solid #000;">
              <image xlink:href="data/eval/cutout/kitakami_futaba/179/rotator_b_editor_y/image/00000000.png" width="400" height="400" y="-240" x="-100"/>/>
            </svg>
          </td>          
        </tr>
        <tr>
          <td colspan="3">
            <table border="1" cellpadding="10" width="100%">
              <tr>                
                <td>
                  <font size="2">
                    Moreover, PIRenderer sometimes disfigured the character's face, but I could not observed my system doing so.
                  </font>                  
                </td>
              </tr>
            </table>
          </td>
        </tr>
      </table>
      <b>Figure 3.5.2.4</b> Some observed qualitative differences between outputs generated by PIRenderer and my system.
    </p>

    <p>A major problem with PIRenderer is its overuse of warping. This becomes the most noticeable when a character wears cloth with a turtleneck that extends upward to just below the chin when seen from the front. When the character turns its face up, PIRenderer would use warping to lift the chin, but the warping would also drag the turtleneck with the chin as well. My system, on the other hand, can use local image change to hallucinate the neck skin that is supposed to become visible, resulting in a much more plausible output.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <video muted controls loop autoplay playsinline width="512">
              <source src="data/eval/aiba_uiha/pirenderer_grid_face.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
      </table>
      <b>Figure 3.5.2.5</b> PIRenderer mainly transforms the input image with warping and so did not produce sensible outputs when a character wearing a turtle neck turns her face up and down. My system, on the other hand, did not exhibit this problem. The character is <a href="https://www.youtube.com/channel/UCnRQYHTnRLSF0cLJwMnedCg">Aiba Uiha</a> (&copy; ANYCOLOR, Inc.)
    </p>

    <p>In conclusion, my system was better at rotating an anime character's head and body than PIRenderer. It better preserved the head's shape, produced fewer artifacts, and could hallucinate disoccluded neck skin. On the other hand, PIRenderer would drag pixels just below the chin around when the face moved.</p>

    <a name="sec:improving-efficiency"></a>
		<h2>4 &nbsp; Improving Efficiency</h2>

		<p>In the last section, I proposed a new body rotator network that can animate the upper body of anime characters. However, recall from <a href="#fig:new-system-overview">Figure 3.2.1</a> that it is a part of a larger system that can also modify facial expression. While the system as a whole became a little smaller (517 MB instead of 600 MB) due to the editor being smaller than the combiner, it is still quite large. It also takes about 35 ms to process an image end to end using my Titan RTX graphics card. The large size makes it impractical to deploy it on mobile devices, and processing speed would only worsen when using less powerful GPUs. It is thus crucial to make the system smaller and faster in order to improve its versatility. I discuss my attempt to do so in this section.</p>

    <a name="results"></a>
		<h3>4.1 &nbsp; Techniques Used</h3>

    <p>To improve my system's efficiency, I experimented with two techniques.</p>

    <p><b>Depthwise separable convolutions.</b> The technique was introduced by Sifre in 2014 <a href="#fn_sifre_2014_2">[Sifre 2014]</a>, but I was particularly inspired by Howard et al.'s MobileNets paper <a href="#fn_howard_2017">[Howard et al. 2017]</a>.</p>
    
    <p>All networks in my system are <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional neural networks</a> (CNNs), meaning that their main building blocks are convolution layers. Such a layer typically takes in a tensor of size $C_1 \times H \times W$ and <a href="https://en.wikipedia.org/wiki/Convolution">convolves</a> it with a kernel of size $C_1 \times C_2 \times K \times K$ in order to produce a new tensor of size $C_2 \times H \times W$. The time complexity of this operation is $O(C_1 C_2 H W K^2)$, and an $O(C_1 C_2 K^2)$ space is required to store the layer's parameters.</p>

    <p>To improve the networks' efficiency, I replaced all convolution layers in their main bodies (i.e., the encoder-decoder networks and the U-Nets) with two convolution layers that are applied in succession.</p>

    <ul>
      <li>The first is a "depthwise" convolution layer whose kernel is a tensor of size $C_1 \times K \times K$. The kernel is applied to the input tensor in a depthwise fashion: each channel in the input tensor is considered to be an independent $1 \times H \times W$ tensor, and it operates on the corresponding $1 \times K \times K$ slice of the filter. The operation produces a new $C_1 \times H \times W$ tensor and takes $O(C_1 H W K^2)$ time. The layer requires $O(C_1 K^2)$ space.</li>

      <li>The second is a standard, but "pointwise" convolution layer with a kernel of size $C_1 \times C_2 \times 1 \times 1$. It thus takes $O(C_1 C_2 H W)$ time to do its operation and $O(C_1 C_2)$ space to store its parameters.</li>
    </ul>

    <p>The effect of separating a standard convolution layer into two is that the time complexity reduces to $O(C_1 H W K^2 + C_1 C_2 H W) = O(C_1 H W (K^2 + C_2))$ and space complexity reduces to $O(C_1K^2 + C_1C_2) = O(C_1 (K^2 + C_2))$. Thus, by employing the technique, both the time and space complexities are reduced by a factor of
    \begin{align*}
      \frac{C_1 H W (K^2 + C_2)}{C_1 C_2 H W K^2} 
      = \frac{C_1(K^2 + C_2)} {C_1 C_2 K^2}
      = \frac{K^2 + C_2}{K^2 C_2}
      = \frac{1}{C_2} + \frac{1}{K^2}.
    \end{align*}
    In my networks, $K$ is 3 most of the time, and $C_2$ is typically at least 32. Thus, in theory, the system could become about 9 times faster and smaller.</p>

    <div class="footnotes"><ul>
      <li class="footnote" id="fn_sifre_2014_2">					
        L. Sifre. <b>Rigid-motion scattering for image classification.</b> Ph.D. Thesis 2014. <a href="https://www.di.ens.fr/data/publications/papers/phd_sifre.pdf">[PDF]</a>
      </li>
      <li class="footnote" id="fn_howard_2017">					
        Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam. <b>MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.</b> 2017. <a href="https://arxiv.org/abs/1704.04861">[arXiv]</a>
      </li>
    </ul></div>

    <p>Note that, while depthwise separable convolutions can reduce the amount of memory needed to store <b><i>model parameters</i></b>, it does not reduce the amount of memory needed to store <b><i>data tensors</i></b> while inference is taking place. More specifically, if $b$ is the number of bytes used to represent an element of a data tensor, the input tensor would need $b C_1 H W$ bytes, and the output tensors $b C_2 H W$ bytes. So, assuming that standard convolution does not produce any intermediate tensors, it would need $b (C_1 + C_2) H W$ bytes to store both the input and output. On the other hand, depthwise separable convolutions have to produce an intermediate tensor of size $C_1 \times H \times W$. As a result, the first convolution would need $2bC_1HW$ bytes, and the second $b(C_1 + C_2)HW$ bytes. The needed amount of memory would be $b [C_1 + \max(C_1,C_2)] HW$ assuming the input is discarded after we have obtained the output. Clearly, this is not smaller than $b(C_1+C_2)HW$. To conclude, the technique can reduce the HDD space needed to store a model, but it may not reduce the amount of RAM used during inference.</p>
  
    <p><b>Using "half."</b> I implemented my network with <a href="http://pytorch.org">PyTorch</a>, which uses the 32-bit floating point type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html">float</a>) to represent almost all data and parameters. Nevertheless, the numerical precision afforded by float might not be necessary, and so PyTorch features an option to use the 16-bit floating type (aka <a href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html">half</a>) instead. While I was not sure a priori how much using half can improve processing speed, there's the obvious benefit that both the system's size and RAM usage would become two times smaller.</p>

    <a name="results"></a>
		<h3>4.2 &nbsp; Results</h3>

    <p>The whole system consists of 5 subnetworks. (3 from the <a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#networks">2021 system</a>, and 2 from <a href="#sec:networks">Section 3.4</a>.) For each subnetwork, I created four variants based on the combination of techniques used. If a variant uses depthwise separable convolutions, it is designated with the word "separable;" otherwise, it is designated with the word "standard." A variant is designed with the floating point type it uses. As a result, the variants are referred to as "standard-float," "separable-float," "standard-half," and "separable-half."</p>

    <p>To create the variants, I trained stardard-float and seperable-float models from scratch. I then created standard-half and separable-half models from the corresponding "float" models by convering all parameters to "half." Note that standard-float is the variant that receives no efficiency improvements, serving as the control group.</p>

    <h4>4.2.1 &nbsp; System Size</h4>

    <p>The clearest benefit of the techniques is size reduction. As predicted, using depthwise separable convolution reduced the size by a factor of about 9, and using half cuts it further in half.</p>
  </div>

  <div class="container" style="max-width: 800px">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>Size in MB (and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Networks</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#eyebrow-segmenter">Eyebrow segmenter</a>
            </td>
            <td align="right">
              <tt>
                120.11
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                12.70
              </tt>
              <br />
              (9.46x)
            </td>
            <td align="right">
              <tt>
                60.07
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                6.36
              </tt>
              <br />
              (18.87x)
            </td>
          </tr>
          <tr>
            <td>
              <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#eyebrow-warper">Eyebrow warper</a>
            </td>
            <td align="right">
              <tt>
                120.32
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                12.72
              </tt>
              <br />
              (9.46x)
            </td>
            <td align="right">
              <tt>
                60.17
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                6.37
              </tt>
              <br />
              (18.88x)
            </td>
          </tr>
          <tr>
            <td>
              <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#eye-mouth-morpher">Eye & mouth morpher</a>
            </td>
            <td align="right">
              <tt>
                120.59
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                12.75
              </tt>
              <br />
              (9.45x)
            </td>
            <td align="right">
              <tt>
                60.31
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                6.39
              </tt>
              <br />
              (18.86x)
            </td>
          </tr>
          <tr>
            <td>
              <a href="#sec:half-res-rotator">Half-resolution rotator</a>
            </td>
            <td align="right">
              <tt>
                124.62
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                13.69
              </tt>
              <br />
              (9.10x)
            </td>
            <td align="right">
              <tt>
                62.32
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                6.86
              </tt>
              <br />
              (18.16x)
            </td>
          </tr>
          <tr>
            <td>
              <a href="#sec:editor">Editor</a>
            </td>
            <td align="right">
              <tt>
                31.92
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                3.63
              </tt>
              <br />
              (8.80x)
            </td>
            <td align="right">
              <tt>
                15.97
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                1.83
              </tt>
              <br />
              (17.45x)
            </td>
          </tr>
          <tr>
            <td>
              Whole system
            </td>
            <td align="right">
              <tt>
                517.56
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                55.48
              </tt>
              <br />
              (9.33x)
            </td>
            <td align="right">
              <tt>
                258.84
              </tt>
              <br />
              (2.00x)
            </td>
            <td align="right">
              <tt>
                27.82
              </tt>
              <br />
              (18.60x)
            </td>
          </tr>
        </tbody>
      </table>      
      <b>Table 4.2.1.1</b> The effect of the efficiency improvement techniques on the size in MB of the networks and the whole system.
    </p>
  </div>

  <div class="container" style="max-width: 640px;">
    <h4>4.2.2 &nbsp; RAM Usage</h4>

    <p>As noted earlier, making the networks smaller does not always means that they would overall use less memory. To see the techniques' impact on memory requirement, I measured GPU RAM usage by the following process.</p>

    <ul>
      <li>Calling <a href="https://pytorch.org/docs/stable/generated/torch.cuda.empty_cache.html"><tt>torch.cuda.empty_cache</tt></a>.</li>
      <li>Calling <a href="https://pytorch.org/docs/stable/generated/torch.cuda.reset_peak_memory_stats.html"><tt>torch.cuda.reset_peak_memory_stats</tt></a>.</li>
      <li>Running a network or the whole system 10 times on an artificial input (tensors whose values are all zero) of batch size 1. <a href="#fn_batch_size_1">[footnote]</a></li>
      <li>Calling <a href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html"><tt>torch.cuda.max_memory_allocated</tt></a> to get the maximum amount of memory occupied by tensors while running the network/system.</li>
    </ul>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_batch_size_1">I chose 1 as the batch size because it corresponded to how the system would be used in a VTuber application. Here, we are typically interested in controlling one character at a time.</li>
      </ul>
    </div>

    <p>I conducted experiments on three computers:</p>

    <ol>
      <li><b><i>Computer A</i></b> is a desktop PC with an Nvidia Titan RTX GPU (driver version 511.79, CUDA version 10.2.89), a 3.60 GHz Intel Core i9-9900KF CPU, and 64 GB of RAM. It represents a high-end gaming PC.</li>

      <li><b><i>Computer B</i></b> is a desktop PC with an Nvidia GeForce GTX 1090 Ti GPU (driver version 456.71, CUDA version 10.2.89), a 3.70 GHz Intel Core i7-8700K CPU, and 32 GB of RAM. It represents a typical (yet somewhat outdated) gaming PC.</li>

      <li><b><i>Computer C</i></b> is a laptop with an Nvidia GeForce MS250 GPU (driver version 511.65, CUDA version 11.6.2), a 1.19 GHz Intel Core i5-1035G1 CPU, and 8 GB of RAM. It represents a general PC with low processing power.</li>
    </ol>

    <p>The measurement values are given in the tables below.</p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>&nbsp;</td>
          <td align="center" colspan="4"><b>RAM usage in MB on Computer A<br>(and improvement over standard-float)</b></td>
        </tr>
        <tr>
          <th>
            Networks
          </th>
          <th>
            standard_float
          </th>
          <th>
            separable_float
          </th>
          <th>
            standard_half
          </th>
          <th>
            separable_half
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            Eyebrow segmenter
          </td>
          <td align="right">
            <tt>
              158.34
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              38.51
            </tt>
            <br />
            (4.11x)
          </td>
          <td align="right">
            <tt>
              94.68
            </tt>
            <br />
            (1.67x)
          </td>
          <td align="right">
            <tt>
              25.32
            </tt>
            <br />
            (6.25x)
          </td>
        </tr>
        <tr>
          <td>
            Eyebrow warper
          </td>
          <td align="right">
            <tt>
              159.10
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              40.27
            </tt>
            <br />
            (3.95x)
          </td>
          <td align="right">
            <tt>
              95.17
            </tt>
            <br />
            (1.67x)
          </td>
          <td align="right">
            <tt>
              25.71
            </tt>
            <br />
            (6.19x)
          </td>
        </tr>
        <tr>
          <td>
            Eye & mouth morpher
          </td>
          <td align="right">
            <tt>
              152.72
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              71.94
            </tt>
            <br />
            (2.12x)
          </td>
          <td align="right">
            <tt>
              94.10
            </tt>
            <br />
            (1.62x)
          </td>
          <td align="right">
            <tt>
              49.37
            </tt>
            <br />
            (3.09x)
          </td>
        </tr>
        <tr>
          <td>
            Half-resolution rotator
          </td>
          <td align="right">
            <tt>
              209.98
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              113.03
            </tt>
            <br />
            (1.86x)
          </td>
          <td align="right">
            <tt>
              121.32
            </tt>
            <br />
            (1.73x)
          </td>
          <td align="right">
            <tt>
              80.72
            </tt>
            <br />
            (2.60x)
          </td>
        </tr>
        <tr>
          <td>
            Editor
          </td>
          <td align="right">
            <tt>
              312.22
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              349.10
            </tt>
            <br />
            (0.89x)
          </td>
          <td align="right">
            <tt>
              207.46
            </tt>
            <br />
            (1.50x)
          </td>
          <td align="right">
            <tt>
              240.81
            </tt>
            <br />
            (1.30x)
          </td>
        </tr>
        <tr>
          <td>
            Whole System
          </td>
          <td align="right">
            <tt>
              816.91
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              417.49
            </tt>
            <br />
            (1.96x)
          </td>
          <td align="right">
            <tt>
              467.39
            </tt>
            <br />
            (1.75x)
          </td>
          <td align="right">
            <tt>
              274.80
            </tt>
            <br />
            (2.97x)
          </td>
        </tr>
      </tbody>
    </table>
    <b>Table 4.2.2.1</b> RAM usage in MB of the whole system and the five constituent networks. The experiments were conducted on Computer A (Nvidia GeForce MS250, a 1.19 GHz Intel Core i5-1035G1 CPU, and 8 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>&nbsp;</td>
          <td align="center" colspan="4"><b>RAM usage in MB on Computer B<br>(and improvement over standard-float)</b></td>
        </tr>
        <tr>
          <th>
            Networks
          </th>
          <th>
            standard_float
          </th>
          <th>
            separable_float
          </th>
          <th>
            standard_half
          </th>
          <th>
            separable_half
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            Eyebrow segmenter
          </td>
          <td align="right">
            <tt>
              158.34
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              38.51
            </tt>
            <br />
            (4.11x)
          </td>
          <td align="right">
            <tt>
              86.17
            </tt>
            <br />
            (1.84x)
          </td>
          <td align="right">
            <tt>
              19.31
            </tt>
            <br />
            (8.20x)
          </td>
        </tr>
        <tr>
          <td>
            Eyebrow warper
          </td>
          <td align="right">
            <tt>
              159.10
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              40.27
            </tt>
            <br />
            (3.95x)
          </td>
          <td align="right">
            <tt>
              86.92
            </tt>
            <br />
            (1.83x)
          </td>
          <td align="right">
            <tt>
              19.70
            </tt>
            <br />
            (8.08x)
          </td>
        </tr>
        <tr>
          <td>
            Eye & mouth morpher
          </td>
          <td align="right">
            <tt>
              152.72
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              71.94
            </tt>
            <br />
            (2.12x)
          </td>
          <td align="right">
            <tt>
              83.75
            </tt>
            <br />
            (1.82x)
          </td>
          <td align="right">
            <tt>
              35.58
            </tt>
            <br />
            (4.29x)
          </td>
        </tr>
        <tr>
          <td>
            Half-resolution rotator
          </td>
          <td align="right">
            <tt>
              209.98
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              113.03
            </tt>
            <br />
            (1.86x)
          </td>
          <td align="right">
            <tt>
              104.80
            </tt>
            <br />
            (2.00x)
          </td>
          <td align="right">
            <tt>
              56.71
            </tt>
            <br />
            (3.70x)
          </td>
        </tr>
        <tr>
          <td>
            Editor
          </td>
          <td align="right">
            <tt>
              312.22
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              349.10
            </tt>
            <br />
            (0.89x)
          </td>
          <td align="right">
            <tt>
              155.96
            </tt>
            <br />
            (2.00x)
          </td>
          <td align="right">
            <tt>
              176.81
            </tt>
            <br />
            (1.77x)
          </td>
        </tr>
        <tr>
          <td>
            Whole System
          </td>
          <td align="right">
            <tt>
              816.91
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              417.49
            </tt>
            <br />
            (1.96x)
          </td>
          <td align="right">
            <tt>
              417.39
            </tt>
            <br />
            (1.96x)
          </td>
          <td align="right">
            <tt>
              210.80
            </tt>
            <br />
            (3.88x)
          </td>
        </tr>
      </tbody>
    </table>
    <b>Table 4.2.2.2</b> RAM usage in MB of the whole system and the five constituent networks. The experiments were conducted on Computer B (Nvidia GeForce GTX 1090 Ti GPU, a 3.70 GHz Intel Core i7-8700K CPU, and 32 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>&nbsp;</td>
          <td align="center" colspan="4"><b>RAM usage in MB on Computer C<br>(and improvement over standard-float)</b></td>
        </tr>
        <tr>
          <th>
            Networks
          </th>
          <th>
            standard_float
          </th>
          <th>
            separable_float
          </th>
          <th>
            standard_half
          </th>
          <th>
            separable_half
          </th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>
            Eyebrow segmenter
          </td>
          <td align="right">
            <tt>
              142.84
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              38.54
            </tt>
            <br />
            (3.71x)
          </td>
          <td align="right">
            <tt>
              71.42
            </tt>
            <br />
            (2.00x)
          </td>
          <td align="right">
            <tt>
              19.32
            </tt>
            <br />
            (7.39x)
          </td>
        </tr>
        <tr>
          <td>
            Eyebrow warper
          </td>
          <td align="right">
            <tt>
              143.61
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              40.30
            </tt>
            <br />
            (3.56x)
          </td>
          <td align="right">
            <tt>
              71.81
            </tt>
            <br />
            (2.00x)
          </td>
          <td align="right">
            <tt>
              19.72
            </tt>
            <br />
            (7.28x)
          </td>
        </tr>
        <tr>
          <td>
            Eye & mouth morpher
          </td>
          <td align="right">
            <tt>
              144.22
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              71.98
            </tt>
            <br />
            (2.00x)
          </td>
          <td align="right">
            <tt>
              71.86
            </tt>
            <br />
            (2.01x)
          </td>
          <td align="right">
            <tt>
              35.59
            </tt>
            <br />
            (4.05x)
          </td>
        </tr>
        <tr>
          <td>
            Half-resolution rotator
          </td>
          <td align="right">
            <tt>
              209.98
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              113.07
            </tt>
            <br />
            (1.86x)
          </td>
          <td align="right">
            <tt>
              108.80
            </tt>
            <br />
            (1.93x)
          </td>
          <td align="right">
            <tt>
              56.33
            </tt>
            <br />
            (3.73x)
          </td>
        </tr>
        <tr>
          <td>
            Editor
          </td>
          <td align="right">
            <tt>
              312.22
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              349.11
            </tt>
            <br />
            (0.89x)
          </td>
          <td align="right">
            <tt>
              156.96
            </tt>
            <br />
            (1.99x)
          </td>
          <td align="right">
            <tt>
              176.81
            </tt>
            <br />
            (1.77x)
          </td>
        </tr>
        <tr>
          <td>
            Whole System
          </td>
          <td align="right">
            <tt>
              813.91
            </tt>
            <br />
            (1.00x)
          </td>
          <td align="right">
            <tt>
              414.08
            </tt>
            <br />
            (1.97x)
          </td>
          <td align="right">
            <tt>
              415.89
            </tt>
            <br />
            (1.96x)
          </td>
          <td align="right">
            <tt>
              209.30
            </tt>
            <br />
            (3.89x)
          </td>
        </tr>
      </tbody>
    </table>
    <b>Table 4.2.2.3</b> RAM usage in MB of the whole system and the five constituent networks. The experiments were conducted on Computer C (Nvidia GeForce MS250, a 1.19 GHz Intel Core i5-1035G1 CPU, and 8 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 640px">
    <p>From the data, we can spot a number of trends.</p>

    <p>Firstly, while GPU RAM usages by the same network did not differ much between the computers, we can see that those with more capacity tended to use more RAM. This might be because PyTorch collected garbage more aggressively on computers with less available memory.</p>

    <p>Secondly, the amount of RAM used by a network was always more than the network's size. This is assuring because it means that my measurement method properly took into account of model parameters.</p>

    <p>Thirdly, using half reduced memory usage by factors close to 2 on all machines. This is consistent with the expectation that the half type should halve the amount of space needed to store everything.</p>

    <p>Fourthly, depthwise separable convolutions reduced RAM usage of all networks except the editor. This can be explained by the observation that the technique can decrease memory used to store model parameters by about 9 times, but it cannot decrease memory used to store data tensors at all. The editor ($\approx$ 30 MB) was much smaller than other networks ($\approx$ 120 MB) so the reduction in model size was dominated by the increase in space for data tensors.</p>

    <p>To summarize, using half reduced memory requirement under all circumstances. However, using depthwise separable convolutions was only beneficial to large networks (i.e., all networks except the editor). All in all, using both techniques resulted in about 3-4 times reduction of RAM usage for the whole system.</p>

    <h4>4.2.3 &nbsp; Speed</h4>
    
    <p>Another metric we care about is how fast the system is. To measure processing speed, I ran the whole system and the individual networks 100 times with artificial inputs (again, tensors whose values are all zeros) of batch size 1, measured the wall clock time of each run, and then computed the average processing time <a href="#fn_cache_warming">[footnote]</a>. The numbers are available in the tables below.</p>    

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_cache_warming">
          To make the measurements more accurate, I ran the networks two times before the 100 measurement runs in order to warm the cache.
        </li>
      </ul>
    </div>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>Average processing time in milliseconds on Computer A<br>(and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Networks</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              Eyebrow segmenter
            </td>
            <td align="right">
              <tt>
                4.159
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                4.792
              </tt>
              <br />
              (0.87x)
            </td>
            <td align="right">
              <tt>
                4.996
              </tt>
              <br />
              (0.83x)
            </td>
            <td align="right">
              <tt>
                5.212
              </tt>
              <br />
              (0.80x)
            </td>
          </tr>
          <tr>
            <td>
              Eyebrow warper
            </td>
            <td align="right">
              <tt>
                4.726
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                5.772
              </tt>
              <br />
              (0.82x)
            </td>
            <td align="right">
              <tt>
                5.450
              </tt>
              <br />
              (0.87x)
            </td>
            <td align="right">
              <tt>
                5.689
              </tt>
              <br />
              (0.83x)
            </td>
          </tr>
          <tr>
            <td>
              Eye & mouth morpher
            </td>
            <td align="right">
              <tt>
                7.163
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                5.223
              </tt>
              <br />
              (1.37x)
            </td>
            <td align="right">
              <tt>
                5.495
              </tt>
              <br />
              (1.30x)
            </td>
            <td align="right">
              <tt>
                5.608
              </tt>
              <br />
              (1.28x)
            </td>
          </tr>
          <tr>
            <td>
              Half-resolution rotator
            </td>
            <td align="right">
              <tt>
                8.865
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                6.001
              </tt>
              <br />
              (1.48x)
            </td>
            <td align="right">
              <tt>
                5.094
              </tt>
              <br />
              (1.74x)
            </td>
            <td align="right">
              <tt>
                5.605
              </tt>
              <br />
              (1.58x)
            </td>
          </tr>
          <tr>
            <td>
              Editor
            </td>
            <td align="right">
              <tt>
                13.699
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                11.185
              </tt>
              <br />
              (1.22x)
            </td>
            <td align="right">
              <tt>
                7.469
              </tt>
              <br />
              (1.83x)
            </td>
            <td align="right">
              <tt>
                7.986
              </tt>
              <br />
              (1.72x)
            </td>
          </tr>
          <tr>
            <td>
              Whole system
            </td>
            <td align="right">
              <tt>
                34.105
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                26.777
              </tt>
              <br />
              (1.27x)
            </td>
            <td align="right">
              <tt>
                23.803
              </tt>
              <br />
              (1.43x)
            </td>
            <td align="right">
              <tt>
                24.540
              </tt>
              <br />
              (1.39x)
            </td>
          </tr>
        </tbody>
      </table>
      <b>Table 4.2.3.1</b> Average processing time in milliseconds of the whole system and the five constituent networks. The experiments were conducted on Computer A (Nvidia Titan RTX GPU, a 3.60 GHz Intel Core i9-9900KF CPU, and 64 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>Average processing time in milliseconds on Computer B<br>(and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Networks</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              Eyebrow segmenter
            </td>
            <td align="right">
              <tt>
                5.125
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                5.024
              </tt>
              <br />
              (1.02x)
            </td>
            <td align="right">
              <tt>
                5.168
              </tt>
              <br />
              (0.99x)
            </td>
            <td align="right">
              <tt>
                5.064
              </tt>
              <br />
              (1.01x)
            </td>
          </tr>
          <tr>
            <td>
              Eyebrow warper
            </td>
            <td align="right">
              <tt>
                6.717
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                5.441
              </tt>
              <br />
              (1.23x)
            </td>
            <td align="right">
              <tt>
                5.887
              </tt>
              <br />
              (1.14x)
            </td>
            <td align="right">
              <tt>
                5.356
              </tt>
              <br />
              (1.25x)
            </td>
          </tr>
          <tr>
            <td>
              Eye & mouth morpher
            </td>
            <td align="right">
              <tt>
                8.522
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                6.373
              </tt>
              <br />
              (1.34x)
            </td>
            <td align="right">
              <tt>
                8.068
              </tt>
              <br />
              (1.06x)
            </td>
            <td align="right">
              <tt>
                6.014
              </tt>
              <br />
              (1.42x)
            </td>
          </tr>
          <tr>
            <td>
              Half-resolution rotator
            </td>
            <td align="right">
              <tt>
                11.259
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                9.246
              </tt>
              <br />
              (1.22x)
            </td>
            <td align="right">
              <tt>
                10.592
              </tt>
              <br />
              (1.06x)
            </td>
            <td align="right">
              <tt>
                8.316
              </tt>
              <br />
              (1.35x)
            </td>
          </tr>
          <tr>
            <td>
              Editor
            </td>
            <td align="right">
              <tt>
                18.374
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                24.277
              </tt>
              <br />
              (0.76x)
            </td>
            <td align="right">
              <tt>
                13.670
              </tt>
              <br />
              (1.34x)
            </td>
            <td align="right">
              <tt>
                19.273
              </tt>
              <br />
              (0.95x)
            </td>
          </tr>
          <tr>
            <td>
              Whole system
            </td>
            <td align="right">
              <tt>
                43.841
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                46.959
              </tt>
              <br />
              (0.93x)
            </td>
            <td align="right">
              <tt>
                38.019
              </tt>
              <br />
              (1.15x)
            </td>
            <td align="right">
              <tt>
                38.848
              </tt>
              <br />
              (1.13x)
            </td>
          </tr>
        </tbody>
      </table>
      <b>Table 4.2.3.2</b> Average processing time in milliseconds of the whole system and the five constituent networks. The experiments were conducted on Computer B (Nvidia GeForce GTX 1090 Ti GPU, a 3.70 GHz Intel Core i7-8700K CPU, and 32 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 800px;">
    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>&nbsp;</td>
            <td align="center" colspan="4"><b>Average processing time in milliseconds on Computer C<br>(and improvement over standard-float)</b></td>
          </tr>
          <tr>
            <td>
              <b>Networks</b>
            </td>
            <td align="right">
              <b>standard-float</b>
            </td>
            <td align="right">
              <b>separable-float</b>
            </td>
            <td align="right">
              <b>standard-half</b>
            </td>
            <td align="right">
              <b>separable-half</b>
            </td>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>
              Eyebrow segmenter
            </td>
            <td align="right">
              <tt>
                100.705
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                31.590
              </tt>
              <br />
              (3.19x)
            </td>
            <td align="right">
              <tt>
                121.911
              </tt>
              <br />
              (0.83x)
            </td>
            <td align="right">
              <tt>
                31.578
              </tt>
              <br />
              (3.19x)
            </td>
          </tr>
          <tr>
            <td>
              Eyebrow warper
            </td>
            <td align="right">
              <tt>
                106.348
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                32.270
              </tt>
              <br />
              (3.30x)
            </td>
            <td align="right">
              <tt>
                131.114
              </tt>
              <br />
              (0.81x)
            </td>
            <td align="right">
              <tt>
                32.172
              </tt>
              <br />
              (3.31x)
            </td>
          </tr>
          <tr>
            <td>
              Eye & mouth morpher
            </td>
            <td align="right">
              <tt>
                132.273
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                70.078
              </tt>
              <br />
              (1.89x)
            </td>
            <td align="right">
              <tt>
                240.195
              </tt>
              <br />
              (0.55x)
            </td>
            <td align="right">
              <tt>
                73.969
              </tt>
              <br />
              (1.79x)
            </td>
          </tr>
          <tr>
            <td>
              Half-resolution rotator
            </td>
            <td align="right">
              <tt>
                211.828
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                72.645
              </tt>
              <br />
              (2.92x)
            </td>
            <td align="right">
              <tt>
                345.056
              </tt>
              <br />
              (0.61x)
            </td>
            <td align="right">
              <tt>
                91.863
              </tt>
              <br />
              (2.31x)
            </td>
          </tr>
          <tr>
            <td>
              Editor
            </td>
            <td align="right">
              <tt>
                269.462
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                157.015
              </tt>
              <br />
              (1.72x)
            </td>
            <td align="right">
              <tt>
                412.179
              </tt>
              <br />
              (0.65x)
            </td>
            <td align="right">
              <tt>
                192.638
              </tt>
              <br />
              (1.40x)
            </td>
          </tr>
          <tr>
            <td>
              Whole system
            </td>
            <td align="right">
              <tt>
                690.751
              </tt>
              <br />
              (1.00x)
            </td>
            <td align="right">
              <tt>
                335.364
              </tt>
              <br />
              (2.06x)
            </td>
            <td align="right">
              <tt>
                1125.345
              </tt>
              <br />
              (0.61x)
            </td>
            <td align="right">
              <tt>
                385.041
              </tt>
              <br />
              (1.79x)
            </td>
          </tr>
        </tbody>
      </table>
      <b>Table 4.2.3.3</b> Average processing time in milliseconds of the whole system and the five constituent networks. The experiments were conducted on Computer C (Nvidia GeForce MS250, a 1.19 GHz Intel Core i5-1035G1 CPU, and 8 GB of RAM).
    </p>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>While there were clear and predictable patterns in network sizes and RAM usages, changes in processing time were much less predictable.</p>

    <ul>
      <li>While in theory, convolution can be sped up by approximately 9 times by using depthwise separable convolutions, I only observed improvement by a factor of at most 2. This is perhaps obvious in hindsight because the networks also contain normalization and activation layers that are not optimized at all.</li>

      <li>Using half always degraded performance on Computer C. This might be because its GPU lacked proper support for the half type.</li>      

      <li>Interestingly, while both techniques were able to improve the overall system's performance on Computer A, they slowed down the eyebrow segmenter and the eyebrow warper. Because these networks deal with small images, it seems that the techniques have non-negligible overhead that can only be offset when processing larger inputs.</li>

      <li>On Computer B, however, depthwise separable convolutions slowed down the editor, which dealt with the largest image size. This might be because depthwise separable convolutions introduced more processing steps and memory usage, and they must have strained the GTX 1080 Ti's capacity.</li>      
    </ul>

    <p>On all machines, however, employing both techniques greatly reduced the system's memory requirement and also had positive (while not impressive) impact on speed. Thus, one should always apply them together.</p>

    <h4>4.2.4 &nbsp; Visual Quality</h4>

    <p>Each of the techniques resulted in fewer bits being used to represent model parameters. As a result, it is expected that smaller variants would be less accurate. This can be confirmed by computing the similarity metrics on the test set. We can see that, because depthwise separable convolutions reduce network sizes by a factor of 9, it has more impact on the metrics than using half.</p>

    <p>
      <table class="table table-striped">
        <thead class="table-dark">
          <tr>
            <td>
              <b>Variants</b>
            </td>
            <td align="right">
              <b>
                RMSE ($\downarrow$)
              </b>
            </td>
            <td align="right">
              <b>
                SSIM ($\uparrow$)
              </b>
            </td>
            <td align="right">
              <b>
                LPIPS ($\downarrow$)
              </b>
            </td>
          </tr>
        </thead>
        <tr>
          <td>
            standard-float
          </td>
          <td align="right">
            <b>
              <tt>0.15518600</tt>
            </b>
          </td>
          <td align="right">
            <b>
              <tt>0.90928100</tt>
            </b>
          </td>
          <td align="right">
            <b>
              <tt>0.04903900</tt>
            </b>
          </td>
        </tr>
        <tr>
          <td>
            separable-float
          </td>
          <td align="right">
            <tt>0.16107100</tt>
          </td>
          <td align="right">
            <tt>0.90425300</tt>            
          </td>
          <td align="right">
            <tt>0.05354000</tt>
          </td>
        </tr>
        <tr>
          <td>
            standard-half
          </td>
          <td align="right">
            <tt>0.15551100</tt>
          </td>
          <td align="right">
            <tt>0.90892600</tt>            
          </td>
          <td align="right">
            <tt>0.05008000</tt>
          </td>
        </tr>
        <tr>
          <td>
            separable-half
          </td>
          <td align="right">
            <tt>0.16141600</tt>
          </td>
          <td align="right">
            <tt>0.90390400</tt>
          </td>
          <td align="right">
            <tt>0.05458700</tt>
          </td>
        </tr>
      </table>
      <b>Table 4.2.4.1</b> Similarity metrics between the ground truth images and the outputs produced by the 4 system variants.
    </p>

    <p>I also rendered animations using the 4 variants for qualitative comparisons.</p>
  </div>

  <div class="container" style="max-width: 1024px;">
    <a name="fig:compare-prefimprove-grid" />
		<table border="1" cellpadding="5">
		<tr>
		<td id="comparePerformanceImprovementGridVideoCell">
			<video muted controls loop width="1024">
			    <source src="data/perfimprove/kizuna_ai/grid.mp4" type="video/mp4">
			</video>
		</td>		
		</tr>
		<tr>
		<td align="center">
			Target Character: <select id="comparePerformanceImprovementGridCharacterSelect">
			<option value="kizuna_ai" selected="selected">Kizuna AI</option>
			<option value="tokino_sora">Tokino Sora</option>      
			<option value="akiyama_rentarou">Akiyama Rentarou</option>
			<option value="kitakami_futaba">Kitakami Futaba</option>
      <option value="kiso_azuki">Kiso Azuki</option>
      <option value="mokota_mememe">Mokota Mememe</option>
			<option value="kagura_suzu">Kagura Suzu</option>
		</select>
		</td>
		</tr>
		</table>
		<b>Figure 4.2.4.2</b> Comparison between the ground truth 3D animations and videos generated by the 4 variants of my system. The characters are Kizuna AI (&copy; Kizuna AI), Tokino Sora (&copy; Tokino Sora Ch.), Akiyama Rentarou (&copy; ひま食堂), Kitakami Futaba</a> (&copy; Appland, Inc.), <a href="https://www.youtube.com/channel/UCmM5LprTu6-mSlIiRNkiXYg">Kiso Azuki</a> (&copy; Appland, Inc.), <a href="https://www.youtube.com/channel/UCz6Gi81kE6p5cdW1rT0ixqw">Mokota Mememe</a> (&copy; Appland, Inc.), and Kagura Suzu (&copy; Appland, Inc.).
		<br>
		<br>
	</div>

  <script type="text/javascript">
		function changeComparePerformanceImprovementGridCharacter() {
			var newTarget = $("#comparePerformanceImprovementGridCharacterSelect").val();
			var videoFileName = "data/perfimprove/" + newTarget + "/grid.mp4";
			var html = "<video muted controls loop width=\"1024\">"
				+ "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
				+ "</video>";
			$("#comparePerformanceImprovementGridVideoCell").html(html);
		}

		$("#comparePerformanceImprovementGridCharacterSelect").change(changeComparePerformanceImprovementGridCharacter);
	</script>

<div class="container" style="max-width: 768px;">
  <a name="fig:compare-prefimprove-grid" />
  <table border="1" cellpadding="5">
  <tr>
  <td id="comparePerformanceImprovementGridFaceVideoCell">
    <video muted controls loop width="768">
        <source src="data/perfimprove/kizuna_ai/grid_face.mp4" type="video/mp4">
    </video>
  </td>		
  </tr>
  <tr>
  <td align="center">
    Target Character: <select id="comparePerformanceImprovementGridFaceCharacterSelect">
    <option value="kizuna_ai" selected="selected">Kizuna AI</option>
    <option value="tokino_sora">Tokino Sora</option>      
    <option value="akiyama_rentarou">Akiyama Rentarou</option>
    <option value="kitakami_futaba">Kitakami Futaba</option>
    <option value="kiso_azuki">Kiso Azuki</option>
    <option value="mokota_mememe">Mokota Mememe</option>
    <option value="kagura_suzu">Kagura Suzu</option>
  </select>
  </td>
  </tr>
  </table>
  <center><b>Figure 4.2.4.3</b> The same animations in Figure 4.2.6 but with the faces being zoomed in.</center>
  <br>
</div>

<script type="text/javascript">
  function changeComparePerformanceImprovementGridFaceCharacter() {
    var newTarget = $("#comparePerformanceImprovementGridFaceCharacterSelect").val();
    var videoFileName = "data/perfimprove/" + newTarget + "/grid_face.mp4";
    var html = "<video muted controls loop width=\"768\">"
      + "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
      + "</video>";
    $("#comparePerformanceImprovementGridFaceVideoCell").html(html);
  }

  $("#comparePerformanceImprovementGridFaceCharacterSelect").change(changeComparePerformanceImprovementGridFaceCharacter);
</script>

  <div class="container" style="max-width: 640px;">
    <p>The variants with the same type of convolution layers produced virtually the same animations because the parameters were essentially the same but with different precisions. Variants with different convolution layers, however, produced different mouth shapes. Mouths produced by separable convolutions were smaller than those produced by standard ones. For Kiso Azuki, the mouth did not move at all, showing that separable convolutions can yield inaccurate results in some cases. This might be the price to pay for a significant reduction in model size.</p>

    <p>In conclusion, the techniques I experimented on this section, when combined, were effective at reducing the system's size and RAM usage. Additionally, they had positive but not significant impact (no more than 2x) on speed. The techniques surely made the system more employable on less powerful devices, but there is still much room for improvement in processing time.</p>
    
    <a name="sec:applications-to-drawings"></a>
		<h2>5 &nbsp; Applications to Drawings</h2>

    <p>As with previous versions of the system, my end goal is to animate drawings, not 3D renderings. In this section, I demonstrate how the system performs on such inputs.</p>

    <h3>5.1 &nbsp; Simple Character Animations</h3>

    <p>I used the system to animate 72 images of VTubers and related characters. Sixteen resulting videos are available below, and the rest can be seen in <a href="#fig:drawn-character-videos">Figure 5.1.2</a>.</p>
  </div>

  <div class="container" style="max-width: 1024px;">
    <p >
      <table align="center">
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/tsukino_mito/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kuzuha/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/sasaki_saku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/yashiro_kizuku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/otogibara_era/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suzuhara_lulu/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_sango/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kitakoji_hisui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shirakami_fubuki/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/minato_aqua/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/amane_kanata/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/houshou_marine/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/inaba_haneru/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/suou_patra/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/shigure_ui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=192 height=192 muted autoplay playsinline loop>
              <source src="data/demo/kagura_mea/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>Figure 5.1.1</b> Videos created by applying the "standard-full" variant of the system to 16 draw characters.
          </td>
        </tr>
      </table>      
    </p>    
  </div>
    
  <div class="container" style="max-width: 640px;">
    <p>
      <a name="fig:drawn-character-videos" />
    <table border="1" cellpadding="5" align="center">
    <tr>
    <td id="drawnCharacterVideoCell">
      <video muted controls loop width="512">
          <source src="data/demo/tsukino_mito/video_for_web.mp4" type="video/mp4">          
      </video>
    </td>		
    </tr>
    <tr>
    <td align="center">
      Target Character: <select id="drawCharacterVideoSelect">
      <optgroup label="Nijisanji">
        <option value='amagase_muyu'>Amagase Muyu </option>
        <option value='bora_nun_02'>Bora Nun</option>
        <option value='dola'>Dola </option>
        <option value='eudric'>Eudric </option>
        <option value='furen_e_lustario'>Furen E. Lustario </option>
        <option value='hakase_fuyuki'>Hakase Fuyuki </option>
        <option value='hanabatake_chaika'>Hanabatake Chaika </option>
        <option value='honma_himawari'>Honma Himawari </option>
        <option value='higuchi_kaede'>Higuchi Kaede </option>
        <option value='ibrahim'>Ibrahim </option>
        <option value='izumo_kasumi'>Izumo Kasumi </option>
        <option value='kagami_hayato'>Kagami Hayato </option>
        <option value='kitakoji_hisui'>Kitakoji Hisui </option>
        <option value='kuzuha'>Kuzuha </option>
        <option value='kanae'>Kanae </option>
        <option value='lain_paterson'>Lain Paterson </option>
        <option value='melissa_kinrenka'>Melissa Kinrenka </option>
        <option value='nakao_azuma'>Nakao Azuma </option>
        <option value='nishizono_chigusa'>Nishizono Chigusa </option>
        <option value='oliver_evans'>Oliver Evans </option>
        <option value='otogibara_era'>Otogibara Era </option>
        <option value='ponto_nei'>Ponto Nei </option>
        <option value='rindou_mikoto'>Rindou Mikoto </option>
        <option value='sasaki_saku'>Sasaki Saku </option>
        <option value='shiina_yuika'>Shiina Yuika </option>
        <option value='shizuka_rin'>Shizuka Rin </option>
        <option value='suou_sango'>Suou Sango </option>
        <option value='suzuhara_lulu'>Suzuhara Lulu </option>
        <option value='takamiya_rion'>Takamiya Rion </option>
        <option value='tsukino_mito' selected="selected">Tsukino Mito </option>
        <option value='umise_yotsuha'>Umise Yotsuha </option>
        <option value='warabeda_meijii'>Warabeda Meijii </option>
        <option value='yashiro_kizuku'>Yashiro Kizuku </option>
        <option value='yorumi_rena'>Yorumi Rena </option>
        <option value='toudou_kohaku'>Toudou Kohaku </option>
        <option value='asahina_akane'>Asahina Akane </option>
      </optgroup>
      <optgroup label="Hololive Productions">
        <option value='a_chan'>A Chan </option>
        <option value='amane_kanata'>Amane Kanata </option>
        <option value='arurandeisu'>Arurandeisu </option>
        <option value='gawr_gura'>Gawr Gura </option>
        <option value='hakui_koyori'>Hakui Koyori </option>
        <option value='harusaki_nodoka'>Harusaki Nodoka </option>
        <option value='houshou_marine'>Houshou Marine </option>
        <option value='inugami_korone'>Inugami Korone </option>
        <option value='kiryuu_koko'>Kiryuu Koko </option>
        <option value='minato_aqua'>Minato Aqua </option>
        <option value='natsuiro_matsuri'>Natsuiro Matsuri </option>
        <option value='nekomata_okayu'>Nekomata Okayu </option>
        <option value='oozora_subaru'>Oozora Subaru </option>
        <option value='sakamata_chloe_02'>Sakamata Chloe</option>
        <option value='shirakami_fubuki'>Shirakami Fubuki </option>
        <option value='usada_pekora'>Usada Pekora </option>
        <option value='vestia_zeta'>Vestia Zeta </option>
        <option value='yuukoku_robel'>Yuukoku Robel </option>
      </optgroup>
      <optgroup label="774 Inc.">
        <option value='inaba_haneru'>Inaba Haneru </option>
        <option value='souya_ichika'>Souya Ichika </option>
        <option value='yunohara_izumi'>Yunohara Izumi </option>
        <option value='kazami_kuku'>Kazami Kuku </option>
        <option value='suou_patra'>Suou Patra </option>
        <option value='sekishiro_miko'>Sekishiro Miko </option>
      </optgroup>
      <optgroup label="Noripro">
        <option value='houzuki_warabe'>Houzuki Warabe </option>
        <option value='shirayuki_mishiro'>Shirayuki Mishiro </option>
        <option value='inuyama_tamaki'>Inuyama Tamaki </option>
      </optgroup>
      <optgroup value="Indie VTubers">
        <option value='kagura_mea'>Kagura Mea </option>
        <option value='shigure_ui'>Shigure Ui </option>
        <option value='natsume_eri'>Natsume Eri </option>
        <option value='natori_sana'>Natori Sana </option>
        <option value='tomari_mari'>Tomari Mari </option>
        <option value='magurona'>Magurona </option>
        <option value='itou_life'>Itou Life </option>
        <option value='anesaki_yukimi'>Anesaki Yukimi </option>
        <option value='yuzuriha_honami'>Yuzuriha Honami </option>
      </optgroup>
    </select>
    </td>
    </tr>
    </table>
    <b>Figure 5.1.2</b> Videos created by applying the "standard-full" variant of the system to drawings of various VTubers and associated characters.
    </p>
      
    <script type="text/javascript">
      function changeDrawnCharacterVideoCharacter() {
        var newTarget = $("#drawCharacterVideoSelect").val();
        var videoFileName = "data/demo/" + newTarget + "/video_for_web.mp4";
        var html = "<video muted controls loop width=\"512\">"
          + "<source src=\"" + videoFileName + "\" type=\"video/mp4\">"
          + "</video>";
        $("#drawnCharacterVideoCell").html(html);
      }
  
      $("#drawCharacterVideoSelect").change(changeDrawnCharacterVideoCharacter);
    </script>

    <h3>5.2 &nbsp; Breathing Motion</h3>

    <p>The animation in the previous subsection does include the breathing motion, but it is hard to notice because its effect is subtle compared other motion types. The figure blow shows the pure breathing motion of the 16 characters in Figure 5.1.1.</p>
  </div>

  <div class="container" style="max-width: 1200px;">
    <p>
      <table align="center">
        <tr>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/tsukino_mito/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/kuzuha/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/sasaki_saku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/yashiro_kizuku/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/otogibara_era/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/suzuhara_lulu/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/suou_sango/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/kitakoji_hisui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/shirakami_fubuki/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/minato_aqua/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/amane_kanata/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/houshou_marine/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/inaba_haneru/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/suou_patra/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/shigure_ui/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
          <td>
            <video width=256 muted autoplay playsinline loop>
              <source src="data/breathing_motion/kagura_mea/video_for_web.mp4" type="video/mp4">
            </video>
          </td>
        </tr>
        <tr>
          <td colspan="4" align="center">
            <b>Figure 5.2.1</b> Breathing motion of 16 drawn characters. The videos were  created using the "standard-full" variant of the system.
          </td>
        </tr>
      </table>      
    </p>
  </div>   

  <div class="container" style="max-width: 640px;">
    <p>We can see that the movement is most of the time plausible. However, there are cases where the network wrongly classified the network wrongly located the chest area. For example, the skirt of the bottom right character also moves with the chest.</p>

    <h3>5.3 &nbsp; Direct Manipulation of Drawings through GUI</h3>

    <p>I created a tool that allows the user to control drawings by manipulating GUI elements.</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/9BzdxrYVSrs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <h3>5.4 &nbsp; Transferring Human Motion to Characters</h3>

    <p>I also created another tool that can transfer a human's movement, captured by an iOS application called <a href="https://www.ifacialmocap.com/">iFacialMocap</a>, to anime characters.</p>

    <p align="center">
      <iframe width="560" height="315" src="https://www.youtube.com/embed/TScnh3XC_14" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
    </p>

    <h3>5.5 &nbsp; Failure Cases</h3>

    <p>The above demonstrations show that my system is capable of generating good-looking animations when applied to many different characters. However, it can yield implausible results when fed inputs that deviate significantly from the training set. Because I did not change the face morpher in any way, its problems, as discussed in <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html#limitations">the 2021 writeup</a>, still remain. These include the inability to handle unnatural skin tone, strong makeups, "maro" eyebrows, and rare occlusion of facial features. I also observed new problems specific to the newly designed body rotator.</p>

    <p>First, the body rotator did not seem to handle large hats well. For examples, it thought that Nui Sociere's face is a part of the hat, erased the ears and tail of Millie Parfait's cat, and moved Nina Kosaka's ears in a way that was inconsistent with her head movement. These errors might be because my dataset does not have many models wearing large hats.</p>
  </div>

  <div class="container" style="max-width: 1024px">
    <table align="center">
      <tr>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nui_sociere/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/millie_parfait/video_for_web.mp4" type="video/mp4">
          </video>            
        </td>
        <td>
          <video autoplay muted playsinline loop>
            <source src="data/failure_cases/nina_kosaka/video_for_web.mp4" type="video/mp4">
          </video>
        </td>
      </tr>
      <tr>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCUc8GZfFxtmk7ZwSO7ccQ0g">Nui Sociere</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UC47rNmkDcNgbOcM-2BwzJTQ">Millie Parfait</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
        <td align="center">
          <a href="https://www.youtube.com/channel/UCkieJGn3pgJikVW8gmMXE2w">Nina Kosaka</a>
          (&copy; ANYCOLOR, Inc.)
        </td>
      </tr>
    </table>
    <br>
  </div>

  <div class="container" style="max-width: 640px;">
    <p>Second, it also had a tendency to erase thin structures around the head. While this might be acceptable for thin hair filaments, the result can be very noticeable for halos and rigid ornaments that should always be present.</p>
  
    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ninomae_inanis/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/ouro_kronii/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>        
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCMwGHR0BTZuLsmjY_NT5Pwg">Ninomae Ina'nis</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/c/OuroKroniiCh">Ouro Kronii</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
      </table>
    </p>
  
    <p>Third, due to the lack of training data, the body rotator cannot correctly deal with props such as weapons and musical instruments.</p>

    <p>
      <table align="center">
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/gawr_gura/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/mori_calliope/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>          
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCoSrY_IQQVpmIRZ9Xf-y93g">Gawr Gura</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UCL_qhgtOy0dy1Agp8vkySQg">Mori Calliope</a>
            (&copy; Cover corp.)
          </td>        
        </tr>
        <tr>
          <td>
            <video autoplay muted playsinline loop>
              <source src="data/failure_cases/kazama_iroha/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/failure_cases/rikka/video_for_web.mp4" type="video/mp4">
            </video>            
          </td>
        </tr>
        <tr>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC_vMYWcDjmfdpH6r4TTn1MQ">Kazama Iroha</a>
            (&copy; Cover corp.)
          </td>
          <td align="center">
            <a href="https://www.youtube.com/channel/UC9mf_ZVpouoILRY9NUIaK-w">Rikka</a>
            (&copy; Cover corp.)
          </td>
        </tr>
      </table>      
    </p>

    <a name="sec:related-works"></a>
		<h2>6 &nbsp; Related Works</h2>

    <p>This project aims to solve a variant of the <b>image animation problem</b>. Here, we are given an image of a character and a description of the pose that the character is supposed to take, and we are supposed to generate a new image of the same character taking the described pose. The problem can the be classified into several variants based on the nature of the pose description. They include the <b><i>parameter-based posing</i></b> and the <b><i>motion transfer</i></b> problem previously mentioned in <a href="#sec:other-work-comparison">Section 3.5.2</a>.</p>

    <p>In my <a href="https://pkhungurn.github.io/talking-head-anime-2/full.html#background">2021 write-up</a>, I wrote an extensive survey of previous research on the two problems. Doing such a survey again would only make this article unreasonably long. So, what I would like to do instead is to discuss new research and development that came out after I published the write-up.</p>

    <h3>6.1 &nbsp; Research on Parameter-Based Posing</h3>

    <p>PIRenderer, the paper I compared my system against in <a href="#sec:other-work-comparison">Section 3.5.2</a>, was published in ICCV 2021 <a href="#fn_ren_2021_2">[Ren et al. 2021]</a>. It stood out to me because there have been much fewer papers on parameter-based posing compared to motion transfer. As previously discussed in <a href="#sec:other-work-comparison">Section 3.5.2</a>, though, my implemention of PIRenderer performed worse than my system, and it seemed to have problems animating the neck.</p>

    <div class="footnotes">
			<ul>									
        <li class="footnote" id="fn_ren_2021_2">
          Yurui Ren, Ge Li, Yuanqi Chen, Thomas H. Li, and Shan Liu.
          <b>PIRenderer: Controllable Portrait Image Generation via Semantic Neural Rendering.</b>
          ICCV 2021.
        </li>
        <li class="footnote" id="fn_karras_2019">
          Tero Karras, Samuli Laine, and Timo Aila.
          <b>A Style-Based Generator Architecture for Generative Adversarial Networks.</b>
          CVPR 2019.
          <a href="https://github.com/NVlabs/stylegan">[Github]</a>
        </li>
        <li class="footnote" id="fn_huang_2017">
          Xun Huang and Serge Belongie.
          <b>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization.</b>
          ICCV 2017.
          <a href="https://arxiv.org/abs/1703.06868">[arXiv]</a>
        </li>
        <li class="footnote" id="fn_johnson_2016_2">
          <p align="left">
            Justin Johnson, Alexandre Alahi, Li Fei-Fei.
            <b>Perceptual Losses for Real-Time Style Transfer and Super-Resolution.</b>
            ECCV 2016.
            <a href="https://cs.stanford.edu/people/jcjohns/eccv16/">[Project]</a>
            <a href="https://arxiv.org/abs/1603.08155">[arXiv]</a>
          </p>
        </li>
			</ul>
		</div>

    <p>I became aware of PIRenderer through the AnimeCeleb  paper by Kim et al. <a href="#fn_kim_2021_2">[2021]</a>. It documents the authors' attempt to create a dataset of posed anime characters by rendering MikuMikuDance models in a way similar to what I did in <a href="http://pkhungurn.github.io/talking-head-anime-2/full.html">my 2021 project</a>. The authors then used the dataset to train PIRenderer and demonstrated that the system also worked on anime characters.</p>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_kim_2021_2">
          Kangyeol Kim, Sunghyun Park, Jaeseong Lee, Sunghyo Chung, Junsoo Lee, and Jaegul Choo.
          <b>AnimeCeleb: Large-Scale Animation CelebFaces Dataset via Controllable 3D Synthetic Models.</b>
          2021.
          <a href="https://arxiv.org/abs/2111.07640">[arXiv]</a>
        </li>
      </ul>
    </div>

    <p>Outside of academia, <a href="http://iriam.com">IRIAM</a>, a streaming application where users can broadcast as anime characters, <a href="https://prtimes.jp/main/html/rd/p/000000006.000070082.html">released a feature</a> where a Live2D-like 2.5D model can be created from a single image. The enabling technology was the work of my friend, <a href="https://aixile.github.io/">Yanghua Jin</a>, while he was employed by <a href="https://www.preferred.jp/">Preferred Networks</a>. From the promotion video, it seems that IRIAM supports facial expression manipulation and rotation of the body and the head around the $z$-axis. Rotation around the $y$-axis, nevertheless, is limited. One great advantage of their approach is that, once a 2.5D model has been created, it can be rendered with very low computational cost on mobile devices. On the other hand, even with the efficiency improvements in Section 4, it is still very hard to deploy my networks on a smart phone.</p>

    <h3>6.2 &nbsp; Research on Motion Transfer</h3>

    <p>There are a number of interesting new approaches to motion transfer, especially those that discover moving parts without explicit supervision.</p>

    <p>The paper "Motion Representations for Articulated Animation" (MRAA) <a href="#fn_siarohin_2021">[Siarohin et al. 2021]</a> has the same first author as the famous "First Order Motion Model for Image Animation" (FOMM) <a href="#fn_siarohin_2019">[Siarohin et al. 2019]</a>. MRAA seeks to address the flaws of FOMM by representing movement through the change of the principal components of the area each body part. It also models background movement in order to not waste network resources on simple background motion. Moreover, it proposes a way to disentangle shape from motion in order to deal better with cross-identity motion transfer.</p>

    <p>The paper "Thin-Plate Spline Motion Model for Image Animation" by Zhao and Zhang proposes another way to represent motion of body parts <a href="#fn_zhao_2022">[Zhano and Zhang 2022]</a>. Here, each part has multiple keypoints. (5 are used in the paper.) As a part moves, the keypoints change their positions, and the motion of the part is determined by the thin-plate spline warp that results from the position changes <a href="#fn_eberly_2020">[Eberly 2022]</a>.</p>

    <p>Lastly, "Structure-Aware Motion Transfer with Deformable Anchor Model" by Tao et al. proposes the deformable anchor model (DAM) as a represention for the character's movement <a href="#fn_tao_2022">[Tao et al. 2022]</a>. Like FOMM, a DAM consists of a number of keypoints, which are used to represent movement of body parts. However, they are now called "motion anchors." It also introduces a "latent root anchor" to represent the motion of the whole body. The movement of the motion anchors is regularized to be similar to that of the latent root anchor. The idea is that, if each keypoint correponds to a body part, this regularization should better preserve the relative position between the parts. The paper also introduces a hierarchical version of DAM where the anchors form a tree with motion anchors as leaves.</p>

    <div class="footnotes">
      <ul>
        <li class="footnote" id="fn_siarohin_2021">
          Aliaksandr Siarohin, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov.
          <b>Motion Representations for Articulated Animation.</b>
          CVPR 2021.
          <a href="https://arxiv.org/abs/2104.11280">[arXiv]</a>
        </li>
        <li class="footnote" id="fn_siarohin_2019">
          Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci and Nicu Sebe.
          <b>First Order Motion Model for Image Animation.</b>
          NeurIPS 2019.
          <a href="https://aliaksandrsiarohin.github.io/first-order-model-website/">[HTML]</a>
        </li>
        <li class="footnote" id="fn_zhao_2022">
          Jian Zhao and Hui Zhang.
          <b>Thin-Plate Spline Motion Model for Image Animation.</b>
          CVPR 2022.
          <a href="https://arxiv.org/abs/2203.14367">[arXiv]</a>
        </li>        
        <li class="footnote" id="fn_tao_2022">
          Jiale Tao, Biao Wang, Borun Xu, Tiezheng Ge, Yuning Jiang, Wen Li, and Lixin Duan.
          <b>Structure-Aware Motion Transfer with Deformable Anchor Model.</b>
          CVPR 2022.
          <a href="https://arxiv.org/abs/2204.05018">[arXiv]</a>
        </li>
        <li class="footnote" id="fn_eberly_2020">
          David Eberly.
          <b>Thin-Plate Splines.</b>                    
          2020.
          <a href="https://www.geometrictools.com/Documentation/ThinPlateSplines.pdf">[PDF]</a>
        </li>
      </ul>

      <p>Unfortunately, I have not tried how well these approaches work on my dataset yet.</p>
    </div>

    <a name="sec:conclusion"></a>
		<h2>7 &nbsp; Conclusion</h2>

		<p>In this article, I have discussed my attempt to improve my animation-from-a-single-image system. By replacing two constituent networks with newly designed ones, I enabled the system to rotate the body and generate breathing motion, making its features closer to those offered by professionally-made Live2D models. The system outputs plausible animations for characters with simple designs, but it struggles on those with props not sufficiently covered by the training dataset. These include large hats, weapons, musical instruments, and other thin ornamental structures.</p>

    <p>I also explored making the system more efficient through using depthwise separable convolutions and the "half" type.  Employing both, I made the system 18 times smaller, descreased its GPU RAM usage by about 3 to 4 times, and also slightly improved its speed. While this makes it easier to deploy the system on less powerful devices, more research is needed to make it significantly faster.</p>

		<h2>8 &nbsp; Disclaimer</h2>

    <p>While I am an employee of Google Japan, this project is my personal hobby which I did in my free time without using Google's resources. It has nothing to do with work as I am a normal software engineer writing Google Maps backends for a living. Moreover, I currently do not belong to any of Google's or Alphabet's research organizations. Opinions expressed in this article is my own and not the company's. Google, though, may claim rights to the article's technical inventions.</p>

		<h2>9 &nbsp; Special Thanks</h2>

		<p>I would like to thank <a href="https://www.linkedin.com/in/andrewluchen">Andrew Chen</a>, <a href="https://ekapolc.github.io/">Ekapol Chuangsuwanich</a>, <a href="https://aixile.github.io/">Yanghua Jin</a>, <a href="https://minjun.li/">Minjun Li</a>, <a href="https://ppasupat.github.io/">Panupong Pasupat</a>, <a href="https://alantian.net/">Yingtao Tian</a>, and <a href="https://puchupala.com/">Pongsakorn U-chupala</a> for their comments.</p>

    <a name="sec:pose-parameters"></a>
    <h2>A &nbsp; Pose Parameters</h2>

    <p>The system in this article takes a 45-dimensional pose vector as input. I show the semantics of each parameter below. The character is <a href="https://www.youtube.com/channel/UC2kyQhzGOB-JPgcQX9OMgEw">Souya Ichika</a> (&copy; 774 inc.).</p>
    
    <h3>A.1 &nbsp; Eyebrow Parameters (12)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>0</td>
          <td><tt>eyebrow_troubled_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_troubled_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>1</td>
          <td><tt>eyebrow_troubled_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_troubled_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>2</td>
          <td><tt>eyebrow_angry_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_troubled_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>3</td>
          <td><tt>eyebrow_angry_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_troubled_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>4</td>
          <td><tt>eyebrow_lowered_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_lowered_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>5</td>
          <td><tt>eyebrow_lowered_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_lowered_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>6</td>
          <td><tt>eyebrow_raised_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_raised_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>7</td>
          <td><tt>eyebrow_raised_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_raised_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>8</td>
          <td><tt>eyebrow_happy_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_happy_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>9</td>
          <td><tt>eyebrow_happy_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_happy_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>10</td>
          <td><tt>eyebrow_serious_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_serious_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>11</td>
          <td><tt>eyebrow_happy_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eyebrow_serious_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
      </tbody>
    </table>

    <h3>A.2 &nbsp; Eye Parameters (12)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>12</td>
          <td><tt>eye_wink_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_wink_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>13</td>
          <td><tt>eye_wink_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_wink_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>14</td>
          <td><tt>eye_happy_wink_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_happy_wink_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>15</td>
          <td><tt>eye_happy_wink_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_happy_wink_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>16</td>
          <td><tt>eye_surprised_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_surprised_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>17</td>
          <td><tt>eye_surprised_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_surprised_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>18</td>
          <td><tt>eye_relaxed_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_relaxed_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>19</td>
          <td><tt>eye_relaxed_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_relaxed_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>20</td>
          <td><tt>eye_unimpressed_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_umimpressed_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>21</td>
          <td><tt>eye_unimpressed_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_unimpressed_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>22</td>
          <td><font size="2"><tt>eye_raised_lower_eyelid_left</tt></font></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_raised_lower_eyelid_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>23</td>
          <td><font size="2"><tt>eye_raised_lower_eyelid_right</tt></font></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/eye_raised_lower_eyelid_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
      </tbody>
    </table>

    <h3>A.3 &nbsp; Iris Parameters (4)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>24</td>
          <td><tt>iris_small_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/iris_small_left.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>25</td>
          <td><tt>iris_small_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/iris_small_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>37</td>
          <td><tt>iris_rotation_x</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/iris_rotation_x.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>38</td>
          <td><tt>iris_rotation_y</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/iris_rotation_y.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>        
      </tbody>
    </table>

    <h3>A.4 &nbsp; Mouth Parameters (11)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>25</td>
          <td><tt>mouth_aaa</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_aaa.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>        
        <tr>
          <td>27</td>
          <td><tt>mouth_iii</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_iii.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>28</td>
          <td><tt>mouth_uuu</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_uuu.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>29</td>
          <td><tt>mouth_eee</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_eee.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>30</td>
          <td><tt>mouth_ooo</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_ooo.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>31</td>
          <td><tt>mouth_delta</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_delta.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>32</td>
          <td><tt>mouth_lowered_corner_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_lowered_corner_left.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>33</td>
          <td><tt>mouth_lowered_corner_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_lowered_corner_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>34</td>
          <td><tt>mouth_raised_corner_left</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_raised_corner_left.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>35</td>
          <td><tt>mouth_raised_corner_right</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_raised_corner_right.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
        <tr>
          <td>36</td>
          <td><tt>mouth_smirk</tt></td>
          <td>
            <video autoplay muted playsinline loop width="300">
              <source src="data/parameter_animations/souya_ichika/mouth_smirk.mp4" type="video/mp4">
          </video>
          </td>
        </tr>
      </tbody>
    </table>

    <h3>A.5 &nbsp; Head Rotation Parameters (3)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>39</td>
          <td><tt>head_x</tt></td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/parameter_animations/souya_ichika/head_x.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>40</td>
          <td><tt>head_y</tt></td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/parameter_animations/souya_ichika/head_y.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>   
        <tr>
          <td>41</td>
          <td><tt>neck_z</tt></td>
          <td>
            <video autoplay muted playsinline loop width="256">
              <source src="data/parameter_animations/souya_ichika/neck_z.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>        
      </tbody>
    </table>

    <h3>A.6 &nbsp; Body Parmeters (3)</h3>
    <table class="table table-striped">
      <thead class="table-dark">
        <tr>
          <td>Index</td>
          <td>Name</td>
          <td>Semantics</td>
        </tr>      
      </thead>
      <tbody>
        <tr>
          <td>42</td>
          <td><tt>body_y</tt></td>
          <td>
            <video autoplay muted playsinline loop width="200">
              <source src="data/parameter_animations/souya_ichika/body_y.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>
        <tr>
          <td>43</td>
          <td><tt>body_z</tt></td>
          <td>
            <video autoplay muted playsinline loop width="200">
              <source src="data/parameter_animations/souya_ichika/body_z.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>   
        <tr>
          <td>44</td>
          <td><tt>breathing</tt></td>
          <td>
            <video autoplay muted playsinline loop width="400">
              <source src="data/parameter_animations/souya_ichika/breathing.mp4" type="video/mp4">
          </video>
          </td>          
        </tr>        
      </tbody>
    </table>

		<hr>
		<b>Update History</b>
		<ul>
			<li>2022/06/06: First publication.</li>
		</ul>

		<hr>
		<p align="right"><font size="1">Project <a href="https://en.wikipedia.org/wiki/Tagetes">Marigold</a></font></p>

    <br>
    <br>
    <br>
	</div>
  <script src="js/bootstrap.bundle.min.js"></script>    
    <script>
    $.bigfoot();
  </script>
  </body>  
</html>
